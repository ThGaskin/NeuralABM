diff --git a/data/Kuramoto/networks/_report.txt b/data/Kuramoto/networks/_report.txt
deleted file mode 100644
index 17c3635..0000000
--- a/data/Kuramoto/networks/_report.txt
+++ /dev/null
@@ -1,28 +0,0 @@
-Runtime Statistics 
------------------- 
- 
-The statistics below are calculated from all individual universe run times. 
- 
-  # universes:  4 / 4 
- 
-  total (CPU)   6.7s 
-  total (wall)  2.1s 
-  mean          1.7s 
-   (last 50%)   1.7s 
-   (last 20%)   1.9s 
-   (last 5%)    1.9s 
-  std           0.1s 
-  min           1.6s 
-  at 25%        1.6s 
-  median        1.6s 
-  at 75%        1.7s 
-  max           1.9s 
- 
- 
-Universe Runtimes 
------------------ 
- 
-  uni1          1.7s 
-  uni2          1.6s 
-  uni3          1.6s 
-  uni4          1.9s
\ No newline at end of file
diff --git a/data/Kuramoto/networks/_sweep_info.txt b/data/Kuramoto/networks/_sweep_info.txt
deleted file mode 100644
index 5539220..0000000
--- a/data/Kuramoto/networks/_sweep_info.txt
+++ /dev/null
@@ -1,25 +0,0 @@
-Sweeping over the following parameter space:
-
-ParamSpace Information
-======================
-
-  Dimensions:  1
-  Coupled:     1
-  Shape:       (4,)
-  Volume:      4
-
-Parameter Dimensions
---------------------
-  (Dimensions further up in the list are iterated over less frequently)
-
-  - N
-      (16, 100, 500, 1000)
-
-
-Coupled Parameter Dimensions
-----------------------------
-  (Move alongside the state of the coupled ParamDim)
-
-  - mean_degree
-      Coupled to:  N
-      Values:      (6, 20, 40, 150)
diff --git a/data/Kuramoto/networks/config/base_cfg.yml b/data/Kuramoto/networks/config/base_cfg.yml
deleted file mode 100644
index 43faede..0000000
--- a/data/Kuramoto/networks/config/base_cfg.yml
+++ /dev/null
@@ -1,518 +0,0 @@
-# This file provides the basic configuration for the utopya Multiverse
-#
-# It is read in by the Multiverse during initialization and is subsequently
-# updated by other configuration files to generate the meta configuration of
-# the Multiverse, which determines all details of how a run is performed.
-#
-# The top-level keys here are used to configure different parts of Multiverse:
-#   - properties of the Multiverse itself: `paths`, `perform_sweep`
-#   - properties of attributes: `worker_manager`, `run_kwargs`, ...
-#   - and the parameter space that is passed on to the model instance
-#
-# NOTE that this configuration file documents some features in the comments.
-#      This cannot be exhaustive. Check the docstrings of the functions for
-#      further information.
----
-# Multiverse configuration ....................................................
-# Output paths
-paths:
-  # base output directory
-  out_dir: ~/utopya_output
-
-  # model note is added to the output directory path
-  model_note: ~
-
-  # From the two above, the run directory will be created at:
-  #     <out_dir>/<model_name>/<timestamp>-<model_note>/
-  # Subfolders will be:  config, eval, data
-
-# Control of the backup of files that belong to a simulation
-backups:
-  # Whether to save all involved config files granularly, i.e. one by one.
-  # If false, only the resulting meta_cfg is saved to the config subdirectory.
-  backup_cfg_files: true
-
-  # Whether to save the executable
-  backup_executable: false
-
-  # Whether to store git information of the project (and framework)
-  include_git_info: true
-
-# Control of the model executable
-executable_control:
-  # Whether to copy the executable to a temporary directory at the
-  # initialization of the Multiverse and execute it from there. This way,
-  # accidental changes to the executable _during_ a simulation are prevented.
-  run_from_tmpdir: true
-
-# Whether to perfom a parameter sweep
-# Is evaluated by the Multiverse.run method
-perform_sweep: false
-# NOTE This will be ignored if run_single or run_sweep are called directly.
-#      Also, the `parameter_space` key (see below) will need to span at least
-#      a volume of 1 in order to be sweep-able.
-
-# Whether to perform parameter validation
-# For large sweeps, validation can take quite some time. For such scenarios, it
-# might make sense to disable parameter validation by setting this to false.
-perform_validation: true
-
-# Parameters that are to be validated
-# This is a mapping of key sequence -> Parameter object
-parameters_to_validate: {}
-
-
-
-# Reporter ....................................................................
-# The Multiverse owns a Reporter object to report on the progress of the
-# WorkerManager. Part of its configuration happens using its init kwargs, which
-# are defined in the following.
-# The rest of the configuration happens on the WorkerManager-side (see there).
-reporter:
-  # Define report formats, which are accessible, e.g. from the WorkerManager
-  report_formats:
-    progress_bar:                     # Name of the report format specification
-      parser: progress_bar            # The parser to use
-      write_to: stdout_noreturn       # The writer to use
-      min_report_intv: 0.5            # Required time (in s) between writes
-
-      # -- All further kwargs on this level are passed to the parser
-      # Terminal width for the progress bar
-      # Can also be `adaptive` (poll each time), or `fixed` (poll once)
-      num_cols: adaptive
-
-      # The format string to use for progress information
-      # Available keys:
-      #   - `total_progress` (in %)
-      #   - `active_progress` (mean progress of _active_ simulations, in %)
-      #   - `cnt` (dict of counters: `total`, `finished`, `active`)
-      info_fstr: "{total_progress:>5.1f}% "
-      # Example of how to access counters in format string:
-      # info_fstr: "finished {cnt[finished]}/{cnt[total]} "
-
-      # Whether to show time information alongside the progress bar
-      show_times: true
-
-      # How to display time information.
-      # Available keys: `elapsed`, `est_left`, `est_end`, `start`, `now`
-      # (see `times` parser for more information)
-      times_fstr: "| {elapsed:>7s} elapsed | ~{est_left:>7s} left "
-      times_fstr_final: "| finished in {elapsed:} "
-      times_kwargs:
-        # How to compute the estimated time left to finish the work session
-        # Available modes:
-        #   - `from_start`:  extrapolates from progress made since start
-        #   - `from_buffer`: uses a buffer to store recent progress
-        #                    information and use the oldest value for
-        #                    making the estimate; see `progress_buffer_size`
-        mode: from_buffer
-
-        # Number of records kept for computing ETA in `from_buffer` mode.
-        # This is in units of parser invocations, so goes back *at least* a
-        # a time interval of `min_report_intv * progress_buffer_size`.
-        # If the reporter is called less frequently (e.g. because of a larger
-        # model-side `monitor_emit_interval`), this interval will be longer.
-        progress_buffer_size: 90
-
-    # Creates a report file containing runtime statistics
-    report_file:
-      parser: report
-      write_to:
-        file:
-          path: _report.txt
-      min_report_intv: 10             # don't update this one too often
-      min_num: 4                      # min. number of universes for statistics
-      show_individual_runtimes: true  # for large number of universes, disable
-      task_label_singular: universe
-      task_label_plural: universes
-
-    # Creates a parameter sweep information file
-    sweep_info:
-      parser: pspace_info
-      write_to:
-        file:
-          path: _sweep_info.txt
-          skip_if_empty: true
-        log:
-          lvl: 18
-          skip_if_empty: true
-      fstr: "Sweeping over the following parameter space:\n\n{sweep_info:}"
-
-  # Can define a default format to use
-  # default_format: ~
-
-
-# Worker Manager ..............................................................
-# Initialization arguments for the WorkerManager
-worker_manager:
-  # Specify how many processes work in parallel
-  num_workers: auto
-  # can be: an int, 'auto' (== #CPUs). For values <= 0: #CPUs - num_workers
-
-  # Delay between polls [seconds]
-  poll_delay: 0.05
-  # NOTE: If this value is too low, the main thread becomes very busy.
-  #       If this value is too high, the log output from simulations is not
-  #       read from the line buffer frequently enough.
-
-  # Maximum number of lines to read from each task's stream per poll cycle.
-  # Choosing a value that is too large may affect poll performance in cases
-  # where the task generates many lines of output.
-  # Set to -1 to read *all* available lines from the stream upon each poll.
-  lines_per_poll: 20
-
-  # Periodic task callback (in units of poll events). Set None to deactivate.
-  periodic_task_callback: ~
-
-  # How to react upon a simulation exiting with non-zero exit code
-  nonzero_exit_handling: raise
-  # can be: ignore, warn, warn_all, raise
-  # warn_all will also warn if the simulation was terminated by the frontend
-  # raise will lead to a SystemExit with the error code of the simulation
-
-  # How to handle keyboard interrupts
-  interrupt_params:
-    # Which signal to send to the workers
-    send_signal: SIGINT  # can be any valid signal name
-    # NOTE that only SIGINT and SIGTERM lead to a graceful shutdown on C++ side
-
-    # How long to wait for workers to shut down before calling SIGKILL on them
-    grace_period: 5.
-    # WARNING Choosing a grace period that is shorter than the duration of one
-    #         iteration step of your model might lead to corrupted HDF5 data!
-
-    # Whether to exit after working; exit code will be 128 + abs(signum)
-    exit: false
-
-  # In which events to save streams *during* the work session
-  # May be: `monitor_updated`, `periodic_callback`
-  save_streams_on: [monitor_updated]
-
-  # Reporters to invoke at different points of the WorkerManager's operation.
-  # Keys refer to events, values are lists of report format names, which can be
-  # defined via the WorkerManagerReporter (see `reporter.report_formats` above)
-  rf_spec:
-    before_working: [sweep_info]
-    while_working: [progress_bar]
-    task_spawned: [progress_bar]
-    monitor_updated: [progress_bar]
-    task_finished: [progress_bar, report_file]
-    after_work: [progress_bar, report_file]
-    after_abort: [progress_bar, report_file]
-
-
-# Configuration for the WorkerManager.start_working method
-run_kwargs:
-  # Total timeout (in s) of a run; to ignore, set to ~
-  timeout: ~
-
-  # A list of StopCondition objects to check during the run _for each worker_.
-  # The entries of the following list are OR-connected, i.e. it suffices that
-  # one is fulfilled for the corresponding worker to be stopped
-  stop_conditions: ~
-  # See docs for how to set these up:
-  #   https://docs.utopia-project.org/html/usage/run/stop-conditions.html
-
-
-# The defaults for the worker_kwargs
-# These are passed to the setup function of each WorkerTask before spawning
-worker_kwargs:
-  # Whether to save the streams of each Universe to a log file
-  save_streams: true
-  # This file is saved only after the WorkerTask has finished in order to
-  # reduce I/O operations on files
-
-  # Whether to forward the streams to stdout
-  forward_streams: in_single_run
-  # can be: true, false, or 'in_single_run' (print only in single runs)
-
-  # Whether to forward the raw stream output or only those lines that were not
-  # parsable to yaml, i.e.: only the lines that came _not_ from the monitor
-  forward_raw: true
-
-  # The log level at which the streams should be forwarded to stdout
-  streams_log_lvl: ~  # if None, uses print instead of the logging module
-
-  # Arguments to subprocess.Popen
-  popen_kwargs:
-    # The encoding of the streams (STDOUT, STDERR) coming from the simulation.
-    # NOTE If your locale is set to some other encoding, or the simulation uses
-    #      a custom one, overwrite this value accordingly via the user config.
-    encoding: utf8
-
-
-# Cluster mode configuration ..................................................
-# Whether cluster mode is enabled
-cluster_mode: false
-
-# Parameters to configure the cluster mode
-cluster_params:
-  # Specify the workload manager to use.
-  # The names of environment variables are chosen accordingly.
-  manager: slurm   # available:  slurm
-
-  # The environment to look for parameters in. If not given, uses os.environ
-  env: ~
-
-  # Specify the name of environment variables for each supported manager
-  # The resolved values are available at the top level of the dict that is
-  # returned by Multiverse.resolved_cluster_params
-  env_var_names:
-    slurm:
-      # --- Required variables ---
-      # ID of the job
-      job_id: SLURM_JOB_ID
-
-      # Number of available nodes
-      num_nodes: SLURM_JOB_NUM_NODES
-
-      # List of node names
-      node_list: SLURM_JOB_NODELIST
-
-      # Name of the current node
-      node_name: SLURMD_NODENAME  # sic!
-
-      # This is used for the name of the run
-      timestamp: RUN_TIMESTAMP
-
-      # --- Optional values ---
-      # Name of the job
-      job_name: SLURM_JOB_NAME
-
-      # Account from which the job is run
-      job_account: SLURM_JOB_ACCOUNT
-
-      # Number of processes on current node
-      num_procs: SLURM_CPUS_ON_NODE
-
-      # Cluster name
-      cluster_name: SLURM_CLUSTER_NAME
-
-      # Custom output directory
-      custom_out_dir: UTOPIA_CLUSTER_MODE_OUT_DIR
-
-    # Could have more managers here, e.g.: docker
-
-  # Which parser to use to extract node names from node list
-  node_list_parser_params:
-    slurm: condensed  # e.g.: node[002,004-011,016]
-
-  # Which additional info to include into the name of the run directory, i.e.
-  # after the timestamp and before the model directory. All information that
-  # is extracted from the environment variables is available as keyword
-  # argument to format. Should be a sequence of format strings.
-  additional_run_dir_fstrs: [ "job{job_id:}" ]
-
-
-# Data Manager ................................................................
-# The DataManager takes care of loading the data into a tree-like structure
-# after the simulations are finished.
-# It is based on the DataManager class from the dantro package. See there for
-# full documentation.
-data_manager:
-  # Where to create the output directory for this DataManager, relative to
-  # the run directory of the Multiverse.
-  out_dir: eval/{timestamp:}
-  # The {timestamp:} placeholder is replaced by the current timestamp such that
-  # future DataManager instances that operate on the same data directory do
-  # not create collisions.
-  # Directories are created recursively, if they do not exist.
-
-  # Define the structure of the data tree beforehand; this allows to specify
-  # the types of groups before content is loaded into them.
-  # NOTE The strings given to the Cls argument are mapped to a type using a
-  #      class variable of the DataManager
-  create_groups:
-    - path: multiverse
-      Cls: MultiverseGroup
-
-  # Where the default tree cache file is located relative to the data
-  # directory. This is used when calling DataManager.dump and .restore without
-  # any arguments, as done e.g. in the Utopia CLI.
-  default_tree_cache_path: data/.tree_cache.d3
-
-  # Supply a default load configuration for the DataManager
-  # This can then be invoked using the dm.load_from_cfg() method.
-  load_cfg:
-    # Load the frontend configuration files from the config/ directory
-    # Each file refers to a level of the configuration that is supplied to
-    # the Multiverse: base <- user <- model <- run <- update
-    cfg:
-      loader: yaml                          # The loader function to use
-      glob_str: 'config/*.yml'              # Which files to load
-      ignore:                               # Which files to ignore
-        - config/parameter_space.yml
-        - config/parameter_space_info.yml
-        - config/full_parameter_space.yml
-        - config/full_parameter_space_info.yml
-        - config/git_info_project.yml
-        - config/git_info_framework.yml
-      required: true                        # Whether these files are required
-      path_regex: config/(\w+)_cfg.yml      # Extract info from the file path
-      target_path: cfg/{match:}             # ...and use in target path
-
-    # Load the parameter space object into the MultiverseGroup attributes
-    pspace:
-      loader: yaml_to_object                # Load into ObjectContainer
-      glob_str: config/parameter_space.yml
-      required: true
-      load_as_attr: true
-      unpack_data: true                     # ... and store as ParamSpace obj.
-      target_path: multiverse
-
-    # Load the configuration files that are generated for _each_ simulation
-    # These hold all information that is available to a single simulation and
-    # are in an explicit, human-readable form.
-    uni_cfg:
-      loader: yaml
-      glob_str: data/uni*/config.yml
-      required: true
-      path_regex: data/uni(\d+)/config.yml
-      target_path: multiverse/{match:}/cfg
-      parallel:
-        enabled: true
-        min_files: 1000
-        min_total_size: 1048576  # 1 MiB
-
-    # Example: Load the binary output data from each simulation.
-    # data:
-    #   loader: hdf5_proxy
-    #   glob_str: data/uni*/data.h5
-    #   required: true
-    #   path_regex: data/uni(\d+)/data.h5
-    #   target_path: multiverse/{match:}/data
-    #   enable_mapping: true   # see DataManager for content -> type mapping
-
-    #   # Options for loading data in parallel (speeds up CPU-limited loading)
-    #   parallel:
-    #     enabled: false
-
-    #     # Number of processes to use; negative is deduced from os.cpu_count()
-    #     processes: ~
-
-    #     # Threshold values for parallel loading; if any is below these
-    #     # numbers, loading will *not* be in parallel.
-    #     min_files: 5
-    #     min_total_size: 104857600  # 100 MiB
-
-    # The resulting data tree is then:
-    #  └┬ cfg
-    #     └┬ base
-    #      ├ meta
-    #      ├ model
-    #      ├ run
-    #      └ update
-    #   └ multiverse
-    #     └┬ 0
-    #        └┬ cfg
-    #         └ data
-    #           └─ ...
-    #      ├ 1
-    #      ...
-
-
-# Plot Manager ................................................................
-# The PlotManager, also from the dantro package, supplies plotting capabilities
-# using the data in the DataManager.
-plot_manager:
-  # Save the plots to the same directory as that of the data manager
-  out_dir: ""
-
-  # Whether to raise exceptions for plotting errors. false: only log them
-  raise_exc: false
-
-  # How to handle already existing plot configuration files
-  cfg_exists_action: raise
-  # NOTE If in cluster mode, this value is set to 'skip' by the Multiverse
-
-  # Save all plot configurations alongside the plots
-  save_plot_cfg: true
-
-  # Include dantro's base plot configuration pool
-  use_dantro_base_cfg_pool: true
-
-  # Base plot configuration pools
-  # These specify the base plot configurations that are made available for each
-  # model run, updated and extended in the order specified here and themselves
-  # based on the dantro base config pool.
-  #
-  # In some cases, defining additional pools can be useful, e.g. to generate
-  # publication-ready output without redundantly defining plots or styles.
-  #
-  # This is expected to be a list of 2-tuples in form (name, dict or path).
-  # If the second entry is a string, it may be a format string and it will have
-  # access to `model_name` and the model's `paths` dict.
-  # If there is no file available at the given path, will warn about it and use
-  # an empty pool for that entry.
-  #
-  # There are some special keys, which can be used instead of the 2-tuple:
-  #   `utopya_base`, `framework_base`, `project_base`, `model_base`
-  # These expand to a respective configuration file path, depending on the
-  # framework, project, or model that is being used.
-  base_cfg_pools:
-    - utopya_base
-    - framework_base
-    - project_base
-    - model_base
-
-  # Initialization arguments for all creators
-  shared_creator_init_kwargs:
-    style:
-      figure.figsize: [8., 5.]  # 16:10
-
-  # Can set creator-specific initialization arguments here
-  creator_init_kwargs:
-    pyplot: {}
-    universe: {}
-    multiverse: {}
-
-
-# Parameter Space .............................................................
-# Only entries below this one will be available to the model executable.
-#
-# The content of the `parameter_space` level is parsed by the frontend and then
-# dumped to a file, the path to which is passed to the binary as positional
-# argument.
-parameter_space:
-  # Set a default PRNG seed
-  seed: 42
-
-  # Number of steps to perform
-  num_steps: 3
-
-  # At which step the write_data method should be invoked for the first time
-  write_start: 0
-
-  # Starting from write_start, how frequently write_data should be called
-  write_every: 1
-  # NOTE `write_start` and `write_every` are passed along to sub-models. Every
-  #       sub model can overwrite this entry by adding an entry in their model
-  #       configuration level (analogous to `log_levels`.)
-
-  # Log levels
-  # NOTE The framework may define further levels in here but may also choose
-  #      to ignore these entries altogether. The `model` and `backend` keys
-  #      are those that are accessible from the utopya CLI.
-  log_levels:
-    model: info
-
-    backend: warning
-    # TODO Implement setting this via CLI… perhaps even more general?
-    #      Coolest would be: allow frameworks to provide a mapping of each CLI
-    #      debug level to an update dict.
-
-  # Monitoring
-  # How frequently to send a monitoring message to the frontend; note that the
-  # timing needs to be implemented by the model itself
-  monitor_emit_interval: 2.
-
-  # The path to the config file to load
-  # output_path: /abs/path/to/uni<#>/cfg.yml
-  # NOTE This entry is always added by the frontend. Depending on which
-  #      universe is to be simulated, the <#> is set.
-
-  # Below here, the model configuration starts, i.e. the config that is used by
-  # a model instance. It's meant to be nested under the model name itself and
-  # a node of that name will always be added.
-  # <model_name>:
-    # ... more parameters ...
diff --git a/data/Kuramoto/networks/config/full_parameter_space.yml b/data/Kuramoto/networks/config/full_parameter_space.yml
deleted file mode 100644
index ea53175..0000000
--- a/data/Kuramoto/networks/config/full_parameter_space.yml
+++ /dev/null
@@ -1,45 +0,0 @@
----
-!pspace
-Kuramoto:
-  Data:
-    synthetic_data:
-      N: !pdim
-        as_type: null
-        assert_unique: true
-        default: 16
-        values: [16, 100, 500, 1000]
-      dt: 0.01
-      network:
-        graph_props:
-          WattsStrogatz: {p_rewire: 0.2}
-          is_directed: false
-          is_weighted: true
-        mean_degree: !coupled-pdim
-          as_type: null
-          target_name: N
-          values: [6, 20, 40, 150]
-        type: random
-      num_steps: 1
-      sigma: 0.0
-      training_set_size: 1
-  NeuralNet:
-    activation_funcs: {default: linear}
-    biases: {default: null}
-    learning_rate: 0.002
-    nodes_per_layer: {default: 20}
-    num_layers: 1
-  Training:
-    batch_size: 1
-    device: cpu
-    loss_function: {name: MSELoss}
-    true_parameters: {sigma: 0.0}
-log_levels: {backend: warning, model: info}
-monitor_emit_interval: 2.0
-num_epochs: 0
-num_steps: 3
-root_model_name: Kuramoto
-seed: 1
-write_every: 1
-write_predictions_every: -1
-write_start: 1
-
diff --git a/data/Kuramoto/networks/config/full_parameter_space_info.yml b/data/Kuramoto/networks/config/full_parameter_space_info.yml
deleted file mode 100644
index 03748b5..0000000
--- a/data/Kuramoto/networks/config/full_parameter_space_info.yml
+++ /dev/null
@@ -1,16 +0,0 @@
----
-coupled_dims:
-- full_path: [Kuramoto, Data, synthetic_data, network, mean_degree]
-  name: mean_degree
-  target_name: N
-  values: [6, 20, 40, 150]
-dims:
-- full_path: [Kuramoto, Data, synthetic_data, N]
-  name: N
-  values: [16, 100, 500, 1000]
-num_coupled_dims: 1
-num_dims: 1
-perform_sweep: true
-shape: [4]
-volume: 4
-
diff --git a/data/Kuramoto/networks/config/git_diff_project.patch b/data/Kuramoto/networks/config/git_diff_project.patch
deleted file mode 100644
index d6f917b..0000000
--- a/data/Kuramoto/networks/config/git_diff_project.patch
+++ /dev/null
@@ -1,1347 +0,0 @@
-diff --git a/model_plots/HarrisWilson/prob_density.py b/model_plots/HarrisWilson/prob_density.py
-index 1398a56..65688a4 100644
---- a/model_plots/HarrisWilson/prob_density.py
-+++ b/model_plots/HarrisWilson/prob_density.py
-@@ -94,7 +94,7 @@ def plot_prob_density(
-             fb = hlpr.ax.fill_between(dset[x], y_vals_lower, y_vals_upper, linewidth=0, **plot_kwargs)
- 
-         if label is not None:
--            _handles.append(ebar[0] if yerr is None else (ebar, fb))
-+            _handles.append(ebar if yerr is None else (ebar, fb))
-             _labels.append(label)
- 
-     hlpr.track_handles_labels(_handles, _labels)
-diff --git a/model_plots/data_ops.py b/model_plots/data_ops.py
-index cd7c757..8160584 100644
---- a/model_plots/data_ops.py
-+++ b/model_plots/data_ops.py
-@@ -288,7 +288,7 @@ def marginal_of_density(vals: xr.DataArray, loss: xr.DataArray, *, coords: dict
-     hist_data = np.reshape(hist_data, (-1, n_bins))
-     n_samples = np.shape(hist_data)[0]
- 
--    loss_data = np.repeat(np.reshape(loss.data, (n_samples, 1)), n_bins, 1)
-+    loss_data = np.repeat(np.reshape(np.exp(-loss.data), (n_samples, 1)), n_bins, 1)
-     loss_data = loss_data / np.sum(loss_data, axis=0)
- 
-     # Calculate the mean of each bin
-@@ -303,9 +303,13 @@ def marginal_of_density(vals: xr.DataArray, loss: xr.DataArray, *, coords: dict
-     std = std/(np.sum(means) * dx)
-     coords.update(dict(bin_idx=np.arange(n_bins)))
- 
-+    MLE = hist_data[np.argmin(np.reshape(loss.data, (n_samples, 1)), axis=0)[0]]
-+    MLE = MLE / (np.sum(MLE) * dx)
-+
-     return xr.Dataset(
-         data_vars=dict(bin_center=("bin_idx", vals.coords['bin_center'].data),
-                        y=("bin_idx", means),
--                       yerr=("bin_idx", std)),
-+                       yerr=("bin_idx", std),
-+                       MLE=("bin_idx", MLE)),
-         coords=coords
-     )
-\ No newline at end of file
-diff --git a/models/Kuramoto/Kuramoto_base_plots.yml b/models/Kuramoto/Kuramoto_base_plots.yml
-index 07195e2..1dbe822 100644
---- a/models/Kuramoto/Kuramoto_base_plots.yml
-+++ b/models/Kuramoto/Kuramoto_base_plots.yml
-@@ -127,6 +127,12 @@
-     - .plot.multiplot
-   compute_only: []
- 
-+.multiplot_multiverse:
-+  based_on:
-+    - .creator.multiverse
-+    - .plot.multiplot
-+  compute_only: []
-+
- .matrix:
-   based_on:
-     - .creator.universe
-@@ -140,6 +146,59 @@
-       1: *darkblue
-     continuous: true
- 
-+.plot.prob_density:
-+  module: model_plots.HarrisWilson
-+  plot_func: plot_prob_density
-+
-+.marginals_density:
-+  dag_options:
-+    define:
-+      n_bins: 100
-+      min_bin: -1
-+      max_bin: 16
-+      bw_method: 0.5
-+      sigma: 6
-+  to_plot:
-+    - function: [model_plots.HarrisWilson, plot_prob_density]
-+      args: [!dag_result data]
-+      x: bin_center
-+      y: y
-+      yerr: yerr
-+      label: $\hat{P}(k)$
-+      smooth_kwargs:
-+        enabled: True
-+        sigma: !dag_result sigma
-+      pass_helper: true
-+      color: *darkblue
-+    - function: [model_plots.HarrisWilson, plot_prob_density]
-+      args: [!dag_result data]
-+      x: bin_center
-+      y: MLE
-+      smooth_kwargs:
-+        enabled: True
-+        sigma: !dag_result sigma
-+      pass_helper: true
-+      color: black
-+      linestyle: dotted
-+      label: MLE
-+    - function: [ seaborn, kdeplot ]
-+      args: [ !dag_result true_param ]
-+      gridsize: !dag_result n_bins
-+      bw_method: !dag_result bw_method
-+      label: $P(k)$
-+      clip: [0, ~]
-+      linestyle: dotted
-+      color: *red
-+  helpers:
-+    set_legend:
-+      use_legend: true
-+    set_tick_locators:
-+      x: &formatting
-+        major:
-+          name: MaxNLocator
-+          integer: true
-+          nbins: !dag_result num_agents
-+
- # ======================================================================================================================
- #  ╦ ╦╔╗╔╦╦  ╦╔═╗╦═╗╔═╗╔═╗  ╔═╗╦  ╔═╗╔╦╗╔═╗
- #  ║ ║║║║║╚╗╔╝║╣ ╠╦╝╚═╗║╣   ╠═╝║  ║ ║ ║ ╚═╗
-@@ -279,10 +338,8 @@ clustering_over_time:
-         out_degree: data/predicted_network/_out_degree_weighted
-         triangles: data/predicted_network/_weighted_triangles
-       tag: clustering_weighted
--    - operation: pd.Index
--      args: [ [ 'clustering coefficient', 'weighted clustering coefficient'] ]
--      kwargs:
--        name: 'type'
-+    - pd.Index: [ [ 'clustering coefficient', 'weighted clustering coefficient'] ]
-+      kwargs: {name: 'type'}
-     - xr.concat: [[!dag_tag clustering, !dag_tag clustering_weighted], !dag_prev ]
-     - .mean: [!dag_prev , 'vertex_idx']
-     - div: [!dag_prev , 3]
-@@ -479,11 +536,8 @@ undirected_adjacency_matrix:
-       x: [ -1, !dag_result num_agents ]
-       y: [ !dag_result num_agents, -1 ]
-     set_tick_locators:
--      x: &formatting
--        major:
--          name: MaxNLocator
--          integer: true
--          nbins: !dag_result num_agents
-+      x:
-+        <<: *formatting
-       y:
-         <<: *formatting
-   cmap:
-@@ -560,10 +614,11 @@ accuracy_on_false_edges:
-     - xr.where: [!dag_prev ^= 0, !dag_tag l1_accuracy, 0]
-       tag: data
- 
--marginals_uni:
--  based_on: .creator.universe
--  module: model_plots.HarrisWilson
--  plot_func: plot_prob_density
-+# Marginal density on a parameter
-+marginals_parameter_uni:
-+  based_on:
-+    - .creator.universe
-+    - .plot.prob_density
-   select:
-     param:
-       path: predicted_network/_in_degree
-@@ -597,156 +652,39 @@ marginals_uni:
-           linestyle: dashed
-           color: gray
- 
-+# Marginal density on a distribution
- marginals_density_uni:
-   based_on:
-     - .multiplot_universe
--  dag_options:
--    define:
--      n_bins: 100
--      min_bin: -1
--      max_bin: 16
--      bw_method: 0.5
--      sigma: 6
-+    - .marginals_density
-   select:
-     num_agents:
-       path: true_network/_vertices
-       transform:
-         - .coords: [!dag_prev , 'vertex_idx']
-         - len: [!dag_prev ]
--    coords:
--      path: predicted_network/_in_degree
--      transform:
--        - .coords: [!dag_prev , 'time']
--        - .isel: [!dag_prev , {time: !slice [~, -2]}]
--    sel_idx:
--      path: output_data/loss
--      transform:
--        - .idxmin: [!dag_prev ]
--          kwargs: {dim: time}
--        - .values: [!dag_prev ]
--        - .item: [!dag_prev ]
--    param:
--      path: predicted_network/_in_degree
--      transform:
-     param_binned:
-       path: predicted_network/_in_degree
-       transform:
--        - .isel: [!dag_prev , {time: !slice [~, -2]}]
-         - np.linspace: [!dag_tag min_bin, !dag_tag max_bin, !dag_tag n_bins]
-         - .data: [ !dag_node -2 ]
-         - NeuralABM.hist: [!dag_prev ]
-           kwargs:
-             density: true
-             bins: !dag_node -2
--    loss:
--      path: output_data/loss
--      transform:
--        - np.maximum: [ !dag_prev , *loss_limit ]
--        - log10: [ !dag_prev ]
--        - mul: [ !dag_prev , -1 ]
--        - np.exp: [!dag_prev ]
-     true_param:
-       path: true_network/_degree
-       transform:
--        - .isel: [!dag_prev , {time: -1}]
--          kwargs:
--            drop: True
-         - .data: [!dag_prev ]
-+    loss:
-+      path: output_data/loss
-+      transform:
-+        - np.maximum: [ !dag_prev , *loss_limit ]
-   transform:
--    - .sel: [!dag_tag loss , {time: !dag_tag coords }]
-     - NeuralABM.marginal_of_density: [!dag_tag param_binned ]
-       kwargs:
-         loss: !dag_prev
-       tag: data
--    - .sel: [!dag_tag param , {time: !dag_tag sel_idx }]
--      kwargs: {drop: true}
--      tag: MLE
--  to_plot:
--    - function: [model_plots.HarrisWilson, plot_prob_density]
--      args: [!dag_result data]
--      x: bin_center
--      y: y
--      yerr: yerr
--      label: $\hat{P}(k)$
--      smooth_kwargs:
--        enabled: True
--        sigma: !dag_result sigma
--      pass_helper: true
--      color: *darkblue
--    - function: [ seaborn, kdeplot ]
--      args: [ !dag_result MLE ]
--      gridsize: !dag_result n_bins
--      color: black
--      bw_method: !dag_result bw_method
--      label: MLE
--      legend: true
--      clip: [0, ~]
--    - function: [ seaborn, kdeplot ]
--      args: [ !dag_result true_param ]
--      gridsize: !dag_result n_bins
--      bw_method: !dag_result bw_method
--      label: $P(k)$
--      clip: [0, ~]
--      linestyle: dotted
--      color: *red
--
--
--  helpers:
--    set_legend:
--      use_legend: true
--    set_tick_locators:
--      x:
--        <<: *formatting
--
--
--marginals_density_mv:
--  based_on:
--    - .creator.multiverse
--  module: model_plots.HarrisWilson
--  plot_func: plot_prob_density
--  select_and_combine:
--    fields:
--      coords:
--        path: predicted_network/_in_degree
--        transform:
--          - .coords: [!dag_prev , 'time']
--          - .isel: [!dag_prev , {time: !slice [~, -2]}]
--      degrees:
--        path: predicted_network/_in_degree
--        transform:
--          - .isel: [!dag_prev , {time: !slice [~, -2]}]
--          - .coords: [!dag_prev , 'vertex_idx']
--          - len: [!dag_prev ]
--          - np.linspace: [0, !dag_prev , 10]
--          - .data: [ !dag_node -4 ]
--          - NeuralABM.hist: [!dag_prev ]
--            kwargs: {bins: !dag_node -2 }
--      loss:
--        path: output_data/loss
--        transform:
--          - np.maximum: [ !dag_prev , *loss_limit ]
--          - log10: [ !dag_prev ]
--          - mul: [ !dag_prev , -1 ]
--          - np.exp: [!dag_prev ]
--  transform:
--    - .isel: [!dag_tag coords, {seed: 0, num_layers: 0}]
--      kwargs: {drop: true}
--    - .sel: [!dag_tag loss , {time: !dag_prev }]
--    - NeuralABM.marginal_of_density: [!dag_tag degrees ]
--      kwargs:
--        loss: !dag_prev
--        along_dim: 'num_layers'
--      tag: data
--  x: bin_center
--  y: y
--  yerr: yerr
--  hue: num_layers
--  helpers:
--    set_legend:
--      use_legend: true
--  smooth_kwargs:
--    enabled: True
--    sigma: 2
- 
- # ======================================================================================================================
- #  ╔╦╗╦ ╦╦ ╔╦╗╦╦  ╦╔═╗╦═╗╔═╗╔═╗  ╔═╗╦  ╔═╗╔╦╗╔═╗
-@@ -770,3 +708,46 @@ loss_stacked:
-       y: log
-     set_labels:
-       y: Training loss $J$
-+
-+marginals_density_mv:
-+  based_on:
-+    - .multiplot_multiverse
-+    - .marginals_density
-+  select_and_combine:
-+    fields:
-+      param_binned:
-+        path: predicted_network/_in_degree
-+        transform:
-+          - np.linspace: [!dag_tag min_bin, !dag_tag max_bin, !dag_tag n_bins]
-+          - .data: [ !dag_node -2 ]
-+          - NeuralABM.hist: [!dag_prev ]
-+            kwargs: {bins: !dag_node -2}
-+      true_val:
-+        path: true_network/_degree
-+        transform:
-+          - .isel: [ !dag_prev , { time: -1 } ]
-+            kwargs:
-+              drop: True
-+          - .data: [ !dag_prev ]
-+        subspace:
-+          seed: 0
-+      loss:
-+        path: output_data/loss
-+        transform:
-+          - np.maximum: [ !dag_prev , *loss_limit ]
-+      N:
-+        path: true_network/_vertices
-+        transform:
-+          - .coords: [ !dag_prev , 'vertex_idx' ]
-+          - len: [ !dag_prev ]
-+        subspace:
-+          seed: 0
-+  transform:
-+    - NeuralABM.marginal_of_density: [!dag_tag param_binned ]
-+      kwargs:
-+        loss: !dag_tag loss
-+      tag: data
-+    - .squeeze: [!dag_tag true_val]
-+      tag: true_param
-+    - .squeeze: [!dag_tag N]
-+      tag: num_agents
-\ No newline at end of file
-diff --git a/models/Kuramoto/Kuramoto_cfg.yml b/models/Kuramoto/Kuramoto_cfg.yml
-index de33357..2483f51 100644
---- a/models/Kuramoto/Kuramoto_cfg.yml
-+++ b/models/Kuramoto/Kuramoto_cfg.yml
-@@ -35,6 +35,3 @@ Training:
-   true_parameters:
-     sigma: *sigma
-   device: cpu
--
--# Whether to write out the training time
--write_time: False
-diff --git a/models/Kuramoto/cfgs/Data_generation/network.yml b/models/Kuramoto/cfgs/Data_generation/network.yml
-index e69de29..2f8ed0b 100644
---- a/models/Kuramoto/cfgs/Data_generation/network.yml
-+++ b/models/Kuramoto/cfgs/Data_generation/network.yml
-@@ -0,0 +1,25 @@
-+---
-+perform_sweep: True
-+paths:
-+  model_note: networks
-+parameter_space:
-+  seed: 1
-+  num_epochs: 0
-+  write_every: 1
-+  write_predictions_every: -1
-+  Kuramoto:
-+    Data:
-+      synthetic_data:
-+        num_steps: 1
-+        training_set_size: 1
-+        N: !sweep
-+          default: 16
-+          values: [16, 100, 500, 1000]
-+        network:
-+          type: random
-+          mean_degree: !coupled-sweep
-+            target_name: N
-+            values: [6, 20, 40, 150]
-+          graph_props:
-+            is_directed: False
-+
-diff --git a/models/Kuramoto/cfgs/Noisy_run/eval.yml b/models/Kuramoto/cfgs/Noisy_run/eval.yml
-index 2080597..34bfad3 100644
---- a/models/Kuramoto/cfgs/Noisy_run/eval.yml
-+++ b/models/Kuramoto/cfgs/Noisy_run/eval.yml
-@@ -12,47 +12,191 @@
- 
-   # Page widths in inches for latex documents: ensures easy integration into latex documents
-   page_widths:
--    full_width:         &full_width         7.5
--    half_width:         &half_width         !expr 7.5 / 2
--    two_thirds_width:   &two_thirds_width   !expr 2 * 7.5 / 3
--    third_width:        &third_width        !expr 7.5 / 3
--    quarter_width:      &quarter_width      !expr 7.5 / 4
--    fifth_width:        &fifth_width        !expr 7.5 / 5
--    eighth_width:       &eighth_width       !expr 7.5 / 8
-+    full_width:     &full_width         7.5
-+    half_width:     &half_width         !expr 7.5 / 2
-+    third_width:    &third_width        !expr 7.5 / 3
-+    quarter_width:  &quarter_width      !expr 7.5 / 4
-+    fifth_width:    &fifth_width        !expr 7.5 / 5
-+    eighth_width:    &eighth_width      !expr 7.5 / 8
- 
--.matrix_defaults:
-+# ======================================================================================================================
-+
-+# Training loss
-+loss:
-+  based_on: loss_stacked
-+  transform:
-+    - .mean: [!dag_tag values, 'seed']
-+    - .assign_coords: [!dag_prev , {sigma: [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1]}]
-+      tag: data
-+  x: time
-+  hue: sigma
-+  helpers:
-+    set_scales:
-+      y: log
-+    set_legend:
-+      custom_labels: [$1e-5$, $5e-5$, $1e-4$, $5e-4$, $1e-3$, $5e-3$, $1e-2$, $5e-2$, $1e-1$, $5e-1$]
-+      ncol: 2
-   style:
--    figure.figsize: [ *third_width, *third_width ]
--    axes.grid: False
--    axes.spines.top: True
--    axes.spines.right: True
--    axes.linewidth: 0.5
--  vmax: 1
-+    figure.figsize: [*half_width, *third_width]
-+
-+# Frobenius error
-+frobenius_loss:
-+  based_on: loss
-+  select_and_combine:
-+    fields:
-+      values: output_data/frobenius_error
-   helpers:
--    set_title:
--      title: ''
--    set_ticks:
--      x:
--        major: []
--      y:
--        major: []
-     set_labels:
--      x: ' '
--      y: ' '
--    set_limits:
--      y: [max, min]
--      x: [min, max]
-+      y: Frobenius error $\Vert \hat{\mathbf{A}} - \mathbf{A} \Vert_\mathrm{Fr}$
-+    set_legend:
-+      use_legend: False
- 
--graphs/true_graph:
--  based_on: graph
-+# Network size comparison
-+network_density:
-+  based_on: .errorbands
-+  select_and_combine:
-+    fields:
-+      sizes:
-+        path: output_data/network_size
-+        transform:
-+          - .isel: [!dag_prev , {time: -1}]
-+          - div: [!dag_prev , 240]
-+      true_size:
-+        path: true_network/_edge_weights
-+        transform:
-+          - .isel: [!dag_prev , {time: -1}]
-+            kwargs: { drop: true }
-+          - len: [ !dag_node -1 ]
-+          - mul: [ !dag_node -1, 2 ]
-+          - div: [!dag_prev , 240]
-+  transform:
-+    - .assign_coords: [!dag_tag sizes , {sigma: [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1]}]
-+    - .rename: [!dag_prev , {sigma: Noise}]
-+      tag: sizes_reassigned
-+    - .mean: [ !dag_prev ]
-+      kwargs: { dim: seed }
-+      tag: means
-+    - .std: [ !dag_tag sizes_reassigned ]
-+      kwargs: { dim: seed }
-+    - xr.Dataset:
-+      - y: !dag_tag means
-+        yerr: !dag_prev
-+      tag: data
-+    - .isel: [!dag_tag true_size , { seed: -1, sigma: -1 }]
-+      tag: nw_size
-+  helpers:
-+    set_hv_lines:
-+      hlines:
-+        - pos: !dag_result nw_size
-+          color: *red
-+          label: $\vert E \vert$
-+          linestyle: dotted
-+    set_labels:
-+      y: Predicted network density
-+    set_scales:
-+      x: log
-+  x: Noise
-+  style:
-+    figure.figsize: [ *half_width, *third_width ]
- 
--graphs/prediction:
--  based_on: graph
--  select:
--    graph_group: predicted_network
-+# Plot the average predicted weight on false edges
-+accuracy_on_false_edges:
-+  based_on: .errorbands
-+  select_and_combine:
-+    fields:
-+      prediction:
-+        path: output_data/predictions
-+        transform:
-+          - .isel: [!dag_prev , {time: -1}]
-+            kwargs: {drop: true}
-+      true_values:
-+        path: true_network/_adjacency_matrix
-+        transform: [.data]
-+  transform:
-+    - sub: [ !dag_tag prediction, !dag_tag true_values ]
-+    - print: [!dag_prev ]
-+    - np.abs: [ !dag_prev ]
-+      tag: l1_accuracy
-+    - ==: [!dag_tag true_values, 0]
-+    - xr.where: [!dag_prev , 1, 0]
-+    - mul: [!dag_tag prediction, !dag_prev ]
-+    - xr.where: [!dag_prev ^= 0, !dag_tag l1_accuracy, 0]
-+    - .assign_coords: [!dag_prev , {sigma: [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1]}]
-+    - .rename: [!dag_prev , {sigma: Noise}]
-+      tag: accuracies_reassigned
-+    - .mean: [!dag_prev , ['seed', 'i', 'j'] ]
-+      tag: means
-+    - .std: [!dag_tag accuracies_reassigned, ['seed', 'i', 'j'] ]
-+    - xr.Dataset:
-+      - y: !dag_tag means
-+        yerr: !dag_prev
-+    - print: [!dag_prev ]
-+      tag: data
-+  x: Noise
-+  style:
-+    figure.figsize: [ *half_width, *third_width ]
-+  helpers:
-+    set_scales:
-+      x: log
-+      y: log
-+    set_labels:
-+      y: Average edge weight on $E_\wedge$
- 
--marginals/degree_distribution:
--  based_on: marginals_density_uni
-+loss_compared:
-+  based_on: .line_multiverse
-+  select_and_combine:
-+    fields:
-+      training_loss:
-+        path: output_data/loss
-+        transform:
-+          - .isel: [!dag_prev , {time: -1}]
-+            kwargs: {drop: true}
-+      frobenius_loss:
-+        path: output_data/frobenius_error
-+        transform:
-+          - .isel: [!dag_prev , {time: -1}]
-+            kwargs: {drop: true}
-+  transform:
-+    - pd.Index: [ [ 'training loss', 'frobenius error'] ]
-+      kwargs: {name: 'loss type'}
-+    - xr.concat: [[!dag_tag training_loss, !dag_tag frobenius_loss], !dag_prev ]
-+    - .mean: [!dag_prev , 'seed']
-+    - .assign_coords: [ !dag_prev , { sigma: [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1 ] } ]
-+      tag: data
-+  x: sigma
-+  hue: loss type
-+  style:
-+    figure.figsize: [ *half_width, *third_width ]
-+  helpers:
-+    set_scales:
-+      x: log
-+      y: log
-+    set_labels:
-+      x: $\sigma$
-+      y: Final loss
-+
-+degree_distribution: !pspace
-+  based_on: marginals_density_mv
-+  select_and_combine:
-+    fields:
-+      N:
-+        subspace:
-+          sigma: /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni01/data.h5
-+    subspace:
-+      sigma: !sweep
-+        default: /Users/thomasgaskin/utopya_output/Kuramoto/training_data/data/uni06/data.h5
-+        values: [
-+          /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni01/data.h5,
-+          /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni02/data.h5,
-+          /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni03/data.h5,
-+          /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni04/data.h5,
-+          /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni05/data.h5,
-+          /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni06/data.h5,
-+          /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni07/data.h5,
-+          /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni08/data.h5,
-+          /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni09/data.h5,
-+          /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni10/data.h5,
-+        ]
-   dag_options:
-     define:
-       n_bins: 200
-@@ -66,66 +210,27 @@ marginals/degree_distribution:
-   style:
-     figure.figsize: [*half_width, *third_width]
- 
--marginals/clustering:
--  based_on: marginals_density_uni
--  dag_options:
--    define:
--      n_bins: 1000
--      min_bin: -8
--      max_bin: 257
--      bw_method: 0.2
--      sigma: 4
--  select:
--    coords:
--      path: predicted_network/_triangles
--    param:
--      path: predicted_network/_triangles
--    param_binned:
--      path: predicted_network/_triangles
--    true_param:
--      path: true_network/_triangles
--  helpers:
--    set_title:
--      title: Triangle distribution $P(c)$
--    set_limits:
--      x: [0, 40]
--    set_labels:
--      y: ' '
--  style:
--    figure.figsize: [ *half_width, *third_width ]
--
--matrices/comparison:
--  based_on:
--    - .matrix
--    - .matrix_defaults
--  select:
--    true_matrix:
--      path: true_network/_adjacency_matrix
--      transform: [.data ]
--    predicted_matrix:
--      path: output_data/predictions
--      transform:
--        - .isel: [ !dag_prev , { time: -1 } ]
--          kwargs: {drop: true}
-+mean_degree:
-+  based_on: .errorbands
-+  select_and_combine:
-+    fields:
-+      degree:
-+        path: predicted_network/_in_degree
-+        transform:
-+          - .isel: [!dag_prev , {time: -1}]
-   transform:
--    - pd.Index: [ [ 'true', 'predicted' ] ]
--      kwargs: {name: 'kind'}
--    - xr.concat: [ [ !dag_tag true_matrix, !dag_tag predicted_matrix ], !dag_prev ]
-+    - .mean: [!dag_tag degree , ['vertex_idx', 'seed']]
-+      tag: mean
-+    - .std: [!dag_tag degree , ['vertex_idx', 'seed']]
-+    - xr.Dataset:
-+      - y: !dag_tag mean
-+        yerr: !dag_prev
-+    - .assign_coords: [ !dag_prev , { sigma: [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1 ] } ]
-       tag: data
--  col: kind
--  aspect: ~
--  size: ~
--  figsize: [*two_thirds_width, *third_width]
--  cbar_kwargs:
--    label: ~
--
--matrices/error:
--  based_on:
--    - accuracy
--    - .matrix_defaults
--  norm:
--    name: LogNorm
--  vmin: 1e-7
--  vmax: ~
--  cbar_kwargs:
--    label: $\Vert \hat{\mathbf{A}} - \mathbf{A} \Vert_1$
-+  y: y
-+  yerr: yerr
-+  helpers:
-+    set_scales:
-+      x: log
-+  style:
-+    figure.figsize: [ *half_width, *third_width ]
-\ No newline at end of file
-diff --git a/models/Kuramoto/cfgs/Noisy_run/run.yml b/models/Kuramoto/cfgs/Noisy_run/run.yml
-index 20af6c2..e76f588 100644
---- a/models/Kuramoto/cfgs/Noisy_run/run.yml
-+++ b/models/Kuramoto/cfgs/Noisy_run/run.yml
-@@ -1,39 +1,42 @@
- ---
--perform_sweep: False
-+perform_sweep: True
- paths:
--  model_note: Sample_run
-+  model_note: noisy_run
- parameter_space:
-   seed: !sweep
--    default: 2
--    values: [1, 2]
--  num_epochs: 500
-+    default: 1
-+    range: [10]
-+  num_epochs: 200
-   write_every: 10
--  write_predictions_every: 20
-+  write_predictions_every: 10
-   Kuramoto:
-     Data:
-       write_adjacency_matrix: True
-       load_from_dir:
-         copy_data: True
-         network: /Users/thomasgaskin/utopya_output/Kuramoto/network/data/uni0/data.h5
--        training_data: /Users/thomasgaskin/utopya_output/Kuramoto/training_data/data/uni7/data.h5
-+        training_data: !sweep
-+          default: /Users/thomasgaskin/utopya_output/Kuramoto/training_data/data/uni06/data.h5
-+          values: [
-+            /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni01/data.h5,
-+            /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni02/data.h5,
-+            /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni03/data.h5,
-+            /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni04/data.h5,
-+            /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni05/data.h5,
-+            /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni06/data.h5,
-+            /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni07/data.h5,
-+            /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni08/data.h5,
-+            /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni09/data.h5,
-+            /Users/thomasgaskin/utopya_output/Kuramoto/noisy_training_data/data/uni10/data.h5,
-+          ]
-+          name: sigma
-         eigen_frequencies: /Users/thomasgaskin/utopya_output/Kuramoto/network/data/uni0/data.h5
-       synthetic_data:
--        num_steps: 10
--        training_set_size: 20
-         N: 16
--        sigma: &sigma 0.0
--        network:
--          type: random
--          mean_degree: 6
--          graph_props:
--            is_directed: False
--            WattsStrogatz:
--              p_rewire: 0.5
-         dt: 0.02
-+
-     NeuralNet:
--      num_layers: !sweep
--        default: 5
--        values: [4, 5]
-+      num_layers: 5
-       nodes_per_layer:
-         default: 20
-       activation_funcs:
-@@ -49,5 +52,4 @@ parameter_space:
-       loss_function:
-         name: MSELoss
-       true_parameters:
--        sigma: *sigma
--
-+        sigma: 0.0
-diff --git a/models/Kuramoto/cfgs/Sample_run/eval.yml b/models/Kuramoto/cfgs/Sample_run/eval.yml
-index 2080597..8a0d154 100644
---- a/models/Kuramoto/cfgs/Sample_run/eval.yml
-+++ b/models/Kuramoto/cfgs/Sample_run/eval.yml
-@@ -45,14 +45,16 @@
- 
- graphs/true_graph:
-   based_on: graph
-+  universes:
-+    seed: 0
- 
- graphs/prediction:
--  based_on: graph
-+  based_on: graphs/true_graph
-   select:
-     graph_group: predicted_network
- 
- marginals/degree_distribution:
--  based_on: marginals_density_uni
-+  based_on: marginals_density_mv
-   dag_options:
-     define:
-       n_bins: 200
-@@ -67,7 +69,7 @@ marginals/degree_distribution:
-     figure.figsize: [*half_width, *third_width]
- 
- marginals/clustering:
--  based_on: marginals_density_uni
-+  based_on: marginals_density_mv
-   dag_options:
-     define:
-       n_bins: 1000
-@@ -75,15 +77,14 @@ marginals/clustering:
-       max_bin: 257
-       bw_method: 0.2
-       sigma: 4
--  select:
--    coords:
--      path: predicted_network/_triangles
--    param:
--      path: predicted_network/_triangles
--    param_binned:
--      path: predicted_network/_triangles
--    true_param:
--      path: true_network/_triangles
-+  select_and_combine:
-+    fields:
-+      param:
-+        path: predicted_network/_triangles
-+      param_binned:
-+        path: predicted_network/_triangles
-+      true_val:
-+        path: true_network/_triangles
-   helpers:
-     set_title:
-       title: Triangle distribution $P(c)$
-@@ -129,3 +130,87 @@ matrices/error:
-   vmax: ~
-   cbar_kwargs:
-     label: $\Vert \hat{\mathbf{A}} - \mathbf{A} \Vert_1$
-+
-+
-+avg_var:
-+  based_on:
-+    - .creator.multiverse
-+    - .plot.prob_density
-+  dag_options:
-+    define:
-+      n_bins: 100
-+      min_bin: -1
-+      max_bin: 16
-+      bw_method: 0.5
-+      sigma: 6
-+  select_and_combine:
-+    fields:
-+      coords:
-+        path: predicted_network/_in_degree
-+        transform:
-+          - .coords: [!dag_prev , 'time']
-+          - .isel: [!dag_prev , {time: !slice [~, -2]}]
-+      degrees:
-+        path: predicted_network/_in_degree
-+        transform:
-+          - .isel: [!dag_prev , {time: !slice [~, -2]}]
-+          - .coords: [!dag_prev , 'vertex_idx']
-+          - len: [!dag_prev ]
-+          - np.linspace: [!dag_tag min_bin, !dag_tag max_bin, !dag_tag n_bins]
-+          - .data: [ !dag_node -4 ]
-+          - NeuralABM.hist: [!dag_prev ]
-+            kwargs: {bins: !dag_node -2}
-+      loss:
-+        path: output_data/loss
-+        transform:
-+          - np.maximum: [ !dag_prev , 1e-17 ]
-+#          - mul: [ !dag_prev , -1 ]
-+#          - np.exp: [!dag_prev ]
-+      t:
-+        path: true_network/_degree
-+        transform:
-+          - .isel: [ !dag_prev , { time: -1 } ]
-+            kwargs:
-+              drop: True
-+          - .data: [ !dag_prev ]
-+        subspace:
-+          seed: 0
-+      na:
-+        path: true_network/_vertices
-+        transform:
-+          - .coords: [ !dag_prev , 'vertex_idx' ]
-+          - len: [ !dag_prev ]
-+        subspace:
-+          seed: 0
-+  x: bin_center
-+  y: yerr
-+  transform:
-+    - .isel: [!dag_tag coords, {seed: 0}]
-+      kwargs: {drop: true}
-+    - .sel: [!dag_tag loss , {time: !dag_prev }]
-+    - NeuralABM.marginal_of_density: [!dag_tag degrees ]
-+      kwargs:
-+        loss: !dag_prev
-+      tag: data
-+    - getitem: [!dag_prev , 'yerr']
-+    - .mean: [!dag_prev ]
-+      tag: mean
-+  smooth_kwargs:
-+    enabled: True
-+    sigma: !dag_result sigma
-+  helpers:
-+    set_legend:
-+      use_legend: true
-+    set_hv_lines:
-+      hlines:
-+        - pos: !dag_result mean
-+          linestyle: dashed
-+          color: *red
-+    set_tick_locators:
-+      x:
-+        major:
-+          name: MaxNLocator
-+          integer: true
-+          nbins: 16
-+  style:
-+    figure.figsize: [ *half_width, *third_width ]
-diff --git a/models/Kuramoto/cfgs/Sample_run/run.yml b/models/Kuramoto/cfgs/Sample_run/run.yml
-index 20af6c2..ba4ce82 100644
---- a/models/Kuramoto/cfgs/Sample_run/run.yml
-+++ b/models/Kuramoto/cfgs/Sample_run/run.yml
-@@ -1,14 +1,14 @@
- ---
--perform_sweep: False
-+perform_sweep: True
- paths:
-   model_note: Sample_run
- parameter_space:
-   seed: !sweep
-     default: 2
--    values: [1, 2]
-+    range: [10]
-   num_epochs: 500
-   write_every: 10
--  write_predictions_every: 20
-+  write_predictions_every: 10
-   Kuramoto:
-     Data:
-       write_adjacency_matrix: True
-@@ -31,9 +31,7 @@ parameter_space:
-               p_rewire: 0.5
-         dt: 0.02
-     NeuralNet:
--      num_layers: !sweep
--        default: 5
--        values: [4, 5]
-+      num_layers: 5
-       nodes_per_layer:
-         default: 20
-       activation_funcs:
-diff --git a/models/Kuramoto/cfgs/Size_comparison/eval.yml b/models/Kuramoto/cfgs/Size_comparison/eval.yml
-index bb5ea75..eb03095 100644
---- a/models/Kuramoto/cfgs/Size_comparison/eval.yml
-+++ b/models/Kuramoto/cfgs/Size_comparison/eval.yml
-@@ -43,13 +43,61 @@
-       y: [max, min]
-       x: [min, max]
- 
--graphs/true_graph:
--  based_on: graph
-+frobenius_loss:
-+  based_on: .line_multiverse
-+  select_and_combine:
-+    fields:
-+      data: output_data/frobenius_error
-+  hue: num_vertices
-+  helpers:
-+    set_scales:
-+      y: log
- 
--graphs/prediction:
--  based_on: graph
-+loss:
-+  based_on: .line_multiverse
-+  select_and_combine:
-+    fields:
-+      data: output_data/loss
-+  hue: num_vertices
-+  helpers:
-+    set_scales:
-+      y: log
-+
-+matrices/comparison:
-+  based_on:
-+    - .matrix
-+    - .matrix_defaults
-   select:
--    graph_group: predicted_network
-+    true_matrix:
-+      path: true_network/_adjacency_matrix
-+      transform: [.data ]
-+    predicted_matrix:
-+      path: output_data/predictions
-+      transform:
-+        - .isel: [ !dag_prev , { time: -1 } ]
-+          kwargs: {drop: true}
-+  transform:
-+    - pd.Index: [ [ 'true', 'predicted' ] ]
-+      kwargs: {name: 'kind'}
-+    - xr.concat: [ [ !dag_tag true_matrix, !dag_tag predicted_matrix ], !dag_prev ]
-+      tag: data
-+  col: kind
-+  aspect: ~
-+  size: ~
-+  figsize: [*two_thirds_width, *third_width]
-+  cbar_kwargs:
-+    label: ~
-+
-+matrices/error:
-+  based_on:
-+    - accuracy
-+    - .matrix_defaults
-+  norm:
-+    name: LogNorm
-+  vmin: 1e-7
-+  vmax: ~
-+  cbar_kwargs:
-+    label: $\Vert \hat{\mathbf{A}} - \mathbf{A} \Vert_1$
- 
- marginals/degree_distribution:
-   based_on: marginals_density_uni
-@@ -92,48 +140,4 @@ marginals/clustering:
-     set_labels:
-       y: ' '
-   style:
--    figure.figsize: [ *half_width, *third_width ]
--
--matrices/comparison:
--  based_on:
--    - .matrix
--    - .matrix_defaults
--  select:
--    true_matrix:
--      path: true_network/_adjacency_matrix
--      transform: [.data ]
--    predicted_matrix:
--      path: output_data/predictions
--      transform:
--        - .isel: [ !dag_prev , { time: -1 } ]
--          kwargs: {drop: true}
--  transform:
--    - pd.Index: [ [ 'true', 'predicted' ] ]
--      kwargs: {name: 'kind'}
--    - xr.concat: [ [ !dag_tag true_matrix, !dag_tag predicted_matrix ], !dag_prev ]
--      tag: data
--  col: kind
--  aspect: ~
--  size: ~
--  figsize: [*two_thirds_width, *third_width]
--  cbar_kwargs:
--    label: ~
--
--matrices/error:
--  based_on:
--    - accuracy
--    - .matrix_defaults
--  norm:
--    name: LogNorm
--  vmin: 1e-7
--  vmax: ~
--  cbar_kwargs:
--    label: $\Vert \hat{\mathbf{A}} - \mathbf{A} \Vert_1$
--
--phases_lines:
--  based_on: phases_lines
--
--loss:
--  based_on: loss
--  select:
--    data: output_data/frobenius_error
-\ No newline at end of file
-+    figure.figsize: [ *half_width, *third_width ]
-\ No newline at end of file
-diff --git a/models/Kuramoto/cfgs/Size_comparison/run.yml b/models/Kuramoto/cfgs/Size_comparison/run.yml
-index 0a9dab5..350ed0d 100644
---- a/models/Kuramoto/cfgs/Size_comparison/run.yml
-+++ b/models/Kuramoto/cfgs/Size_comparison/run.yml
-@@ -1,14 +1,12 @@
- ---
--perform_sweep: False
-+perform_sweep: True
- paths:
--  model_note: Noisy_run
-+  model_note: Size_comparison
- parameter_space:
--  seed: !sweep
--    default: 2
--    values: [1, 2]
--  num_epochs: 500
--  write_every: 10
--  write_predictions_every: 20
-+  seed: 1
-+  num_epochs: 30
-+  write_every: 2
-+  write_predictions_every: 1000
-   Kuramoto:
-     Data:
-       write_adjacency_matrix: True
-@@ -18,22 +16,26 @@ parameter_space:
- #        training_data: /Users/thomasgaskin/utopya_output/Kuramoto/training_data/data/uni7/data.h5
- #        eigen_frequencies: /Users/thomasgaskin/utopya_output/Kuramoto/network/data/uni0/data.h5
-       synthetic_data:
--        num_steps: 200
--        training_set_size: 1
--        N: 16
--        sigma: &sigma 0.05
-+        num_steps: 10
-+        training_set_size: 100
-+        N: !sweep
-+          default: 500
-+          values: [50, 100, 150, 500, 1000]
-+          name: num_vertices
-+        sigma: &sigma 0.0
-         network:
-           type: random
--          mean_degree: 6
-+          mean_degree: !coupled-sweep
-+            default: 50
-+            values: [10, 20, 30, 100, 200]
-+            target_name: num_vertices
-           graph_props:
-             is_directed: False
-             WattsStrogatz:
-               p_rewire: 0.5
-         dt: 0.02
-     NeuralNet:
--      num_layers: !sweep
--        default: 5
--        values: [4, 5]
-+      num_layers: 5
-       nodes_per_layer:
-         default: 20
-       activation_funcs:
-@@ -43,7 +45,7 @@ parameter_space:
-       biases:
-         default: ~
-     Training:
--      batch_size: 10
-+      batch_size: 2
-       optimizer: Adam
-       learning_rate: 0.002
-       loss_function:
-diff --git a/models/Kuramoto/run.py b/models/Kuramoto/run.py
-index b1f44e7..50672ae 100755
---- a/models/Kuramoto/run.py
-+++ b/models/Kuramoto/run.py
-@@ -44,7 +44,6 @@ class Kuramoto_NN:
-         write_predictions_every: int = 1,
-         write_start: int = 1,
-         num_steps: int = 3,
--        write_time: bool = False,
-         **__,
-     ):
-         """Initialize the model instance with a previously constructed RNG and
-@@ -62,7 +61,6 @@ class Kuramoto_NN:
-             write_predictions_every: write out predicted parameters every iteration
-             write_start: iteration at which to start writing
-             num_steps: number of iterations of the ABM
--            write_time: whether to write out the training time into a dataset
-         """
-         self._name = name
-         self._time = 0
-@@ -85,7 +83,6 @@ class Kuramoto_NN:
-         self._write_predictions_every = write_predictions_every
-         self._write_start = write_start
-         self._num_steps = num_steps
--        self._write_time = write_time
- 
-         # Current training loss, Frobenius error, and current predictions
-         self.current_loss = torch.tensor(0.0)
-@@ -129,7 +126,7 @@ class Kuramoto_NN:
-         self._dset_predictions.attrs["coords_mode__time"] = "start_and_step"
-         self._dset_predictions.attrs["coords__time"] = [
-             write_start,
--            self._write_predictions_every,
-+            max(self._write_predictions_every, 1),
-         ]
-         self._dset_predictions.attrs["coords_mode__i"] = "trivial"
-         self._dset_predictions.attrs["coords_mode__j"] = "trivial"
-@@ -180,14 +177,14 @@ class Kuramoto_NN:
-         self._dset_edge_weights.attrs["coords_mode__time"] = "start_and_step"
-         self._dset_edge_weights.attrs["coords__time"] = [
-             write_start,
--            write_predictions_every,
-+            self._write_predictions_every,
-         ]
-         self._dset_edge_weights.attrs["coords_mode__edge_idx"] = "trivial"
- 
-         # In-degree
-         self._dset_in_degree = predicted_nw_group.create_dataset(
-             "_in_degree",
--            (1, self.num_agents),
-+            (0, self.num_agents),
-             maxshape=(None, self.num_agents),
-             chunks=True,
-             compression=3,
-@@ -203,7 +200,7 @@ class Kuramoto_NN:
-         # Weighted in-degree
-         self._dset_in_degree_w = predicted_nw_group.create_dataset(
-             "_in_degree_weighted",
--            (1, self.num_agents),
-+            (0, self.num_agents),
-             maxshape=(None, self.num_agents),
-             chunks=True,
-             compression=3,
-@@ -219,7 +216,7 @@ class Kuramoto_NN:
-         # Out-degree
-         self._dset_out_degree = predicted_nw_group.create_dataset(
-             "_out_degree",
--            (1, self.num_agents),
-+            (0, self.num_agents),
-             maxshape=(None, self.num_agents),
-             chunks=True,
-             compression=3,
-@@ -235,7 +232,7 @@ class Kuramoto_NN:
-         # Weighted out-degree
-         self._dset_out_degree_w = predicted_nw_group.create_dataset(
-             "_out_degree_weighted",
--            (1, self.num_agents),
-+            (0, self.num_agents),
-             maxshape=(None, self.num_agents),
-             chunks=True,
-             compression=3,
-@@ -251,7 +248,7 @@ class Kuramoto_NN:
-         # Clustering coefficients
-         self._dset_triangles = predicted_nw_group.create_dataset(
-             "_triangles",
--            (1, self.num_agents),
-+            (0, self.num_agents),
-             maxshape=(None, self.num_agents),
-             chunks=True,
-             compression=3,
-@@ -267,7 +264,7 @@ class Kuramoto_NN:
-         # Weighted clustering coefficients
-         self._dset_triangles_w = predicted_nw_group.create_dataset(
-             "_weighted_triangles",
--            (1, self.num_agents),
-+            (0, self.num_agents),
-             maxshape=(None, self.num_agents),
-             chunks=True,
-             compression=3,
-@@ -331,7 +328,7 @@ class Kuramoto_NN:
-                 # Penalise the trace (cannot be learned)
-                 loss = loss + torch.trace(pred_adj_matrix)
- 
--                loss = loss + torch.nn.MSELoss()(
-+                loss = loss + self.loss_function(
-                     pred_adj_matrix, torch.transpose(pred_adj_matrix, 0, 1)
-                 )
- 
-@@ -351,9 +348,8 @@ class Kuramoto_NN:
-                 self.write_data()
-                 self.write_predictions()
- 
--        if self._write_time:
--            self.dset_time.resize(self.dset_time.shape[0] + 1, axis=0)
--            self.dset_time[-1, :] = time.time() - start_time
-+        self.dset_time.resize(self.dset_time.shape[0] + 1, axis=0)
-+        self.dset_time[-1, :] = time.time() - start_time
- 
-     def write_data(self):
-         """Write the current loss and predicted network size into the state dataset.
-@@ -384,16 +380,13 @@ class Kuramoto_NN:
-         extend the dataset size prior to writing; this way, the newly written
-         data is always in the last row of the dataset.
-         """
--
-         if self._write_predictions_every == -1 and not write_final:
-             pass
- 
--        elif self._write_predictions_every != -1:
--            if self._time % self._write_predictions_every != 0:
--                pass
-+        else:
-+            if self._time >= self._write_start and self._time % self._write_predictions_every == 0:
- 
--            else:
--                log.info("    Writing prediction data ... ")
-+                log.debug(f"    Writing prediction data ... ")
-                 self._dset_predictions.resize(
-                     self._dset_predictions.shape[0] + 1, axis=0
-                 )
-@@ -401,7 +394,6 @@ class Kuramoto_NN:
- 
-                 # Write predicted network structure and edge weights, corresponding to the probability of that
-                 # edge existing. Write topological properties.
--
-                 curr_edges = torch.nonzero(self.current_adjacency_matrix).numpy()
-                 edge_weights = torch.flatten(
-                     self.current_predictions[torch.nonzero(self.current_predictions)]
-@@ -558,8 +550,7 @@ if __name__ == "__main__":
-         num_steps=training_data.shape[1],
-         write_every=cfg["write_every"],
-         write_predictions_every=write_predictions_every,
--        write_start=cfg["write_start"],
--        write_time=model_cfg.get("write_time", False),
-+        write_start=cfg["write_start"]
-     )
- 
-     log.info(f"   Initialized model '{model_name}'.")
-@@ -574,7 +565,10 @@ if __name__ == "__main__":
-             f"   Completed epoch {i + 1} / {num_epochs}; current loss: {model.current_loss}"
-         )
- 
--    model.write_predictions(write_final=write_predictions_every == -1)
-+    if write_predictions_every == -1:
-+        model.write_predictions(write_final=True)
-+
-+    print(len(model._dset_in_degree), len(model._dset_loss))
- 
-     log.info("   Simulation run finished.")
-     log.info("   Wrapping up ...")
\ No newline at end of file
diff --git a/data/Kuramoto/networks/config/git_info_project.yml b/data/Kuramoto/networks/config/git_info_project.yml
deleted file mode 100644
index cea0e8c..0000000
--- a/data/Kuramoto/networks/config/git_info_project.yml
+++ /dev/null
@@ -1,27 +0,0 @@
----
-dirty: true
-git_diff: /Users/thomasgaskin/utopya_output/Kuramoto/221201-165933_networks/config/git_diff_project.patch
-git_status:
-- [M, model_plots/HarrisWilson/prob_density.py]
-- ['', M model_plots/data_ops.py]
-- ['', M models/Kuramoto/Kuramoto_base_plots.yml]
-- ['', M models/Kuramoto/Kuramoto_cfg.yml]
-- [AM, models/Kuramoto/cfgs/Data_generation/network.yml]
-- [A, ' models/Kuramoto/cfgs/Data_generation/training_data_noiseless.yml']
-- [A, ' models/Kuramoto/cfgs/Data_generation/training_data_noisy.yml']
-- [AM, models/Kuramoto/cfgs/Noisy_run/eval.yml]
-- [AM, models/Kuramoto/cfgs/Noisy_run/run.yml]
-- ['', M models/Kuramoto/cfgs/Sample_run/eval.yml]
-- ['', M models/Kuramoto/cfgs/Sample_run/run.yml]
-- [AM, models/Kuramoto/cfgs/Size_comparison/eval.yml]
-- [AM, models/Kuramoto/cfgs/Size_comparison/run.yml]
-- ['', M models/Kuramoto/run.py]
-have_git_repo: true
-latest_commit: {author: Thomas Gaskin <tgaskin@live.co.uk>, author_date: '2022-11-15
-    14:13:12', commit: fe668d5919b3f591a71f84f65b98896f7404187e, gitdir: /Users/thomasgaskin/NeuralABM/.git,
-  message: Modify data ops and prob_density plot to allow for marginals on densities
-    and errorbands, parent: a06adcf6e743f72cb73506cb1a13a40e491ebed9, refs: add_Kuramoto_model,
-  tree: 79d0db89885b68d827ed10c219dc3265cb386ea1}
-project_base_dir: /Users/thomasgaskin/NeuralABM
-project_name: NeuralABM
-
diff --git a/data/Kuramoto/networks/config/meta_cfg.yml b/data/Kuramoto/networks/config/meta_cfg.yml
deleted file mode 100644
index 70fcb14..0000000
--- a/data/Kuramoto/networks/config/meta_cfg.yml
+++ /dev/null
@@ -1,176 +0,0 @@
----
-backups: {backup_cfg_files: true, backup_executable: false, include_git_info: true}
-cluster_mode: false
-cluster_params:
-  additional_run_dir_fstrs: ['job{job_id:}']
-  env: null
-  env_var_names:
-    slurm: {cluster_name: SLURM_CLUSTER_NAME, custom_out_dir: UTOPIA_CLUSTER_MODE_OUT_DIR,
-      job_account: SLURM_JOB_ACCOUNT, job_id: SLURM_JOB_ID, job_name: SLURM_JOB_NAME,
-      node_list: SLURM_JOB_NODELIST, node_name: SLURMD_NODENAME, num_nodes: SLURM_JOB_NUM_NODES,
-      num_procs: SLURM_CPUS_ON_NODE, timestamp: RUN_TIMESTAMP}
-  manager: slurm
-  node_list_parser_params: {slurm: condensed}
-data_manager:
-  create_groups:
-  - {Cls: MultiverseGroup, path: multiverse}
-  default_tree_cache_path: data/.tree_cache.d3
-  load_cfg:
-    cfg:
-      glob_str: config/*.yml
-      ignore: [config/parameter_space.yml, config/parameter_space_info.yml, config/full_parameter_space.yml,
-        config/full_parameter_space_info.yml, config/git_info_project.yml, config/git_info_framework.yml]
-      loader: yaml
-      path_regex: config/(\w+)_cfg.yml
-      required: true
-      target_path: cfg/{match:}
-    data:
-      enable_mapping: true
-      glob_str: data/uni*/data.h5
-      loader: hdf5_proxy
-      parallel: {enabled: false, min_files: 5, min_total_size: 104857600, processes: null}
-      path_regex: data/uni(\d+)/data.h5
-      required: true
-      target_path: multiverse/{match:}/data
-    pspace: {glob_str: config/parameter_space.yml, load_as_attr: true, loader: yaml_to_object,
-      required: true, target_path: multiverse, unpack_data: true}
-    uni_cfg:
-      glob_str: data/uni*/config.yml
-      loader: yaml
-      parallel: {enabled: true, min_files: 1000, min_total_size: 1048576}
-      path_regex: data/uni(\d+)/config.yml
-      required: true
-      target_path: multiverse/{match:}/cfg
-  out_dir: eval/{timestamp:}
-debug_level: 0
-executable_control: {run_from_tmpdir: false}
-parameter_space: !pspace
-  Kuramoto:
-    Data:
-      synthetic_data:
-        N: !pdim
-          as_type: null
-          assert_unique: true
-          default: 16
-          values: [16, 100, 500, 1000]
-        dt: 0.01
-        network:
-          graph_props:
-            WattsStrogatz: {p_rewire: 0.2}
-            is_directed: false
-            is_weighted: true
-          mean_degree: !coupled-pdim
-            as_type: null
-            target_name: N
-            values: [6, 20, 40, 150]
-          type: random
-        num_steps: 1
-        sigma: 0.0
-        training_set_size: 1
-    NeuralNet:
-      activation_funcs: {default: linear}
-      biases: {default: null}
-      learning_rate: 0.002
-      nodes_per_layer: {default: 20}
-      num_layers: 1
-    Training:
-      batch_size: 1
-      device: cpu
-      loss_function: {name: MSELoss}
-      true_parameters: {sigma: 0.0}
-  log_levels: {backend: warning, model: info}
-  monitor_emit_interval: 2.0
-  num_epochs: 0
-  num_steps: 3
-  root_model_name: Kuramoto
-  seed: 1
-  write_every: 1
-  write_predictions_every: -1
-  write_start: 1
-parameters_to_validate:
-  [Kuramoto, Data, synthetic_data, N]: !is-positive-int 100
-  [Kuramoto, Data, synthetic_data, dt]: !is-positive 0.01
-  [Kuramoto, Data, synthetic_data, network, graph_props, WattsStrogatz, p_rewire]: !is-probability 0.2
-  [Kuramoto, Data, synthetic_data, network, graph_props, is_directed]: !is-bool False
-  [Kuramoto, Data, synthetic_data, network, graph_props, is_weighted]: !is-bool True
-  [Kuramoto, Data, synthetic_data, network, mean_degree]: !is-positive-int 5
-  [Kuramoto, Data, synthetic_data, network, type]: !param
-    default: random
-    dtype: <U0
-    is_any_of: [random, ErdosRenyi, WattsStrogatz, scale-free]
-  [Kuramoto, Data, synthetic_data, num_steps]: !is-positive-int 60
-  [Kuramoto, Data, synthetic_data, sigma]: &id001 !is-positive-or-zero 0.0
-  [Kuramoto, Data, synthetic_data, training_set_size]: !is-positive-int 1
-  [Kuramoto, NeuralNet, learning_rate]: !is-positive 0.002
-  [Kuramoto, NeuralNet, nodes_per_layer, default]: !is-positive-int 20
-  [Kuramoto, NeuralNet, num_layers]: !is-positive-int 1
-  [Kuramoto, Training, batch_size]: !is-positive-int 1
-  [Kuramoto, Training, true_parameters, sigma]: *id001
-paths: {model_note: networks, out_dir: ~/utopya_output}
-perform_sweep: true
-perform_validation: true
-plot_manager:
-  base_cfg_pools: [utopya_base, framework_base, project_base, model_base]
-  cfg_exists_action: raise
-  creator_init_kwargs:
-    multiverse: {}
-    pyplot: {}
-    universe: {}
-  out_dir: ''
-  raise_exc: false
-  save_plot_cfg: true
-  shared_creator_init_kwargs:
-    style:
-      figure.figsize: [8.0, 5.0]
-  use_dantro_base_cfg_pool: true
-reporter:
-  report_formats:
-    progress_bar:
-      info_fstr: '{total_progress:>5.1f}% '
-      min_report_intv: 0.5
-      num_cols: adaptive
-      parser: progress_bar
-      show_times: true
-      times_fstr: '| {elapsed:>7s} elapsed | ~{est_left:>7s} left '
-      times_fstr_final: '| finished in {elapsed:} '
-      times_kwargs: {mode: from_buffer, progress_buffer_size: 90}
-      write_to: stdout_noreturn
-    report_file:
-      min_num: 4
-      min_report_intv: 10
-      parser: report
-      show_individual_runtimes: true
-      task_label_plural: universes
-      task_label_singular: universe
-      write_to:
-        file: {path: _report.txt}
-    sweep_info:
-      fstr: "Sweeping over the following parameter space:\n\n{sweep_info:}"
-      parser: pspace_info
-      write_to:
-        file: {path: _sweep_info.txt, skip_if_empty: true}
-        log: {lvl: 18, skip_if_empty: true}
-run_kwargs: {stop_conditions: null, timeout: null}
-worker_kwargs:
-  forward_raw: true
-  forward_streams: in_single_run
-  popen_kwargs: {encoding: utf8}
-  save_streams: true
-  streams_log_lvl: null
-worker_manager:
-  interrupt_params: {exit: false, grace_period: 5.0, send_signal: SIGINT}
-  lines_per_poll: 20
-  nonzero_exit_handling: raise
-  num_workers: auto
-  periodic_task_callback: null
-  poll_delay: 0.05
-  rf_spec:
-    after_abort: [progress_bar, report_file]
-    after_work: [progress_bar, report_file]
-    before_working: [sweep_info]
-    monitor_updated: [progress_bar]
-    task_finished: [progress_bar, report_file]
-    task_spawned: [progress_bar]
-    while_working: [progress_bar]
-  save_streams_on: [monitor_updated]
-
diff --git a/data/Kuramoto/networks/config/model_cfg.yml b/data/Kuramoto/networks/config/model_cfg.yml
deleted file mode 100644
index 2483f51..0000000
--- a/data/Kuramoto/networks/config/model_cfg.yml
+++ /dev/null
@@ -1,37 +0,0 @@
-Data:
-  synthetic_data:
-    N: !is-positive-int 100
-    network:
-      mean_degree: !is-positive-int 5
-      type: !param
-        default: random
-        dtype: str
-        is_any_of: [random, ErdosRenyi, WattsStrogatz, scale-free]
-      graph_props:
-        is_directed: !is-bool false
-        is_weighted: !is-bool true
-        WattsStrogatz:
-          p_rewire: !is-probability 0.2
-    sigma: !is-positive-or-zero &sigma 0.0
-    dt: !is-positive 0.01
-    num_steps: !is-positive-int 60
-    training_set_size: !is-positive-int 1
-
-# Neural net configuration
-NeuralNet:
-  num_layers: !is-positive-int 1
-  nodes_per_layer:
-    default: !is-positive-int 20
-  activation_funcs:
-    default: linear
-  biases:
-    default: ~
-  learning_rate: !is-positive 0.002
-
-Training:
-  batch_size: !is-positive-int 1
-  loss_function:
-    name: MSELoss
-  true_parameters:
-    sigma: *sigma
-  device: cpu
diff --git a/data/Kuramoto/networks/config/parameter_space.yml b/data/Kuramoto/networks/config/parameter_space.yml
deleted file mode 100644
index ea53175..0000000
--- a/data/Kuramoto/networks/config/parameter_space.yml
+++ /dev/null
@@ -1,45 +0,0 @@
----
-!pspace
-Kuramoto:
-  Data:
-    synthetic_data:
-      N: !pdim
-        as_type: null
-        assert_unique: true
-        default: 16
-        values: [16, 100, 500, 1000]
-      dt: 0.01
-      network:
-        graph_props:
-          WattsStrogatz: {p_rewire: 0.2}
-          is_directed: false
-          is_weighted: true
-        mean_degree: !coupled-pdim
-          as_type: null
-          target_name: N
-          values: [6, 20, 40, 150]
-        type: random
-      num_steps: 1
-      sigma: 0.0
-      training_set_size: 1
-  NeuralNet:
-    activation_funcs: {default: linear}
-    biases: {default: null}
-    learning_rate: 0.002
-    nodes_per_layer: {default: 20}
-    num_layers: 1
-  Training:
-    batch_size: 1
-    device: cpu
-    loss_function: {name: MSELoss}
-    true_parameters: {sigma: 0.0}
-log_levels: {backend: warning, model: info}
-monitor_emit_interval: 2.0
-num_epochs: 0
-num_steps: 3
-root_model_name: Kuramoto
-seed: 1
-write_every: 1
-write_predictions_every: -1
-write_start: 1
-
diff --git a/data/Kuramoto/networks/config/parameter_space_info.yml b/data/Kuramoto/networks/config/parameter_space_info.yml
deleted file mode 100644
index 03748b5..0000000
--- a/data/Kuramoto/networks/config/parameter_space_info.yml
+++ /dev/null
@@ -1,16 +0,0 @@
----
-coupled_dims:
-- full_path: [Kuramoto, Data, synthetic_data, network, mean_degree]
-  name: mean_degree
-  target_name: N
-  values: [6, 20, 40, 150]
-dims:
-- full_path: [Kuramoto, Data, synthetic_data, N]
-  name: N
-  values: [16, 100, 500, 1000]
-num_coupled_dims: 1
-num_dims: 1
-perform_sweep: true
-shape: [4]
-volume: 4
-
diff --git a/data/Kuramoto/networks/config/run_cfg.yml b/data/Kuramoto/networks/config/run_cfg.yml
deleted file mode 100644
index 2f8ed0b..0000000
--- a/data/Kuramoto/networks/config/run_cfg.yml
+++ /dev/null
@@ -1,25 +0,0 @@
----
-perform_sweep: True
-paths:
-  model_note: networks
-parameter_space:
-  seed: 1
-  num_epochs: 0
-  write_every: 1
-  write_predictions_every: -1
-  Kuramoto:
-    Data:
-      synthetic_data:
-        num_steps: 1
-        training_set_size: 1
-        N: !sweep
-          default: 16
-          values: [16, 100, 500, 1000]
-        network:
-          type: random
-          mean_degree: !coupled-sweep
-            target_name: N
-            values: [6, 20, 40, 150]
-          graph_props:
-            is_directed: False
-
diff --git a/data/Kuramoto/networks/config/update_cfg.yml b/data/Kuramoto/networks/config/update_cfg.yml
deleted file mode 100644
index ae2b46c..0000000
--- a/data/Kuramoto/networks/config/update_cfg.yml
+++ /dev/null
@@ -1,3 +0,0 @@
----
-{debug_level: 0}
-
diff --git a/data/Kuramoto/networks/data/N_100/config.yml b/data/Kuramoto/networks/data/N_100/config.yml
deleted file mode 100644
index 7917095..0000000
--- a/data/Kuramoto/networks/data/N_100/config.yml
+++ /dev/null
@@ -1,39 +0,0 @@
----
-Kuramoto:
-  Data:
-    synthetic_data:
-      N: 100
-      dt: 0.01
-      network:
-        graph_props:
-          WattsStrogatz: {p_rewire: 0.2}
-          is_directed: false
-          is_weighted: true
-        mean_degree: 20
-        type: random
-      num_steps: 1
-      sigma: 0.0
-      training_set_size: 1
-  NeuralNet:
-    activation_funcs: {default: linear}
-    biases: {default: null}
-    learning_rate: 0.002
-    nodes_per_layer: {default: 20}
-    num_layers: 1
-  Training:
-    batch_size: 1
-    device: cpu
-    loss_function: {name: MSELoss}
-    true_parameters: {sigma: 0.0}
-log_levels: {backend: warning, model: info}
-monitor_emit_interval: 2.0
-num_epochs: 0
-num_steps: 3
-output_dir: /Users/thomasgaskin/utopya_output/Kuramoto/221201-165933_networks/data/uni2
-output_path: /Users/thomasgaskin/utopya_output/Kuramoto/221201-165933_networks/data/uni2/data.h5
-root_model_name: Kuramoto
-seed: 1
-write_every: 1
-write_predictions_every: -1
-write_start: 1
-
diff --git a/data/Kuramoto/networks/data/N_100/data.h5 b/data/Kuramoto/networks/data/N_100/data.h5
deleted file mode 100644
index e7487bd..0000000
Binary files a/data/Kuramoto/networks/data/N_100/data.h5 and /dev/null differ
diff --git a/data/Kuramoto/networks/data/N_100/out.log b/data/Kuramoto/networks/data/N_100/out.log
deleted file mode 100644
index 7d2c4d9..0000000
--- a/data/Kuramoto/networks/data/N_100/out.log
+++ /dev/null
@@ -1,21 +0,0 @@
-Log of 'out' stream of WorkerTask 'uni2'
----
-
-INFO    Using 'cpu' as training device. Number of threads: 8
-INFO    Generating training data ...
-INFO    Generating graph ...
-INFO    Generating eigenfrequencies  ...
-INFO    Generating training data ...
-INFO    Training data generated.
-INFO    Network generated and saved.
-INFO    Initializing the neural net; input size: 100, output size: 10000 ...
-INFO    Initialized model 'Kuramoto'.
-INFO    Now commencing training for 0 epochs ...
-INFO    Simulation run finished.
-INFO    Wrapping up ...
-SUCCESS    All done.
-0 0
-
----
-end of log. exit code: 0
-
diff --git a/data/Kuramoto/networks/data/N_1000/config.yml b/data/Kuramoto/networks/data/N_1000/config.yml
deleted file mode 100644
index 97d71fd..0000000
--- a/data/Kuramoto/networks/data/N_1000/config.yml
+++ /dev/null
@@ -1,39 +0,0 @@
----
-Kuramoto:
-  Data:
-    synthetic_data:
-      N: 1000
-      dt: 0.01
-      network:
-        graph_props:
-          WattsStrogatz: {p_rewire: 0.2}
-          is_directed: false
-          is_weighted: true
-        mean_degree: 150
-        type: random
-      num_steps: 1
-      sigma: 0.0
-      training_set_size: 1
-  NeuralNet:
-    activation_funcs: {default: linear}
-    biases: {default: null}
-    learning_rate: 0.002
-    nodes_per_layer: {default: 20}
-    num_layers: 1
-  Training:
-    batch_size: 1
-    device: cpu
-    loss_function: {name: MSELoss}
-    true_parameters: {sigma: 0.0}
-log_levels: {backend: warning, model: info}
-monitor_emit_interval: 2.0
-num_epochs: 0
-num_steps: 3
-output_dir: /Users/thomasgaskin/utopya_output/Kuramoto/221201-165933_networks/data/uni4
-output_path: /Users/thomasgaskin/utopya_output/Kuramoto/221201-165933_networks/data/uni4/data.h5
-root_model_name: Kuramoto
-seed: 1
-write_every: 1
-write_predictions_every: -1
-write_start: 1
-
diff --git a/data/Kuramoto/networks/data/N_1000/data.h5 b/data/Kuramoto/networks/data/N_1000/data.h5
deleted file mode 100644
index 2df4cd4..0000000
Binary files a/data/Kuramoto/networks/data/N_1000/data.h5 and /dev/null differ
diff --git a/data/Kuramoto/networks/data/N_1000/out.log b/data/Kuramoto/networks/data/N_1000/out.log
deleted file mode 100644
index 984f427..0000000
--- a/data/Kuramoto/networks/data/N_1000/out.log
+++ /dev/null
@@ -1,21 +0,0 @@
-Log of 'out' stream of WorkerTask 'uni4'
----
-
-INFO    Using 'cpu' as training device. Number of threads: 8
-INFO    Generating training data ...
-INFO    Generating graph ...
-INFO    Generating eigenfrequencies  ...
-INFO    Generating training data ...
-INFO    Training data generated.
-INFO    Network generated and saved.
-INFO    Initializing the neural net; input size: 1000, output size: 1000000 ...
-INFO    Initialized model 'Kuramoto'.
-INFO    Now commencing training for 0 epochs ...
-INFO    Simulation run finished.
-INFO    Wrapping up ...
-SUCCESS    All done.
-0 0
-
----
-end of log. exit code: 0
-
diff --git a/data/Kuramoto/networks/data/N_16/config.yml b/data/Kuramoto/networks/data/N_16/config.yml
deleted file mode 100644
index 6f2d751..0000000
--- a/data/Kuramoto/networks/data/N_16/config.yml
+++ /dev/null
@@ -1,39 +0,0 @@
----
-Kuramoto:
-  Data:
-    synthetic_data:
-      N: 16
-      dt: 0.01
-      network:
-        graph_props:
-          WattsStrogatz: {p_rewire: 0.2}
-          is_directed: false
-          is_weighted: true
-        mean_degree: 6
-        type: random
-      num_steps: 1
-      sigma: 0.0
-      training_set_size: 1
-  NeuralNet:
-    activation_funcs: {default: linear}
-    biases: {default: null}
-    learning_rate: 0.002
-    nodes_per_layer: {default: 20}
-    num_layers: 1
-  Training:
-    batch_size: 1
-    device: cpu
-    loss_function: {name: MSELoss}
-    true_parameters: {sigma: 0.0}
-log_levels: {backend: warning, model: info}
-monitor_emit_interval: 2.0
-num_epochs: 0
-num_steps: 3
-output_dir: /Users/thomasgaskin/utopya_output/Kuramoto/221201-165933_networks/data/uni1
-output_path: /Users/thomasgaskin/utopya_output/Kuramoto/221201-165933_networks/data/uni1/data.h5
-root_model_name: Kuramoto
-seed: 1
-write_every: 1
-write_predictions_every: -1
-write_start: 1
-
diff --git a/data/Kuramoto/networks/data/N_16/data.h5 b/data/Kuramoto/networks/data/N_16/data.h5
deleted file mode 100644
index f3cc8c5..0000000
Binary files a/data/Kuramoto/networks/data/N_16/data.h5 and /dev/null differ
diff --git a/data/Kuramoto/networks/data/N_16/out.log b/data/Kuramoto/networks/data/N_16/out.log
deleted file mode 100644
index 6a6bdd3..0000000
--- a/data/Kuramoto/networks/data/N_16/out.log
+++ /dev/null
@@ -1,21 +0,0 @@
-Log of 'out' stream of WorkerTask 'uni1'
----
-
-INFO    Using 'cpu' as training device. Number of threads: 8
-INFO    Generating training data ...
-INFO    Generating graph ...
-INFO    Generating eigenfrequencies  ...
-INFO    Generating training data ...
-INFO    Training data generated.
-INFO    Network generated and saved.
-INFO    Initializing the neural net; input size: 16, output size: 256 ...
-INFO    Initialized model 'Kuramoto'.
-INFO    Now commencing training for 0 epochs ...
-INFO    Simulation run finished.
-INFO    Wrapping up ...
-SUCCESS    All done.
-0 0
-
----
-end of log. exit code: 0
-
diff --git a/data/Kuramoto/networks/data/N_500/config.yml b/data/Kuramoto/networks/data/N_500/config.yml
deleted file mode 100644
index 8fe2664..0000000
--- a/data/Kuramoto/networks/data/N_500/config.yml
+++ /dev/null
@@ -1,39 +0,0 @@
----
-Kuramoto:
-  Data:
-    synthetic_data:
-      N: 500
-      dt: 0.01
-      network:
-        graph_props:
-          WattsStrogatz: {p_rewire: 0.2}
-          is_directed: false
-          is_weighted: true
-        mean_degree: 40
-        type: random
-      num_steps: 1
-      sigma: 0.0
-      training_set_size: 1
-  NeuralNet:
-    activation_funcs: {default: linear}
-    biases: {default: null}
-    learning_rate: 0.002
-    nodes_per_layer: {default: 20}
-    num_layers: 1
-  Training:
-    batch_size: 1
-    device: cpu
-    loss_function: {name: MSELoss}
-    true_parameters: {sigma: 0.0}
-log_levels: {backend: warning, model: info}
-monitor_emit_interval: 2.0
-num_epochs: 0
-num_steps: 3
-output_dir: /Users/thomasgaskin/utopya_output/Kuramoto/221201-165933_networks/data/uni3
-output_path: /Users/thomasgaskin/utopya_output/Kuramoto/221201-165933_networks/data/uni3/data.h5
-root_model_name: Kuramoto
-seed: 1
-write_every: 1
-write_predictions_every: -1
-write_start: 1
-
diff --git a/data/Kuramoto/networks/data/N_500/data.h5 b/data/Kuramoto/networks/data/N_500/data.h5
deleted file mode 100644
index c90f8bf..0000000
Binary files a/data/Kuramoto/networks/data/N_500/data.h5 and /dev/null differ
diff --git a/data/Kuramoto/networks/data/N_500/out.log b/data/Kuramoto/networks/data/N_500/out.log
deleted file mode 100644
index 3da66a1..0000000
--- a/data/Kuramoto/networks/data/N_500/out.log
+++ /dev/null
@@ -1,21 +0,0 @@
-Log of 'out' stream of WorkerTask 'uni3'
----
-
-INFO    Using 'cpu' as training device. Number of threads: 8
-INFO    Generating training data ...
-INFO    Generating graph ...
-INFO    Generating eigenfrequencies  ...
-INFO    Generating training data ...
-INFO    Training data generated.
-INFO    Network generated and saved.
-INFO    Initializing the neural net; input size: 500, output size: 250000 ...
-INFO    Initialized model 'Kuramoto'.
-INFO    Now commencing training for 0 epochs ...
-INFO    Simulation run finished.
-INFO    Wrapping up ...
-SUCCESS    All done.
-0 0
-
----
-end of log. exit code: 0
-
diff --git a/data/SIR/ABM_data/config/git_info_project.yml b/data/SIR/ABM_data/config/git_info_project.yml
index 893c147..51c1340 100644
--- a/data/SIR/ABM_data/config/git_info_project.yml
+++ b/data/SIR/ABM_data/config/git_info_project.yml
@@ -4,12 +4,12 @@ git_diff: /Users/thomasgaskin/utopya_output/SIR/220927-081223_ABM_data/config/gi
 git_status:
 - [M, models/HarrisWilson/ABM.py]
 - ['', M models/HarrisWilson/HarrisWilsonNW_cfg.yml]
-- ['', M models/HarrisWilson/cfgs/London_dataset/run.yml]
+- ['', M models/HarrisWilson/cfgs/N_750_example/run.yml]
 - ['', M models/HarrisWilson/cfgs/Loss_landscape/run.yml]
 - ['', M models/HarrisWilson/cfgs/Marginals/eval.yml]
 - ['', M models/HarrisWilson/cfgs/Marginals/run.yml]
 - ['', M models/HarrisWilson/cfgs/Performance_analysis/run.yml]
-- ['', M models/HarrisWilson/cfgs/Small_sample/run.yml]
+- ['', M models/HarrisWilson/cfgs/N_100_example/run.yml]
 - ['', M models/SIR/SIR_cfg.yml]
 - ['', M models/SIR/cfgs/Predictions/eval.yml]
 - ['', M models/SIR/cfgs/Predictions/run.yml]
diff --git a/include/graph_utilities.py b/include/graph_utilities.py
index 62ed135..5a7bfaf 100644
--- a/include/graph_utilities.py
+++ b/include/graph_utilities.py
@@ -1,6 +1,6 @@
-import numpy as np
-import networkx as nx
 import h5py as h5
+import networkx as nx
+import numpy as np
 
 
 def save_nw(
diff --git a/include/regression.py b/include/regression.py
index 5b62226..d6ce116 100644
--- a/include/regression.py
+++ b/include/regression.py
@@ -1,18 +1,20 @@
-import torch
-import h5py as h5
 import time
+
+import h5py as h5
 import numpy as np
+import torch
 
 
 def Kuramoto_regression(
-        training_data: torch.Tensor,
-        eigenfrequencies: torch.Tensor,
-        h5file: h5.File,
-        dt: float,
-        *,
-        second_order: bool
+    training_data: torch.Tensor,
+    eigenfrequencies: torch.Tensor,
+    h5file: h5.File,
+    dt: float,
+    *,
+    second_order: bool,
+    gamma: float = None,
 ):
-    """ Estimates the network from first- or second-order dynamics via an OLS-regression, and stores the predictions in
+    """Estimates the network from first- or second-order dynamics via an OLS-regression, and stores the predictions in
     an h5.File.
 
     :param training_data: the training data from which to estimate the network
@@ -20,6 +22,7 @@ def Kuramoto_regression(
     :param h5file: the h5 file to write the predictions to
     :param dt: the time differential of the forward Euler method
     :param second_order: whether to use second order dynamics
+    :param gamma: the coefficient of the first derivative used in the second-order dynamics
     """
 
     # Extract the number of nodes from the training data
@@ -29,7 +32,8 @@ def Kuramoto_regression(
         # Stack the observations into a matrix for each node
         X = torch.transpose(
             torch.reshape(
-                torch.diff(training_data, dim=1) / dt - eigenfrequencies, (-1, N)
+                torch.diff(training_data, dim=1) / dt - eigenfrequencies[:, :-1, :, :],
+                (-1, N),
             ),
             0,
             1,
@@ -37,7 +41,10 @@ def Kuramoto_regression(
 
         # Stack the sine-couplings into a matrix for each node
         G = torch.zeros(
-            training_data.shape[0], training_data.shape[1] - 1, training_data.shape[2], N
+            training_data.shape[0],
+            training_data.shape[1] - 1,
+            training_data.shape[2],
+            N,
         )
 
     else:
@@ -45,13 +52,19 @@ def Kuramoto_regression(
 
         X = torch.transpose(
             torch.reshape(
-                torch.diff(velocities, dim=1) / dt + velocities[:, :-1, :, :] - eigenfrequencies, (-1, N)
+                torch.diff(velocities, dim=1) / dt
+                + gamma * velocities[:, :-1, :, :]
+                - eigenfrequencies[:, :-2, :, :],
+                (-1, N),
             ),
             0,
             1,
         )
         G = torch.zeros(
-            training_data.shape[0], training_data.shape[1] - 2, training_data.shape[2], N
+            training_data.shape[0],
+            training_data.shape[1] - 2,
+            training_data.shape[2],
+            N,
         )
 
     for dset in range(G.shape[0]):
diff --git a/model_plots/HarrisWilson/prob_density.py b/model_plots/HarrisWilson/prob_density.py
index 5e37737..20383af 100644
--- a/model_plots/HarrisWilson/prob_density.py
+++ b/model_plots/HarrisWilson/prob_density.py
@@ -154,3 +154,5 @@ def plot_prob_density(
             **info_box_labels.get("info_box_kwargs", {}),
         )
         plt.gca().add_artist(legend)
+
+    smooth_kwargs.update(dict(enabled=smooth, sigma=sigma))
diff --git a/model_plots/Kuramoto/__init__.py b/model_plots/Kuramoto/__init__.py
index e69de29..4f73711 100644
--- a/model_plots/Kuramoto/__init__.py
+++ b/model_plots/Kuramoto/__init__.py
@@ -0,0 +1 @@
+from .time_and_loss import *
diff --git a/model_plots/Kuramoto/time_and_loss.py b/model_plots/Kuramoto/time_and_loss.py
index e69de29..04d865c 100644
--- a/model_plots/Kuramoto/time_and_loss.py
+++ b/model_plots/Kuramoto/time_and_loss.py
@@ -0,0 +1,52 @@
+import logging
+from typing import Union
+
+import matplotlib.lines as mlines
+import matplotlib.pyplot as plt
+import scipy.ndimage
+import xarray as xr
+from dantro.plot.funcs.generic import errorbars
+
+from utopya.eval import PlotHelper, is_plot_func
+
+log = logging.getLogger(__name__)
+
+
+@is_plot_func(
+    use_dag=True,
+    use_helper=True,
+    required_dag_tags=(
+        "loss_data",
+        "time_data",
+    ),
+)
+def time_and_loss(hlpr: PlotHelper, *, data: dict, loss_color: str, time_color: str):
+    loss_data: xr.Dataset = data["loss_data"]
+    time_data: xr.Dataset = data["time_data"]
+
+    ax1 = hlpr.ax
+
+    # Plot the loss data
+    ax2 = hlpr.ax.twinx()
+    loss_data["y"].plot(ax=ax2, color=loss_color)
+    ax2.fill_between(
+        loss_data.coords["N"],
+        (loss_data["y"] + loss_data["yerr"]),
+        (loss_data["y"] - loss_data["yerr"]),
+        alpha=0.2,
+        color=loss_color,
+        lw=0,
+    )
+    ax2.set_ylabel(r"$l1$ error after 10 epochs")
+    ax2.set_ylim([0.1, 0.5])
+
+    hlpr.select_axis(ax=ax1)
+    time_data["y"].plot(color=time_color)
+    ax1.fill_between(
+        time_data.coords["N"],
+        (time_data["y"] + time_data["yerr"]),
+        (time_data["y"] - time_data["yerr"]),
+        alpha=0.5,
+        color=time_color,
+        lw=0,
+    )
diff --git a/model_plots/__init__.py b/model_plots/__init__.py
index c302dfb..1fdf19d 100644
--- a/model_plots/__init__.py
+++ b/model_plots/__init__.py
@@ -18,6 +18,7 @@ import pandas as pd
 import xarray as xr
 
 import model_plots.HarrisWilson
+import model_plots.Kuramoto
 from utopya.eval import register_operation
 
 register_operation(name="pd.Index", func=pd.Index)
@@ -35,4 +36,8 @@ register_operation(name="np.nonzero", func=np.nonzero)
 register_operation(name="xr.where", func=xr.where)
 register_operation(name="xr.apply_ufunc", func=xr.apply_ufunc)
 register_operation(name=".to_dataset", func=lambda d, *a, **k: d.to_dataset(*a, **k))
+register_operation(name="sin", func=np.sin)
+register_operation(name="cos", func=np.cos)
+register_operation(name="zip", func=zip)
+register_operation(name="np.around", func=np.around)
 from .data_ops import *
diff --git a/model_plots/data_ops.py b/model_plots/data_ops.py
index cf28b46..e593b75 100644
--- a/model_plots/data_ops.py
+++ b/model_plots/data_ops.py
@@ -335,6 +335,16 @@ def marginal_of_density(
     error: str = "standard",
 ) -> xr.Dataset:
 
+    """Calculates the marginal density over a family of distributions.
+
+    :param vals: the family of distributions
+    :param loss: the normalised probability associated with each family
+    :param coords:
+    :param MLE_index:
+    :param error:
+    :return:
+    """
+
     n_samples, n_bins = list(vals.sizes.values())[:]
 
     # Calculate the mean of each bin
@@ -368,8 +378,53 @@ def marginal_of_density(
         coords=coords,
     )
 
+
 @is_operation("NeuralABM.triangles")
 def triangles(ds: xr.DataArray, *args, **kwargs):
 
+    """Calculates the number of triangles on each node, given an adjacency matrix"""
     res = xr.apply_ufunc(np.linalg.matrix_power, ds, 3, *args, **kwargs)
-    return xr.apply_ufunc(np.diagonal, res, 0, 1, 2, input_core_dims=[['j'], [], [], []])
+    return xr.apply_ufunc(
+        np.diagonal, res, 0, 1, 2, input_core_dims=[["j"], [], [], []]
+    )
+
+
+@is_operation("NeuralABM.largest_entry_indices")
+def largest_entry_indices(
+    ds: xr.DataArray, n: int, *, symmetric: bool = True
+) -> tuple[Sequence, Sequence, Sequence]:
+
+    """Returns the 2d-indices of the n largest entries in an adjacency matrix, as well as the corresponding values.
+    If the matrix is symmetric, only the upper triangle is considered. Sorted from highest to lowest."""
+
+    if symmetric:
+        indices_i, indices_j = np.unravel_index(
+            np.argsort(np.triu(ds.data).ravel()), np.shape(ds)
+        )
+    else:
+        indices_i, indices_j = np.unravel_index(
+            np.argsort(ds.data.ravel()), np.shape(ds)
+        )
+
+    i, j = indices_i[-n:][::-1], indices_j[-n:][::-1]
+    vals = ds.data[i, j]
+
+    return i, j, vals
+
+
+@is_operation("NeuralABM.sel_matrix_indices")
+def matrix_indices_sel(
+    ds: xr.DataArray, indices: tuple[Sequence, Sequence], dims: str = "edge_idx"
+) -> xr.DataArray:
+
+    """Returns the predictions on the weights of the entries given by indices"""
+    return ds.isel(
+        i=(
+            xr.DataArray(
+                indices[0], dims=dims, coords=dict(edge_idx=np.arange(len(indices[0])))
+            )
+        ),
+        j=xr.DataArray(
+            indices[1], dims=dims, coords=dict(edge_idx=np.arange(len(indices[0])))
+        ),
+    )
diff --git a/models/HarrisWilson/ABM.py b/models/HarrisWilson/ABM.py
index a424534..451cb5b 100644
--- a/models/HarrisWilson/ABM.py
+++ b/models/HarrisWilson/ABM.py
@@ -140,7 +140,7 @@ class HarrisWilsonABM:
                 epsilon * (demand - kappa * curr_vals)
                 + sigma
                 * 1
-                / torch.sqrt(torch.tensor(2 * torch.pi * dt, dtype=torch.float)).to(
+                / torch.sqrt(torch.tensor(2.0, dtype=torch.float) * torch.pi * dt).to(
                     self.device
                 )
                 * torch.normal(0, 1, size=(self.M, 1)).to(self.device),
diff --git a/models/HarrisWilson/DataGeneration.py b/models/HarrisWilson/DataGeneration.py
index 1d20970..64b2ac5 100644
--- a/models/HarrisWilson/DataGeneration.py
+++ b/models/HarrisWilson/DataGeneration.py
@@ -29,7 +29,7 @@ def load_from_dir(dir) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:
         # If data is in h5 format
         if dir.lower().endswith(".h5"):
             with h5.File(dir, "r") as f:
-                origins = np.array(f["HarrisWilson"]["origin_sizes"])[0]
+                origins = np.array(f["HarrisWilson"]["origin_sizes"])
                 training_data = np.array(f["HarrisWilson"]["training_data"])
                 nw = np.array(f["network"]["_edge_weights"])
 
@@ -52,23 +52,14 @@ def load_from_dir(dir) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:
         ).to_numpy()
         nw = pd.read_csv(dir["network"], header=0, index_col=0).to_numpy()
 
-    # Reshape the origin zone sizes
-    or_sizes = torch.tensor(origins, dtype=torch.float)
-    N_origin = len(or_sizes)
-    or_sizes = torch.reshape(or_sizes, (N_origin, 1))
-
-    # Reshape the time series of the destination zone sizes
-    time_series = torch.tensor(training_data, dtype=torch.float)
-    N_destination = training_data.shape[1]
-    time_series = torch.reshape(time_series, (len(time_series), N_destination, 1))
-
-    # Reshape the network
-    network = torch.reshape(
-        torch.tensor(nw, dtype=torch.float), (N_origin, N_destination)
+    origins = torch.from_numpy(origins).float()
+    training_data = torch.unsqueeze(torch.from_numpy(training_data).float(), -1)
+    nw = torch.reshape(
+        torch.from_numpy(nw).float(), (origins.shape[0], training_data.shape[1])
     )
 
-    # Return all three datasets
-    return or_sizes, time_series, network
+    # Return the data as torch tensors
+    return origins, training_data, nw
 
 
 def generate_synthetic_data(*, cfg) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:
@@ -87,7 +78,13 @@ def generate_synthetic_data(*, cfg) -> Tuple[torch.tensor, torch.tensor, torch.t
     num_steps = data_cfg["num_steps"]
 
     # Generate the initial origin sizes
-    or_sizes = torch.abs(torch.normal(0.1, 0.01, size=(N_origin, 1)))
+    or_sizes = torch.abs(
+        torch.normal(
+            data_cfg["origin_sizes"]["init_mean"],
+            data_cfg["origin_sizes"]["init_std"],
+            size=(N_origin, 1),
+        )
+    )
 
     # Generate the edge weights
     network = torch.exp(
@@ -103,7 +100,11 @@ def generate_synthetic_data(*, cfg) -> Tuple[torch.tensor, torch.tensor, torch.t
 
     # Generate the initial destination zone sizes
     init_dest_sizes = torch.abs(
-        torch.normal(0.1, 0.01, size=(data_cfg["N_destination"], 1))
+        torch.normal(
+            data_cfg["init_dest_sizes"]["mean"],
+            data_cfg["init_dest_sizes"]["std"],
+            size=(data_cfg["N_destination"], 1),
+        )
     )
 
     # Extract the underlying parameters from the config
@@ -149,38 +150,38 @@ def get_HW_data(cfg, h5file: h5.File, h5group: h5.Group, *, device: str):
     :return: the origin zone sizes, the training data, and the network
     """
 
-    data_dir = cfg.pop("load_from_dir", None)
+    data_dir = cfg.get("load_from_dir", {})
 
     # Get the origin sizes, time series, and network data
     or_sizes, dest_sizes, network = (
-        load_from_dir(data_dir)
-        if data_dir is not None
-        else generate_synthetic_data(cfg=cfg)
+        load_from_dir(data_dir) if data_dir else generate_synthetic_data(cfg=cfg)
     )
 
-    N_origin = or_sizes.shape[0]
-    N_destination = dest_sizes.shape[1]
+    N_origin, N_destination = or_sizes.shape[0], dest_sizes.shape[1]
 
     # Only save individual time frames
-    synthetic_data = cfg.pop("synthetic_data", None)
-    if synthetic_data is not None:
-        write_start = synthetic_data.pop("write_start", 0)
-        write_every = synthetic_data.pop("write_every", 1)
+    synthetic_data = cfg.get("synthetic_data", {})
+    if synthetic_data:
+        write_start = synthetic_data.get("write_start", 0)
+        write_every = synthetic_data.get("write_every", 1)
         time_series = dest_sizes[write_start::write_every]
     else:
         time_series = dest_sizes
 
     # If time series has a single frame, double it to enable visualisation.
     # This does not affect the training data
-    training_data_size = cfg.pop("training_data_size", len(time_series))
-    if len(time_series) == 1:
-        time_series = torch.stack([time_series, time_series])
+    training_data_size = cfg.get("training_data_size", time_series.shape[0])
+    if time_series.shape[0] == 1:
+        time_series = torch.concat((time_series, time_series), axis=0)
+
+    # Extract the training data from the time series data
+    training_data = dest_sizes[-training_data_size:]
 
     # Set up dataset for complete synthetic time series
     dset_time_series = h5group.create_dataset(
         "time_series",
-        (len(time_series), N_destination),
-        maxshape=(len(time_series), N_destination),
+        time_series.shape[:-1],
+        maxshape=time_series.shape[:-1],
         chunks=True,
         compression=3,
     )
@@ -193,13 +194,13 @@ def get_HW_data(cfg, h5file: h5.File, h5group: h5.Group, *, device: str):
     )
 
     # Write the time series data
-    dset_time_series[:, :] = torch.flatten(time_series, start_dim=1)
+    dset_time_series[:, :] = torch.flatten(time_series, start_dim=-2)
 
-    # Training time series
+    # Save the training time series
     dset_training_data = h5group.create_dataset(
         "training_data",
-        (training_data_size, N_destination),
-        maxshape=(training_data_size, N_destination),
+        training_data.shape[:-1],
+        maxshape=training_data.shape[:-1],
         chunks=True,
         compression=3,
     )
@@ -209,24 +210,20 @@ def get_HW_data(cfg, h5file: h5.File, h5group: h5.Group, *, device: str):
     dset_training_data.attrs["coords__zone_id"] = np.arange(
         N_origin, N_origin + N_destination, 1
     )
-
-    # Extract the training data from the time series data and save
-    training_data = dest_sizes[-training_data_size:]
-    dset_training_data[:, :] = torch.flatten(training_data, start_dim=1)
+    dset_training_data[:, :] = torch.flatten(training_data, start_dim=-2)
 
     # Set up chunked dataset to store the state data in
     # Origin zone sizes
     dset_origin_sizes = h5group.create_dataset(
         "origin_sizes",
-        (1, N_origin),
-        maxshape=(1, N_origin),
+        or_sizes.shape[:-1],
+        maxshape=or_sizes.shape[:-1],
         chunks=True,
         compression=3,
     )
-    dset_origin_sizes.attrs["dim_names"] = ["dim_name__0", "zone_id"]
-    dset_origin_sizes.attrs["coords_mode__zone_id"] = "values"
-    dset_origin_sizes.attrs["coords__zone_id"] = np.arange(0, N_origin, 1)
-    dset_origin_sizes[0, :] = torch.flatten(or_sizes)
+    dset_origin_sizes.attrs["dim_names"] = ["zone_id"]
+    dset_origin_sizes.attrs["coords_mode__zone_id"] = "trivial"
+    dset_origin_sizes[:] = torch.flatten(or_sizes)
 
     # Create a network group
     nw_group = h5file.create_group("network")
@@ -237,30 +234,29 @@ def get_HW_data(cfg, h5file: h5.File, h5group: h5.Group, *, device: str):
     # Add vertices
     vertices = nw_group.create_dataset(
         "_vertices",
-        (1, N_origin + N_destination),
-        maxshape=(1, N_origin + N_destination),
+        (N_origin + N_destination,),
+        maxshape=(N_origin + N_destination,),
         chunks=True,
         compression=3,
         dtype=int,
     )
-    vertices.attrs["dim_names"] = ["dim_name__0", "vertex_idx"]
+    vertices.attrs["dim_names"] = ["vertex_idx"]
     vertices.attrs["coords_mode__vertex_idx"] = "trivial"
-    vertices[0, :] = np.arange(0, N_origin + N_destination, 1)
+    vertices[:] = np.arange(0, N_origin + N_destination, 1)
     vertices.attrs["node_type"] = [0] * N_origin + [1] * N_destination
 
     # Add edges. The network is a complete bipartite graph
-    # TODO: allow more general network topologies?
     edges = nw_group.create_dataset(
         "_edges",
-        (1, N_origin * N_destination, 2),
-        maxshape=(1, N_origin * N_destination, 2),
+        (N_origin * N_destination, 2),
+        maxshape=(N_origin * N_destination, 2),
         chunks=True,
         compression=3,
     )
-    edges.attrs["dim_names"] = ["dim_name__1", "edge_idx", "vertex_idx"]
+    edges.attrs["dim_names"] = ["edge_idx", "vertex_idx"]
     edges.attrs["coords_mode__edge_idx"] = "trivial"
     edges.attrs["coords_mode__vertex_idx"] = "trivial"
-    edges[0, :] = np.reshape(
+    edges[:,] = np.reshape(
         [
             [[i, j] for i in range(N_origin)]
             for j in range(N_origin, N_origin + N_destination)
@@ -271,13 +267,13 @@ def get_HW_data(cfg, h5file: h5.File, h5group: h5.Group, *, device: str):
     # Edge weights
     edge_weights = nw_group.create_dataset(
         "_edge_weights",
-        (1, N_origin * N_destination),
-        maxshape=(1, N_origin * N_destination),
+        (N_origin * N_destination,),
+        maxshape=(N_origin * N_destination,),
         chunks=True,
         compression=3,
     )
-    edge_weights.attrs["dim_names"] = ["dim_name__1", "edge_idx"]
+    edge_weights.attrs["dim_names"] = ["edge_idx"]
     edge_weights.attrs["coords_mode__edge_idx"] = "trivial"
-    edge_weights[0, :] = torch.reshape(network, (N_origin * N_destination,))
+    edge_weights[:] = torch.reshape(network, (N_origin * N_destination,))
 
     return or_sizes.to(device), training_data.to(device), network.to(device)
diff --git a/models/HarrisWilson/HarrisWilson_cfg.yml b/models/HarrisWilson/HarrisWilson_cfg.yml
index 8ded54f..426d439 100644
--- a/models/HarrisWilson/HarrisWilson_cfg.yml
+++ b/models/HarrisWilson/HarrisWilson_cfg.yml
@@ -12,6 +12,31 @@ Data:
     sigma: !is-positive-or-zero 0
     epsilon: !is-positive 10
     dt: !is-positive 0.01
+    origin_sizes:
+      init_mean: !param
+        default: 0.1
+        limits: [0, ~]
+        dtype: float
+        description: mean of the initial origin size distribution
+      init_std: !param
+        default: 0.01
+        limits: [0, ~]
+        dtype: float
+        description: variance of the initial origin size distribution
+      temporal_std: !param
+        default: 0.0
+        limits: [0, ~]
+        dtype: float
+        description: variance of the origin size temporal fluctuations
+    init_dest_sizes:
+      mean: !param
+        default: 0.1
+        limits: [ 0, ~ ]
+        dtype: float
+      std: !param
+        default: 0.01
+        limits: [ 0, ~ ]
+        dtype: float
     init_weights:
       mean: !param
         default: 1.2
@@ -43,6 +68,8 @@ Training:
     name: MSELoss
   batch_size: !is-positive-int 1
   device: cpu
+  epsilon: 1
+
 
 # Whether to write out the computation time for each epoch
 write_time: False
diff --git a/models/HarrisWilson/README.md b/models/HarrisWilson/README.md
index 6cf539d..b1bf44b 100644
--- a/models/HarrisWilson/README.md
+++ b/models/HarrisWilson/README.md
@@ -1,5 +1,6 @@
 # The Harris-Wilson model
 
+To do
 ## How to load data
 
 WIP
diff --git a/models/HarrisWilson/cfgs/London_dataset/run.yml b/models/HarrisWilson/cfgs/London_dataset/run.yml
index 517da14..2a8db3a 100644
--- a/models/HarrisWilson/cfgs/London_dataset/run.yml
+++ b/models/HarrisWilson/cfgs/London_dataset/run.yml
@@ -1,6 +1,6 @@
 perform_sweep: True
 paths:
-  model_note: London_dataset
+  model_note: N_750_example
 parameter_space:
   seed: !sweep
     default: 2
diff --git a/models/HarrisWilson/cfgs/Performance_analysis/run.yml b/models/HarrisWilson/cfgs/Performance_analysis/run.yml
index 5961177..021228f 100644
--- a/models/HarrisWilson/cfgs/Performance_analysis/run.yml
+++ b/models/HarrisWilson/cfgs/Performance_analysis/run.yml
@@ -49,6 +49,3 @@ parameter_space:
       batch_size: 1
       true_parameters:
         sigma: 0
-
-    # Write out the training time for each epoch
-    write_time: True
diff --git a/models/HarrisWilson/cfgs/Sample_run/run.yml b/models/HarrisWilson/cfgs/Sample_run/run.yml
index e7f4864..07873b1 100644
--- a/models/HarrisWilson/cfgs/Sample_run/run.yml
+++ b/models/HarrisWilson/cfgs/Sample_run/run.yml
@@ -1,6 +1,6 @@
 ---
 paths:
-  model_note: Small_sample
+  model_note: N_100_example
 parameter_space:
   seed: 0
   num_epochs: 6000
diff --git a/models/HarrisWilson/run.py b/models/HarrisWilson/run.py
index e1b4d7c..be24a00 100755
--- a/models/HarrisWilson/run.py
+++ b/models/HarrisWilson/run.py
@@ -28,7 +28,6 @@ coloredlogs.install(fmt="%(levelname)s %(message)s", level="INFO", logger=log)
 class HarrisWilson_NN:
     def __init__(
         self,
-        name: str,
         *,
         rng: np.random.Generator,
         h5group: h5.Group,
@@ -38,14 +37,12 @@ class HarrisWilson_NN:
         to_learn: list,
         write_every: int = 1,
         write_start: int = 1,
-        write_time: bool = False,
         **__,
     ):
         """Initialize the model instance with a previously constructed RNG and
         HDF5 group to write the output data to.
 
         Args:
-            name (str): The name of this model instance
             rng (np.random.Generator): The shared RNG
             h5group (h5.Group): The output file group to write data to
             neural_net: The neural network
@@ -55,20 +52,19 @@ class HarrisWilson_NN:
             write_every: write every iteration
             write_start: iteration at which to start writing
             num_steps: number of iterations of the ABM
-            write_time: whether to write out the training time into a dataset
         """
-        self._name = name
-        self._time = 0
         self._h5group = h5group
         self._rng = rng
 
+        # The numerical solver
         self._ABM = ABM
+
+        # Initialise neural net, loss tracker and prediction tracker
         self._neural_net = neural_net
         self._neural_net.optimizer.zero_grad()
         self.loss_function = base.LOSS_FUNCTIONS[loss_function.get("name").lower()](
             loss_function.get("args", None), **loss_function.get("kwargs", {})
         )
-
         self._current_loss = torch.tensor(0.0, requires_grad=False)
         self._current_predictions = torch.stack(
             [torch.tensor(0.0, requires_grad=False)] * len(to_learn)
@@ -77,65 +73,72 @@ class HarrisWilson_NN:
         # Setup chunked dataset to store the state data in
         self._dset_loss = self._h5group.create_dataset(
             "loss",
-            (0, 1),
-            maxshape=(None, 1),
+            (0,),
+            maxshape=(None,),
             chunks=True,
             compression=3,
         )
-        self._dset_loss.attrs["dim_names"] = ["time", "training_loss"]
+        self._dset_loss.attrs["dim_names"] = ["time"]
         self._dset_loss.attrs["coords_mode__time"] = "start_and_step"
         self._dset_loss.attrs["coords__time"] = [write_start, write_every]
 
-        if write_time:
-            self.dset_time = self._h5group.create_dataset(
-                "computation_time",
-                (0, 1),
-                maxshape=(None, 1),
-                chunks=True,
-                compression=3,
-            )
-            self.dset_time.attrs["dim_names"] = ["epoch", "training_time"]
-            self.dset_time.attrs["coords_mode__epoch"] = "trivial"
-            self.dset_time.attrs["coords_mode__training_time"] = "trivial"
+        self.dset_time = self._h5group.create_dataset(
+            "computation_time",
+            (0,),
+            maxshape=(None,),
+            chunks=True,
+            compression=3,
+        )
+        self.dset_time.attrs["dim_names"] = ["epoch"]
+        self.dset_time.attrs["coords_mode__epoch"] = "trivial"
 
         dset_predictions = []
         for p_name in to_learn:
             dset = self._h5group.create_dataset(
-                p_name, (0, 1), maxshape=(None, 1), chunks=True, compression=3
+                p_name, (0,), maxshape=(None,), chunks=True, compression=3
             )
-            dset.attrs["dim_names"] = ["time", "parameter"]
+            dset.attrs["dim_names"] = ["time"]
             dset.attrs["coords_mode__time"] = "start_and_step"
             dset.attrs["coords__time"] = [write_start, write_every]
 
             dset_predictions.append(dset)
-
         self._dset_predictions = dset_predictions
+
+        # Count the number of gradient descent steps
+        self._time = 0
         self._write_every = write_every
         self._write_start = write_start
-        self._write_time = write_time
 
     def epoch(
         self,
         *,
-        training_data,
-        batch_size: int = 1,
-        epsilon: float = 1,
-        dt: float = 0.001,
+        training_data: torch.tensor,
+        batch_size: int,
+        epsilon: float = None,
+        dt: float = None,
+        **__,
     ):
 
         """Trains the model for a single epoch.
 
         :param training_data: the training data
-        :param batch_size: (optional) the number of passes (batches) over the training data before conducting a gradient descent
+        :param batch_size: the number of time series elements to process before conducting a gradient descent
                 step
         :param epsilon: (optional) the epsilon value to use during training
         :param dt: (optional) the time differential to use during training
+        :param __: other parameters (ignored)
         """
 
+        # Track the epoch training time
         start_time = time.time()
 
+        # Track the training loss
         loss = torch.tensor(0.0, requires_grad=True)
 
+        # Count the number of batch items processed
+        n_processed_steps = 0
+
+        # Process the training set elementwise, updating the loss after batch_size steps
         for t, sample in enumerate(training_data):
 
             predicted_parameters = self._neural_net(torch.flatten(sample))
@@ -147,26 +150,28 @@ class HarrisWilson_NN:
                 requires_grad=True,
             )
 
-            loss = loss + self.loss_function(predicted_data, sample) / batch_size
+            loss = loss + self.loss_function(predicted_data, sample)
 
-            self._current_loss = loss.clone().detach().cpu().numpy().item()
-            self._current_predictions = predicted_parameters.clone().detach().cpu()
-            self.write_data()
+            n_processed_steps += 1
 
             # Update the model parameters after every batch and clear the loss
             if t % batch_size == 0 or t == len(training_data) - 1:
                 loss.backward()
                 self._neural_net.optimizer.step()
                 self._neural_net.optimizer.zero_grad()
+                self._time += 1
+                self._current_loss = (
+                    loss.clone().detach().cpu().numpy().item() / n_processed_steps
+                )
+                self._current_predictions = predicted_parameters.clone().detach().cpu()
+                self.write_data()
                 del loss
                 loss = torch.tensor(0.0, requires_grad=True)
+                n_processed_steps = 0
 
-            self._time += 1
-
-        # Write the training time (wall clock time)
-        if self._write_time:
-            self.dset_time.resize(self.dset_time.shape[0] + 1, axis=0)
-            self.dset_time[-1, :] = time.time() - start_time
+        # Write the epoch training time (wall clock time)
+        self.dset_time.resize(self.dset_time.shape[0] + 1, axis=0)
+        self.dset_time[-1] = time.time() - start_time
 
     def write_data(self):
         """Write the current state into the state dataset.
@@ -178,7 +183,7 @@ class HarrisWilson_NN:
         if self._time >= self._write_start and (self._time % self._write_every == 0):
 
             self._dset_loss.resize(self._dset_loss.shape[0] + 1, axis=0)
-            self._dset_loss[-1, :] = self._current_loss
+            self._dset_loss[-1] = self._current_loss
 
             for idx, dset in enumerate(self._dset_predictions):
                 dset.resize(dset.shape[0] + 1, axis=0)
@@ -202,7 +207,7 @@ if __name__ == "__main__":
     model_cfg = cfg[model_name]
 
     # Select the training device and number of threads to use
-    device = model_cfg["Training"].get("device", None)
+    device = model_cfg["Training"].pop("device", None)
     if device is None:
         device = (
             "mps"
@@ -251,19 +256,16 @@ if __name__ == "__main__":
         M=dest_sizes.shape[1],
         device=device,
     )
-    write_time = model_cfg.get("write_time", False)
 
     model = HarrisWilson_NN(
-        model_name,
         rng=rng,
         h5group=h5group,
         neural_net=net,
-        loss_function=model_cfg["Training"]["loss_function"],
+        loss_function=model_cfg["Training"].pop("loss_function"),
         ABM=ABM,
-        to_learn=model_cfg["Training"]["to_learn"],
+        to_learn=model_cfg["Training"].pop("to_learn"),
         write_every=cfg["write_every"],
         write_start=cfg["write_start"],
-        write_time=write_time,
     )
     log.info(f"   Initialized model '{model_name}'.")
 
@@ -273,9 +275,7 @@ if __name__ == "__main__":
 
     for _ in range(num_epochs):
 
-        model.epoch(
-            training_data=dest_sizes, batch_size=model_cfg["Training"]["batch_size"]
-        )
+        model.epoch(training_data=dest_sizes, **model_cfg["Training"])
 
         log.progress(f"   Completed epoch {_+1} / {num_epochs}.")
 
diff --git a/models/HarrisWilsonNW/ABM.py b/models/HarrisWilsonNW/ABM.py
index d06f890..41b546e 100644
--- a/models/HarrisWilsonNW/ABM.py
+++ b/models/HarrisWilsonNW/ABM.py
@@ -33,12 +33,12 @@ class HarrisWilsonABM:
         self.M = M
 
         # Model parameters
-        self.alpha = torch.tensor(alpha).to(device)
-        self.beta = torch.tensor(beta).to(device)
-        self.kappa = torch.tensor(kappa).to(device)
-        self.sigma = torch.tensor(sigma).to(device)
-        self.epsilon = torch.tensor(epsilon).to(device)
-        self.dt = torch.tensor(dt).to(device)
+        self.alpha = torch.tensor(alpha, dtype=torch.float).to(device)
+        self.beta = torch.tensor(beta, dtype=torch.float).to(device)
+        self.kappa = torch.tensor(kappa, dtype=torch.float).to(device)
+        self.sigma = torch.tensor(sigma, dtype=torch.float).to(device)
+        self.epsilon = torch.tensor(epsilon, dtype=torch.float).to(device)
+        self.dt = torch.tensor(dt, dtype=torch.float).to(device)
         self.device = device
 
     # ... Model run functions ..........................................................................................
diff --git a/models/HarrisWilsonNW/DataGeneration.py b/models/HarrisWilsonNW/DataGeneration.py
index f6bfd7c..e353c29 100644
--- a/models/HarrisWilsonNW/DataGeneration.py
+++ b/models/HarrisWilsonNW/DataGeneration.py
@@ -71,6 +71,25 @@ def generate_synthetic_data(*, cfg) -> Tuple[torch.tensor, torch.tensor, torch.t
     :returns the origin sizes, network, and the time series
     """
 
+    def _distributed_values(*, distribution: str, parameters: dict, size: tuple, **__):
+
+        """Generates random variables"""
+
+        # Uniform distribution in an interval
+        if distribution == "uniform":
+            return (parameters.get("upper") - parameters.get("lower")) * torch.rand(
+                size, dtype=torch.float
+            ) + parameters.get("lower")
+
+        # Normal distribution
+        elif distribution == "normal":
+            return torch.normal(
+                parameters.get("mean"), parameters.get("std"), size=size
+            )
+
+        else:
+            raise ValueError(f"  Unrecognised distribution type {distribution}!")
+
     log.note("   Generating synthetic data ...")
 
     # Get run configuration properties
@@ -82,17 +101,15 @@ def generate_synthetic_data(*, cfg) -> Tuple[torch.tensor, torch.tensor, torch.t
     network: torch.tensor = torch.exp(
         -1
         * torch.abs(
-            torch.normal(
-                data_cfg["init_weights"]["mean"],
-                data_cfg["init_weights"]["std"],
-                size=(N_origin, N_destination),
+            _distributed_values(
+                **data_cfg.get("init_network_weights"), size=(N_origin, N_destination)
             )
         )
     )
-    network = network * torch.bernoulli(network)
+    network = network * torch.bernoulli(network)  # what if we turn this off?
 
     # Normalise the rowsums
-    norms = torch.sum(network, axis=1, keepdim=True)
+    norms = torch.sum(network, dim=1, keepdim=True)
     network /= torch.where(norms != 0, norms, 1)
 
     # Extract the underlying parameters from the config
@@ -110,7 +127,7 @@ def generate_synthetic_data(*, cfg) -> Tuple[torch.tensor, torch.tensor, torch.t
         M=data_cfg["N_destination"],
         dt=data_cfg["dt"],
         device="cpu",
-        **true_parameters
+        **true_parameters,
     )
     origin_sizes, dest_sizes = [], []
 
@@ -118,11 +135,17 @@ def generate_synthetic_data(*, cfg) -> Tuple[torch.tensor, torch.tensor, torch.t
 
         # Generate the initial destination zone sizes
         init_dest_sizes = torch.abs(
-            torch.normal(1, 0.1, size=(data_cfg["N_destination"], 1))
+            _distributed_values(
+                **data_cfg.get("init_dest_sizes"), size=(data_cfg["N_destination"], 1)
+            )
         )
 
         # Generate the origin sizes time series
-        or_sizes = torch.abs(torch.normal(1, 0.1, size=(1, N_origin, 1)))
+        or_sizes = torch.abs(
+            _distributed_values(
+                **data_cfg.get("init_origin_sizes"), size=(1, N_origin, 1)
+            )
+        )
 
         if data_cfg["origin_size_std"] == 0:
             or_sizes = or_sizes.repeat(num_steps, 1, 1)
diff --git a/models/HarrisWilsonNW/HarrisWilsonNW_base_plots.yml b/models/HarrisWilsonNW/HarrisWilsonNW_base_plots.yml
index b1e358e..b9b4f77 100644
--- a/models/HarrisWilsonNW/HarrisWilsonNW_base_plots.yml
+++ b/models/HarrisWilsonNW/HarrisWilsonNW_base_plots.yml
@@ -51,7 +51,7 @@
   helpers:
     save_figure:
       dpi: 900
-  file_ext: png
+  file_ext: pdf
 
 .calculate_clustering_coefficient:
   dag_options:
@@ -222,6 +222,28 @@ loss:
     set_scales:
       y: log
 
+losses_combined:
+  based_on: .line_universe
+  select:
+    training_loss:
+      path: output_data/training_loss
+      transform: [.data]
+    Frobenius_error:
+      path: output_data/frobenius_error
+      transform: [.data]
+  transform:
+    - pd.Index: [ [ 'Training loss', 'Frobenius error' ] ]
+      kwargs: {name: 'loss type'}
+    - xr.concat: [[!dag_tag training_loss, !dag_tag Frobenius_error], !dag_prev ]
+      tag: data
+  hue: loss type
+  helpers:
+    set_labels:
+      x: iteration
+      y: loss
+    set_scales:
+      y: log
+
 density:
   based_on: .line_universe
   select:
diff --git a/models/HarrisWilsonNW/HarrisWilsonNW_cfg.yml b/models/HarrisWilsonNW/HarrisWilsonNW_cfg.yml
index 416be81..7cb2009 100644
--- a/models/HarrisWilsonNW/HarrisWilsonNW_cfg.yml
+++ b/models/HarrisWilsonNW/HarrisWilsonNW_cfg.yml
@@ -12,16 +12,32 @@ Data:
     sigma: !is-positive-or-zero &sigma 0
     epsilon: !is-positive &epsilon 10
     dt: !is-positive &dt 0.01
-    init_weights:
-      mean: !param
-        default: 1.2
-        limits: [0, ~]
-        dtype: float
-      std: !param
-        default: 1.2
-        limits: [0, ~]
-        dtype: float
+
+    # Initialisation of the network weights
+    init_network_weights:
+      distribution: normal
+      parameters:
+        mean: 1.2
+        std: 1.2
+
+    # Initialisation of the origin sizes
+    init_origin_sizes:
+      distribution: normal
+      parameters:
+        mean: 0.01
+        std: 0.001
+
+    # Fluctuations of the origin zones over time
     origin_size_std: 0.0
+
+    # Initialisation of the destination sizes
+    init_dest_sizes:
+      distribution: normal
+      parameters:
+        mean: 0.1
+        std: 0.01
+
+  # Number of independently training sets to use
   training_data_size: 1
 
 # Settings for the neural net architecture
diff --git a/models/HarrisWilsonNW/cfgs/N_750_example/run.yml b/models/HarrisWilsonNW/cfgs/N_750_example/run.yml
index 1f7fd15..ac43232 100644
--- a/models/HarrisWilsonNW/cfgs/N_750_example/run.yml
+++ b/models/HarrisWilsonNW/cfgs/N_750_example/run.yml
@@ -1,6 +1,6 @@
 ---
 paths:
-  model_note: N_1000_example
+  model_note: N_750_example
 parameter_space:
   num_epochs: 40
   write_predictions_every: 2
diff --git a/models/HarrisWilsonNW/cfgs/Sample_run/eval.yml b/models/HarrisWilsonNW/cfgs/Sample_run/eval.yml
index 382cc7d..c9724db 100644
--- a/models/HarrisWilsonNW/cfgs/Sample_run/eval.yml
+++ b/models/HarrisWilsonNW/cfgs/Sample_run/eval.yml
@@ -25,7 +25,16 @@ training_data:
     data:
       path: training_data/training_data
   style:
-    figure.figsize: [ *half_width, *fifth_width ]
+    figure.figsize: [ *third_width, 1.0 ]
+  helpers:
+    set_limits:
+      x: [0, ~]
+    set_ticks:
+      x:
+        major:
+          locs: [0, 200, 400]
+          labels: ['', '', '']
+
 
 origin_sizes:
   based_on: training_data
@@ -34,4 +43,160 @@ origin_sizes:
       path: training_data/origin_sizes
   helpers:
     set_labels:
+      x: ' '
       y: $O_i$
+    set_limits:
+      x: [500, 1000]
+    set_ticks:
+      x:
+        major:
+          locs: [500, 700, 900]
+          labels: ['', '', '']
+
+loss:
+  based_on: losses_combined
+
+matrices/true_adjacency_matrix:
+  based_on: adjacency_matrix
+
+matrices/prediction:
+  based_on: adjacency_matrix
+  select:
+    data:
+      path: output_data/predictions
+      transform:
+        - .isel: [!dag_prev , {time: -1}]
+
+matrices/accuracy:
+  based_on: accuracy
+
+matrices/accuracy_on_true_edges:
+  based_on: accuracy_on_true_edges
+
+matrices/accuracy_on_false_edges:
+  based_on: accuracy_on_false_edges
+
+matrices/ratio:
+  based_on: .matrix
+  select:
+    true_data: true_network/_adjacency_matrix
+    predicted_data:
+      path: output_data/predictions
+      transform:
+        - .isel: [ !dag_prev , { time: -1 } ]
+  transform:
+    - div: [!dag_tag true_data, !dag_tag predicted_data]
+      tag: data
+  vmin: ~
+  vmax: ~
+
+# Plot the synthetic time series
+time_series:
+  based_on: time_series
+  select:
+    data:
+      transform:
+        - .isel: [ !dag_prev , { training_set: 0 } ]
+          kwargs: { drop: true }
+
+predicted_time_series:
+  based_on: time_series
+  select:
+    data: output_data/predicted_time_series
+
+degree:
+  based_on: .multiplot_universe
+  dag_options:
+    meta_operations:
+      hist:
+        - np.linspace: [ 0, 20, 500 ]
+        - NeuralABM.hist: [ !arg 0 ]
+          kwargs: { bins: !dag_node -1, axis: 1 }
+  select:
+    param_binned:
+      path: output_data/predictions
+      transform:
+        - .sum: [!dag_prev , i ]
+        - hist: [!dag_prev ]
+    true_val:
+      path: true_network/_adjacency_matrix
+      transform:
+        - .sum: [!dag_prev , i ]
+        - hist: [!dag_prev ]
+    loss:
+      path: output_data/training_loss
+      transform:
+        - mul: [!dag_prev , -1]
+        - np.exp: [!dag_prev ]
+
+  transform:
+
+    # Get the true value
+    - NeuralABM.flatten_dims: [ !dag_tag true_val , { sample: [ time ] } ]
+    - NeuralABM.normalise_degrees_to_edges: [ !dag_prev ]
+    - .squeeze: [ !dag_prev ]
+    - .to_dataset: [!dag_prev ]
+      kwargs: {name: y}
+      tag: true_param
+
+    # Calculate mean, MLE, and error
+    - NeuralABM.flatten_dims: [ !dag_tag param_binned , { sample: [ time ] } ]
+    - NeuralABM.normalise_degrees_to_edges: [!dag_prev ]
+      tag: samples
+    - NeuralABM.flatten_dims: [ !dag_tag loss , { sample: [ epoch ] } ]
+    - .expand_dims: [!dag_prev ]
+      kwargs:
+        bin_center: 1
+        axis: -1
+      tag: loss_flattened
+    - .sum: [!dag_prev , 'sample']
+    - div: [!dag_tag loss_flattened, !dag_prev ]
+      tag: loss_normalised
+    - .argmax: [!dag_tag loss ]
+    - NeuralABM.marginal_of_density: [ !dag_tag samples ]
+      kwargs:
+        loss: !dag_tag loss_normalised
+        error: Hellinger
+        MLE_index: -1
+      tag: data
+    - div: [!dag_tag loss_flattened , 20 ]
+      tag: losses
+    - .data: [!dag_tag loss_flattened]
+    - .to_dataset: [ !dag_tag samples ]
+      kwargs: { name: y }
+      tag: distributions
+  to_plot:
+    - function: [ model_plots.HarrisWilson, plot_prob_density ]
+      args: [ !dag_result distributions ]
+      y: y
+      hue: sample
+      alpha: !dag_result losses
+      lw: 0.3 #!dag_result losses
+      suppress_labels: true
+      color: *darkblue
+      pass_helper: True
+    - function: [ model_plots.HarrisWilson, plot_prob_density ]
+      args: [ !dag_result data ]
+      x: bin_center
+      y: MLE
+      yerr: yerr
+      label: $\hat{P}(k)$
+      pass_helper: true
+      color: *grey
+    - function: [model_plots.HarrisWilson, plot_prob_density]
+      args: [ !dag_result true_param ]
+      y: y
+      linestyle: dotted
+      color: *red
+      label: $P(k)$
+      pass_helper: True
+  x: bin_center
+  smooth_kwargs:
+    enabled: True
+    sigma: 2.8
+  helpers:
+    set_limits:
+      x: [0, 5]
+    set_labels:
+      x: Weighted degree $k$
+      y: $P(k)$
diff --git a/models/HarrisWilsonNW/cfgs/Sample_run/run.yml b/models/HarrisWilsonNW/cfgs/Sample_run/run.yml
index 8958eb9..30e1ec0 100644
--- a/models/HarrisWilsonNW/cfgs/Sample_run/run.yml
+++ b/models/HarrisWilsonNW/cfgs/Sample_run/run.yml
@@ -1,25 +1,27 @@
 ---
+paths:
+  model_note: Sample_run
 parameter_space:
-  num_epochs: 150
-  write_predictions_every: -1
+  num_epochs: 100
+  write_predictions_every: 1
   write_every: 1
   seed: 39
   HarrisWilsonNW:
     Data:
-      num_training_steps: 500
-      training_set_size: 1
+      num_training_steps: 100
+      training_set_size: 50
       synthetic_data:
-        N_origin: 5
+        N_origin: 20
         N_destination: 10
         dt: &dt 0.001
-        alpha: &alpha 0.8
-        beta: &beta 4
-        kappa: &kappa 3
+        alpha: &alpha 0.2
+        beta: &beta 1.2
+        kappa: &kappa 1
         sigma: &sigma 0.0
         epsilon: &epsilon 10
         write_start: 1
         write_every: 1
-        num_steps: 1000
+        num_steps: 600
         origin_size_std: 0.1
     Training:
       batch_size: 10
@@ -27,10 +29,10 @@ parameter_space:
         alpha: *alpha
         beta: *beta
         kappa: *kappa
-        sigma: *sigma
+        sigma: 0.0
         epsilon: *epsilon
         dt: *dt
-      learning_rate: 0.002
+      learning_rate: 0.02
     NeuralNet:
       activation_funcs:
         layer_specific:
diff --git a/models/HarrisWilsonNW/run.py b/models/HarrisWilsonNW/run.py
index b98cb27..7cc3730 100755
--- a/models/HarrisWilsonNW/run.py
+++ b/models/HarrisWilsonNW/run.py
@@ -174,19 +174,24 @@ class HarrisWilson_NN:
             if batches[-1] != training_data.shape[1] - 1:
                 batches = np.append(batches, training_data.shape[1] - 1)
 
+        # Track the number of gradient descent updates
+        counter = 0
+
+        # Make an initial prediction
+        predicted_parameters = self.neural_net(torch.flatten(training_data[0, 0]))
+        pred_adj_matrix = torch.reshape(
+            predicted_parameters, (self.n_origin, self.n_dest)
+        )
+
+        loss = torch.tensor(0.0, requires_grad=True)
+
         for batch_no, batch_idx in enumerate(batches[:-1]):
 
             for i, dset in enumerate(training_data):
 
-                predicted_parameters = self.neural_net(torch.flatten(dset[batch_idx]))
-                pred_adj_matrix = torch.reshape(
-                    predicted_parameters, (self.n_origin, self.n_dest)
-                )
                 current_values = dset[batch_idx].clone()
                 current_values.requires_grad_(True)
 
-                loss = torch.tensor(0.0, requires_grad=True)
-
                 for ele in range(batch_idx + 1, batches[batch_no + 1] + 1):
 
                     # Solve the ODE
@@ -199,27 +204,43 @@ class HarrisWilson_NN:
                     )
 
                     # Calculate the loss
-                    loss = loss + (
-                        self.loss_function(current_values, dset[ele])
-                        + self.loss_function(
-                            torch.sum(pred_adj_matrix, axis=1),
-                            torch.ones(self.n_origin),
-                        )
-                    ) / (batches[batch_no + 1] - batch_idx + 1)
-
-                loss.backward()
-                self.neural_net.optimizer.step()
-                self.neural_net.optimizer.zero_grad()
-                self.current_loss += loss.clone().detach().numpy().item()
-                self.current_frob_error += (
-                    torch.nn.functional.mse_loss(self.true_network, pred_adj_matrix)
-                    .clone()
-                    .detach()
-                    .numpy()
-                    .item()
-                )
-                self.current_predictions = predicted_parameters.clone().detach()
-                self.current_adjacency_matrix = pred_adj_matrix.clone().detach()
+                    loss = loss + (self.loss_function(current_values, dset[ele])) / (
+                        batches[batch_no + 1] - batch_idx + 1
+                    )
+
+                    counter += 1
+
+                if counter % batch_size == 0:
+
+                    # Enforce row sum normalisation
+                    loss = loss + self.loss_function(
+                        torch.sum(pred_adj_matrix, dim=1),
+                        torch.ones(self.n_origin),
+                    )
+
+                    loss.backward()
+                    self.neural_net.optimizer.step()
+                    self.neural_net.optimizer.zero_grad()
+                    self.current_loss = loss.clone().detach().numpy().item()
+                    self.current_frob_error = (
+                        torch.nn.functional.mse_loss(self.true_network, pred_adj_matrix)
+                        .clone()
+                        .detach()
+                        .numpy()
+                        .item()
+                    )
+                    self.current_predictions = predicted_parameters.clone().detach()
+                    self.current_adjacency_matrix = pred_adj_matrix.clone().detach()
+
+                    predicted_parameters = self.neural_net(
+                        torch.flatten(dset[batches[batch_no + 1]])
+                    )
+                    pred_adj_matrix = torch.reshape(
+                        predicted_parameters, (self.n_origin, self.n_dest)
+                    )
+
+                    del loss
+                    loss = torch.tensor(0.0, requires_grad=True)
 
         self._time += 1
         self.write_data()
diff --git a/models/Kuramoto/ABM.py b/models/Kuramoto/ABM.py
index c690208..9499211 100644
--- a/models/Kuramoto/ABM.py
+++ b/models/Kuramoto/ABM.py
@@ -1,3 +1,5 @@
+import logging
+
 import torch
 
 """ The Kuramoto model of synchronised oscillation """
@@ -5,14 +7,7 @@ import torch
 # --- The Kuramoto ABM ------------------------------------------------------------------------------------------
 class Kuramoto_ABM:
     def __init__(
-        self,
-        *,
-        N: int,
-        sigma: float = 0,
-        dt: float = 0.01,
-        gamma: float = 1,
-        eigen_frequencies: torch.Tensor,
-        **__
+        self, *, N: int, sigma: float, dt: float, gamma: float, device: str, **__
     ):
 
         """The Kuramoto model numerical solver, for first and second-order dynamics.
@@ -21,7 +16,6 @@ class Kuramoto_ABM:
         :param sigma: the default noise variance
         :param dt: the time differential to use
         :param gamma: the parameter used for the second-order model
-        :param eigen_frequencies: the eigenfrequencies of the oscillators
         :param **__: other kwargs (ignored)
         """
 
@@ -29,12 +23,11 @@ class Kuramoto_ABM:
         self.N = N
 
         # Scalar parameter: noise variance
-        self.sigma = sigma
-        self.dt = dt
-        self.gamma = gamma
+        self.sigma = torch.tensor(sigma, device=device, dtype=torch.float)
+        self.dt = torch.tensor(dt, device=device, dtype=torch.float)
+        self.gamma = torch.tensor(gamma, device=device, dtype=torch.float)
 
-        # Initialise the opinions uniformly at random on the unit interval
-        self.eigen_frequencies = eigen_frequencies
+        self.device = device
 
     def run_single(
         self,
@@ -42,6 +35,7 @@ class Kuramoto_ABM:
         current_phases: torch.tensor,
         current_velocities: torch.tensor = None,
         adjacency_matrix: torch.tensor,
+        eigen_frequencies: torch.tensor,
         sigma: float = None,
         requires_grad: bool = True,
     ):
@@ -59,35 +53,43 @@ class Kuramoto_ABM:
         """
 
         sigma = self.sigma if sigma is None else sigma
-
         new_phases = current_phases.clone().detach().requires_grad_(requires_grad)
+
         diffs = torch.sin(current_phases - torch.reshape(current_phases, (self.N,)))
 
         # First-order dynamics
         if current_velocities is None:
+
             new_phases = (
                 new_phases
                 + (
-                    self.eigen_frequencies
+                    eigen_frequencies
                     + torch.reshape(
                         torch.matmul(adjacency_matrix, diffs).diag(), (self.N, 1)
                     )
                 )
                 * self.dt
-                + torch.normal(0.0, sigma, (self.N, 1))
+                + torch.normal(0.0, sigma, (self.N, 1), device=self.device)
             )
 
         # Second-order dynamics
         else:
+
             new_phases = (
-                    new_phases
-                    + ((self.eigen_frequencies
+                new_phases
+                + (
+                    (
+                        eigen_frequencies
                         + torch.reshape(
-                        torch.matmul(adjacency_matrix, diffs).diag(), (self.N, 1)
+                            torch.matmul(adjacency_matrix, diffs).diag(), (self.N, 1)
+                        )
+                        - self.gamma * current_velocities
                     )
-                        - self.gamma * current_velocities) * self.dt + current_velocities
-                       ) * self.dt
-                    + torch.normal(0.0, sigma, (self.N, 1))
+                    * self.dt
+                    + current_velocities
+                )
+                * self.dt
+                + torch.normal(0.0, sigma, (self.N, 1))
             )
 
         return new_phases
diff --git a/models/Kuramoto/DataGeneration.py b/models/Kuramoto/DataGeneration.py
index 662309a..aff39a6 100644
--- a/models/Kuramoto/DataGeneration.py
+++ b/models/Kuramoto/DataGeneration.py
@@ -21,10 +21,16 @@ log = logging.getLogger(__name__)
 
 
 def get_data(
-    cfg, h5file: h5.File, h5group: h5.Group, *, seed: int, device: str, second_order: bool,
+    cfg,
+    h5file: h5.File,
+    h5group: h5.Group,
+    *,
+    seed: int,
+    device: str,
+    second_order: bool,
 ) -> (torch.Tensor, Union[nx.Graph, None]):
 
-    """ Either loads data from an external file or synthetically generates Kuramoto data (including the network)
+    """Either loads data from an external file or synthetically generates Kuramoto data (including the network)
     from a configuration file.
 
     :param cfg: The configuration file containing either the paths to files to be loaded or the configuration settings
@@ -35,6 +41,28 @@ def get_data(
     :param device: the training device to which to move the data
     :return: the training data and, if given, the network
     """
+
+    def _distributed_values(
+        *, distribution: str, parameters: dict, size: tuple, device: str, **__
+    ):
+
+        """Generates random variables"""
+
+        # Uniform distribution in an interval
+        if distribution == "uniform":
+            return (parameters.get("upper") - parameters.get("lower")) * torch.rand(
+                size, dtype=torch.float, device=device
+            ) + parameters.get("lower")
+
+        # Normal distribution
+        elif distribution == "normal":
+            return torch.normal(
+                parameters.get("mean"), parameters.get("std"), size=size, device=device
+            )
+
+        else:
+            raise ValueError(f"  Unrecognised distribution type {distribution}!")
+
     load_from_dir = cfg.pop("load_from_dir", {})
     write_adjacency_matrix = cfg.pop("write_adjacency_matrix", load_from_dir == {})
 
@@ -47,7 +75,7 @@ def get_data(
 
             # Load the training data
             training_data = torch.from_numpy(
-                np.array(f["training_data"]["training_data"])
+                np.array(f["training_data"]["phases"])
             ).float()
 
     if load_from_dir.get("network", None) is not None:
@@ -82,17 +110,23 @@ def get_data(
     if load_from_dir.get("eigen_frequencies", None) is not None:
 
         log.info("   Loading eigenfrequencies")
-        with h5.File(load_from_dir["eigen_frequencies"], "r") as f:
+        with h5.File(load_from_dir["training_data"], "r") as f:
 
             # Load the network
             eigen_frequencies = torch.from_numpy(
-                np.array(f["true_network"]["_eigen_frequencies"])
+                np.array(f["training_data"]["eigen_frequencies"])
             ).float()
-            eigen_frequencies = torch.reshape(eigen_frequencies, (-1, 1))
 
     # Get the config and number of agents
+    dt = cfg.get("dt")
+    gamma = cfg.get("gamma", 1)
     cfg = cfg.get("synthetic_data")
     nw_cfg = cfg.pop("network", {})
+
+    # If network was loaded, set the number of nodes to be the network size
+    if network is not None:
+        cfg.update(dict(N=network.number_of_nodes()))
+    cfg.update(dict(dt=dt, gamma=gamma))
     N: int = cfg["N"]
 
     # If network was not loaded, generate the network
@@ -105,13 +139,21 @@ def get_data(
     if eigen_frequencies is None:
 
         log.info("   Generating eigenfrequencies  ...")
-        eigen_frequencies = 2 * torch.rand((N, 1), dtype=torch.float) + 1
 
-    nx.set_node_attributes(
-        network,
-        {idx: val for idx, val in enumerate(eigen_frequencies)},
-        "eigen_frequency",
-    )
+        num_steps: int = cfg.get("num_steps")
+        training_set_size: int = cfg.get("training_set_size")
+
+        # If set, generate a time series of i.i.d distributed eigenfrequencies
+        if cfg.get("eigen_frequencies")["time_series"]:
+            eigen_frequencies = _distributed_values(
+                **cfg.get("eigen_frequencies"),
+                size=(training_set_size, num_steps + 1, N, 1),
+                device=device,
+            )
+        else:
+            eigen_frequencies = _distributed_values(
+                **cfg.get("eigen_frequencies"), size=(1, 1, N, 1), device=device
+            ).repeat(training_set_size, num_steps + 1, 1, 1)
 
     # If training data was not loaded, generate
     if training_data is None:
@@ -119,35 +161,46 @@ def get_data(
         log.info("   Generating training data ...")
         num_steps: int = cfg.get("num_steps")
         training_set_size = cfg.get("training_set_size")
-        training_data = torch.empty((training_set_size, num_steps + 1, N, 1))
+        training_data = torch.empty(
+            (training_set_size, num_steps + 1, N, 1), device=device
+        )
 
-        adj_matrix = torch.from_numpy(nx.to_numpy_matrix(network)).float()
+        adj_matrix = torch.from_numpy(nx.to_numpy_matrix(network)).float().to(device)
 
-        ABM = Kuramoto_ABM(**cfg, eigen_frequencies=eigen_frequencies)
+        ABM = Kuramoto_ABM(**cfg, device=device)
 
         for idx in range(training_set_size):
 
-            training_data[idx, 0, :, :] = 2 * torch.pi * torch.rand(N, 1)
-            i_0 = 0
+            training_data[idx, 0, :, :] = _distributed_values(
+                **cfg.get("init_phases"), size=(N, 1), device=device
+            )
 
             # For the second-order dynamics, the initial velocities must also be given
             if second_order:
-                training_data[idx, 1, :, :] = training_data[idx, 0, :, :] + torch.rand(N, 1)
-                i_0 = 1
+                training_data[idx, 1, :, :] = training_data[idx, 0, :, :] + torch.rand(
+                    N, 1
+                )
+
+            # Second order dynamics require an additional initial condition and so start one step later
+            t_0 = 0 if not second_order else 1
 
             # Run the ABM for n iterations and write the data
-            for i in range(i_0, num_steps):
+            for i in range(t_0, num_steps):
                 training_data[idx, i + 1] = ABM.run_single(
                     current_phases=training_data[idx, i],
-                    current_velocities=(training_data[idx, i] - training_data[idx, i-1])/ABM.dt if second_order else None,
+                    current_velocities=(
+                        training_data[idx, i] - training_data[idx, i - 1]
+                    )
+                    / ABM.dt
+                    if second_order
+                    else None,
                     adjacency_matrix=adj_matrix,
+                    eigen_frequencies=eigen_frequencies[idx, i],
                     requires_grad=False,
                 )
 
         log.info("   Training data generated.")
 
-        del ABM
-
     # Save the data. If data was loaded, data can be copied if specified
     if load_from_dir.get("copy_data", True):
 
@@ -156,49 +209,54 @@ def get_data(
         nw_group.attrs["content"] = "graph"
         nw_group.attrs["allows_parallel"] = False
         nw_group.attrs["is_directed"] = network.is_directed()
-
-        # Save the network
         base.save_nw(network, nw_group, write_adjacency_matrix)
+        log.info("   Network generated and saved.")
 
-        # Vertex properties: eigenfrequencies
-        eigen_frequencies = nw_group.create_dataset(
-            "_eigen_frequencies",
-            (1, network.number_of_nodes()),
+        # Save the eigenfrequencies
+        dset_eigen_frequencies = h5group.create_dataset(
+            "eigen_frequencies",
+            eigen_frequencies.shape,
             chunks=True,
             compression=3,
             dtype=float,
         )
-        eigen_frequencies.attrs["dim_names"] = ["time", "vertex_idx"]
-        eigen_frequencies.attrs["coords_mode__vertex_idx"] = "trivial"
-
-        # Write node properties
-        eigen_frequencies[0, :] = (
-            torch.stack(list(nx.get_node_attributes(network, "eigen_frequency").values()))
-                .numpy()
-                .flatten()
-        )
-        log.info("   Network generated and saved.")
+        dset_eigen_frequencies.attrs["dim_names"] = [
+            "training_set",
+            "time",
+            "vertex_idx",
+            "dim_name__0",
+        ]
+        dset_eigen_frequencies.attrs["coords_mode__training_set"] = "trivial"
+        dset_eigen_frequencies.attrs["coords_mode__time"] = "values"
+        dset_eigen_frequencies.attrs["coords__time"] = [
+            0 + n * dt for n in range(training_data.shape[1])
+        ]
+        dset_eigen_frequencies.attrs["coords_mode__vertex_idx"] = "values"
+        dset_eigen_frequencies.attrs["coords__vertex_idx"] = network.nodes()
+        dset_eigen_frequencies[:, :] = eigen_frequencies.cpu()
 
         # Save training data
-        dset_training_data = h5group.create_dataset(
-            "training_data",
+        dset_phases = h5group.create_dataset(
+            "phases",
             training_data.shape,
             chunks=True,
             compression=3,
         )
-        dset_training_data.attrs["dim_names"] = [
-            "runs",
+        dset_phases.attrs["dim_names"] = [
+            "training_set",
             "time",
             "vertex_idx",
             "dim_name__0",
         ]
-        dset_training_data.attrs["coords_mode__runs"] = "trivial"
-        dset_training_data.attrs["coords_mode__time"] = "start_and_step"
-        dset_training_data.attrs["coords__time"] = [1, 1]
-        dset_training_data.attrs["coords_mode__vertex_idx"] = "trivial"
+        dset_phases.attrs["coords_mode__training_set"] = "trivial"
+        dset_phases.attrs["coords_mode__time"] = "values"
+        dset_phases.attrs["coords__time"] = [
+            0 + n * dt for n in range(training_data.shape[1])
+        ]
+        dset_phases.attrs["coords_mode__vertex_idx"] = "values"
+        dset_phases.attrs["coords__vertex_idx"] = network.nodes()
 
-        dset_training_data[:, :] = training_data
+        dset_phases[:, :] = training_data.cpu()
 
     # Return the training data and the network
-    return training_data.to(device), network
-
+    return training_data, eigen_frequencies, network
diff --git a/models/Kuramoto/Kuramoto_base_plots.yml b/models/Kuramoto/Kuramoto_base_plots.yml
index cb4573a..118e8c5 100644
--- a/models/Kuramoto/Kuramoto_base_plots.yml
+++ b/models/Kuramoto/Kuramoto_base_plots.yml
@@ -54,7 +54,7 @@
       dpi: 900
     set_legend:
       facecolor: None
-  file_ext: png
+  file_ext: pdf
 
 
 # ======================================================================================================================
@@ -126,89 +126,26 @@
   module: model_plots.HarrisWilson
   plot_func: plot_prob_density
 
-.marginals_density:
-  dag_options:
-    define:
-      n_bins: 100
-      min_bin: -1
-      max_bin: 16
-      bw_method: 0.5
-      sigma: 6
-  to_plot:
-    - function: [ model_plots.HarrisWilson, plot_prob_density ]
-      args: [ !dag_result distributions ]
-      x: bin_center
-      y: y
-      smooth_kwargs:
-        enabled: True
-        sigma: !dag_result sigma
-      pass_helper: true
-      hue: sample
-      alpha: !dag_result losses
-      lw: !dag_result losses
-      suppress_labels: True
-      color: *grey
-    - function: [model_plots.HarrisWilson, plot_prob_density]
-      args: [!dag_result data]
-      x: bin_center
-      y: MLE
-      yerr: yerr
-      smooth_kwargs:
-        enabled: True
-        sigma: !dag_result sigma
-      pass_helper: true
-      color: black
-      linestyle: dotted
-      label: MLE
-    - function: [model_plots.HarrisWilson, plot_prob_density]
-      args: [ !dag_result true_param ]
-      x: bin_center
-      y: y
-      smooth_kwargs:
-        enabled: True
-        sigma: !dag_result sigma
-      linestyle: dotted
-      color: *red
-      pass_helper: true
-  helpers:
-    set_legend:
-      use_legend: true
-    set_limits:
-      y: [0, ~]
-    set_tick_locators:
-      x: &formatting
-        major:
-          name: MaxNLocator
-          integer: true
-          nbins: !dag_result num_agents
+.marginals:
+  module: model_plots.HarrisWilson
+  plot_func: plot_prob_density
 
 # ======================================================================================================================
 #  ╦ ╦╔╗╔╦╦  ╦╔═╗╦═╗╔═╗╔═╗  ╔═╗╦  ╔═╗╔╦╗╔═╗
 #  ║ ║║║║║╚╗╔╝║╣ ╠╦╝╚═╗║╣   ╠═╝║  ║ ║ ║ ╚═╗
 #  ╚═╝╝╚╝╩ ╚╝ ╚═╝╩╚═╚═╝╚═╝  ╩  ╩═╝╚═╝ ╩ ╚═╝
 # ======================================================================================================================
-# Training loss
 loss:
-  based_on: .line_universe
-  select:
-    data: output_data/Training loss
-  helpers:
-    set_labels:
-      x: iteration
-    set_scales:
-      y: log
-
-losses_combined:
   based_on: .line_universe
   select:
     training_loss:
       path: output_data/Training loss
       transform: [.data]
     Frobenius_error:
-      path: output_data/Frobenius error
+      path: output_data/Prediction error
       transform: [.data]
   transform:
-    - pd.Index: [ [ 'Training loss', 'Frobenius error' ] ]
+    - pd.Index: [ [ 'Training loss', 'Prediction error' ] ]
       kwargs: {name: 'loss type'}
     - xr.concat: [[!dag_tag training_loss, !dag_tag Frobenius_error], !dag_prev ]
       tag: data
@@ -220,172 +157,6 @@ losses_combined:
     set_scales:
       y: log
 
-density:
-  based_on: .line_universe
-  select:
-    predicted_size: output_data/network_size
-    num_vertices:
-      path: true_network/_vertices
-      transform:
-        - .coords: [!dag_prev , 'vertex_idx']
-        - len: [!dag_prev ]
-    true_size:
-      path: true_network/_edge_weights
-      transform:
-        - .isel: [!dag_prev , {time: -1}]
-          kwargs:
-            drop: True
-        - len: [!dag_prev ]
-        - mul: [!dag_prev , 2]
-  transform:
-    - sub: [!dag_tag num_vertices, 1 ]
-    - mul: [!dag_tag num_vertices, !dag_prev ]
-      tag: complete_size
-    - div: [!dag_tag true_size, !dag_tag complete_size]
-      tag: target_size
-    - div: [!dag_tag predicted_size, !dag_tag complete_size]
-      tag: data
-  helpers:
-    set_hv_lines:
-      hlines:
-        - pos: !dag_result target_size
-          linestyle: dashed
-          color: gray
-    set_labels:
-      y: Network density
-
-# Comparison of true and predicted degrees
-degree:
-  based_on: .multiplot_universe
-  select:
-    true_degree:
-      path: true_network/_degree_weighted
-      transform:
-        - .isel: [!dag_prev , {time: -1}]
-          kwargs:
-            drop: True
-        - .data: [!dag_prev ]
-    predicted_out_degree:
-      path: predicted_network/_out_degree_weighted
-      transform:
-        - .isel: [ !dag_prev , { time: -1 } ]
-          kwargs:
-            drop: True
-        - .data: [ !dag_prev ]
-    predicted_in_degree:
-      path: predicted_network/_in_degree_weighted
-      transform:
-        - .isel: [ !dag_prev , { time: -1 } ]
-          kwargs:
-            drop: True
-        - .data: [ !dag_prev ]
-  to_plot:
-    - function: sns.kdeplot
-      args:
-        - !dag_result true_degree
-      label: $P(k)$
-    - function: sns.kdeplot
-      args:
-        - !dag_result predicted_out_degree
-      label: $\hat{P}_\mathrm{out}(k)$
-    - function: sns.kdeplot
-      args:
-        - !dag_result predicted_in_degree
-      label: $\hat{P}_\mathrm{in}(k)$
-  helpers:
-    set_legend:
-      use_legend: True
-      title: ~
-      loc: 'best'
-    set_title:
-      title: Node degrees
-
-# Predicted mean degree over time
-mean_degree_over_time:
-  based_on: .line_universe
-  select:
-    in_degree:
-      path: predicted_network/_in_degree
-      transform: [.data]
-    in_degree_w:
-      path: predicted_network/_in_degree_weighted
-      transform: [.data]
-    out_degree:
-      path: predicted_network/_out_degree
-      transform: [.data]
-    out_degree_w:
-      path: predicted_network/_out_degree_weighted
-      transform: [.data]
-  transform:
-    - pd.Index: [ [ 'in degree', 'weighted in degree', 'out degree', 'weighted out degree' ] ]
-      kwargs:
-        name: 'type'
-    - xr.concat: [[!dag_tag in_degree, !dag_tag in_degree_w, !dag_tag out_degree, !dag_tag out_degree_w], !dag_prev ]
-    - .mean: [!dag_prev , 'vertex_idx']
-      tag: data
-  hue: type
-  x: time
-
-# Predicted clustering over time
-clustering_over_time:
-  based_on:
-    - .line_universe
-    - .calculate_clustering_coefficient
-  transform:
-    - calculate_clustering_coefficient:
-        in_degree: data/predicted_network/_in_degree
-        out_degree: data/predicted_network/_out_degree
-        triangles: data/predicted_network/_triangles
-      tag: clustering
-    - calculate_clustering_coefficient:
-        in_degree: data/predicted_network/_in_degree_weighted
-        out_degree: data/predicted_network/_out_degree_weighted
-        triangles: data/predicted_network/_weighted_triangles
-      tag: clustering_weighted
-    - pd.Index: [ [ 'clustering coefficient', 'weighted clustering coefficient'] ]
-      kwargs: {name: 'type'}
-    - xr.concat: [[!dag_tag clustering, !dag_tag clustering_weighted], !dag_prev ]
-    - .mean: [!dag_prev , 'vertex_idx']
-    - div: [!dag_prev , 3]
-      tag: data
-  hue: type
-  x: time
-
-# Comparison of the graph clustering
-clustering:
-  based_on:
-    - .multiplot_universe
-    - .calculate_clustering_coefficient
-  select:
-    true_clustering:
-      path: data/true_network/_clustering_weighted
-      transform:
-        - .isel: [ !dag_prev , { time: -1 } ]
-        - .data: [ !dag_prev ]
-  transform:
-    - calculate_clustering_coefficient:
-        in_degree: data/predicted_network/_in_degree_weighted
-        out_degree: data/predicted_network/_out_degree_weighted
-        triangles: data/predicted_network/_weighted_triangles
-    - .isel: [!dag_prev , {time: -1}]
-      tag: predicted_clustering
-  to_plot:
-    - function: sns.kdeplot
-      args:
-        - !dag_result true_clustering
-      label: $P(c)$
-    - function: sns.kdeplot
-      args:
-        - !dag_result predicted_clustering
-      label: $\hat{P}(c)$
-  helpers:
-    set_legend:
-      use_legend: True
-      title: ~
-      loc: 'best'
-    set_title:
-      title: Clustering coefficient
-
 # Plot of the phases in a polar plot
 phases:
   based_on:
@@ -395,7 +166,7 @@ phases:
     phases:
       path: training_data/training_data
       transform:
-        - .isel: [!dag_prev , {time: [0, -1], runs: 0}]
+        - .isel: [!dag_prev , {time: [0, -1], training_set: 0}]
           kwargs: {drop: true}
   transform:
     - mod: [!dag_tag phases , 6.283 ]
@@ -454,9 +225,9 @@ phases_lines:
     - .plot.facet_grid.line
   select:
     data:
-      path: training_data/training_data
+      path: training_data/phases
       transform:
-        - .isel: [ !dag_prev , { runs: 0 } ]
+        - .isel: [ !dag_prev , { training_set: 0 } ]
           kwargs:
             drop: True
   x: time
@@ -466,6 +237,41 @@ phases_lines:
     set_legend:
       use_legend: False
 
+oscillations:
+  based_on:
+    - .creator.universe
+    - .plot.facet_grid.line
+  select:
+    eigen_frequencies:
+      path: training_data/eigen_frequencies
+      transform:
+        - .isel: [ !dag_prev , { training_set: 0 } ]
+          kwargs:
+            drop: True
+    phases:
+      path: training_data/phases
+      transform:
+        - .isel: [ !dag_prev , { training_set: 0 } ]
+          kwargs:
+            drop: True
+  transform:
+    - .coords: [!dag_tag phases, 'time']
+      tag: t
+    - .squeeze: [!dag_tag phases]
+    - mul: [!dag_prev , !dag_tag t ]
+    - add: [!dag_tag eigen_frequencies , !dag_prev ]
+    - sin: [!dag_prev ]
+#    - .isel: [!dag_prev , {vertex_idx: [0, 1, 255]}]
+      tag: data
+  x: time
+  hue: vertex_idx
+#  color: black
+  helpers:
+    set_legend:
+      use_legend: True
+#    set_limits:
+#      x: [2.5, 5.5]
+
 # ======================================================================================================================
 #  ╔╗╔╔═╗╔╦╗╦ ╦╔═╗╦═╗╦╔═  ╔═╗╦  ╔═╗╔╦╗╔═╗
 #  ║║║║╣  ║ ║║║║ ║╠╦╝╠╩╗  ╠═╝║  ║ ║ ║ ╚═╗
@@ -540,8 +346,11 @@ undirected_adjacency_matrix:
       x: [ -1, !dag_result num_agents ]
       y: [ !dag_result num_agents, -1 ]
     set_tick_locators:
-      x:
-        <<: *formatting
+      x: &formatting
+        major:
+          name: MaxNLocator
+          integer: true
+          nbins: !dag_result num_agents
       y:
         <<: *formatting
   cmap:
@@ -656,40 +465,6 @@ marginals_parameter_uni:
           linestyle: dashed
           color: gray
 
-# Marginal density on a distribution
-marginals_density_uni:
-  based_on:
-    - .multiplot_universe
-    - .marginals_density
-  select:
-    num_agents:
-      path: true_network/_vertices
-      transform:
-        - .coords: [!dag_prev , 'vertex_idx']
-        - len: [!dag_prev ]
-    param_binned:
-      path: predicted_network/_in_degree
-      transform:
-        - np.linspace: [!dag_tag min_bin, !dag_tag max_bin, !dag_tag n_bins]
-        - .data: [ !dag_node -2 ]
-        - NeuralABM.hist: [!dag_prev ]
-          kwargs:
-            density: true
-            bins: !dag_node -2
-    true_param:
-      path: true_network/_degree
-      transform:
-        - .data: [!dag_prev ]
-        - .squeeze: [!dag_prev ]
-    loss:
-      path: output_data/loss
-      transform:
-        - np.maximum: [ !dag_prev , *loss_limit ]
-  transform:
-    - NeuralABM.marginal_of_density: [!dag_tag param_binned ]
-      kwargs:
-        loss: !dag_tag loss
-      tag: data
 
 # ======================================================================================================================
 #  ╔╦╗╦ ╦╦ ╔╦╗╦╦  ╦╔═╗╦═╗╔═╗╔═╗  ╔═╗╦  ╔═╗╔╦╗╔═╗
diff --git a/models/Kuramoto/Kuramoto_cfg.yml b/models/Kuramoto/Kuramoto_cfg.yml
index 2835845..f03c8ee 100644
--- a/models/Kuramoto/Kuramoto_cfg.yml
+++ b/models/Kuramoto/Kuramoto_cfg.yml
@@ -1,6 +1,6 @@
 Data:
   synthetic_data:
-    N: !is-positive-int 100
+    N: !is-positive-int 16
     network:
       mean_degree: !is-positive-int 5
       type: !param
@@ -12,29 +12,47 @@ Data:
         is_weighted: !is-bool true
         WattsStrogatz:
           p_rewire: !is-probability 0.2
-    sigma: !is-positive-or-zero &sigma 0.0
-    dt: !is-positive 0.01
-    gamma: !is-positive 1
-    num_steps: !is-positive-int 60
-    training_set_size: !is-positive-int 1
+    eigen_frequencies:
+      distribution: !param
+        default: uniform
+        is_any_of: [uniform, normal]
+      parameters:
+        lower: 1
+        upper: 3
+      time_series: !is-bool false
+    init_phases:
+      distribution: !param
+        default: uniform
+        is_any_of: [ uniform, normal ]
+      parameters:
+        lower: 0
+        upper: 6.283
+    sigma: !is-positive-or-zero 0.0
+    num_steps: !is-positive-int 5
+    training_set_size: !is-positive-int 40
+  dt: !is-positive 0.01
+  gamma: !is-positive 1
 
 second_order: !is-bool False
 
 # Neural net configuration
 NeuralNet:
-  num_layers: !is-positive-int 1
+  num_layers: !is-positive-int 5
   nodes_per_layer:
     default: !is-positive-int 20
   activation_funcs:
-    default: linear
+    default: tanh
+    layer_specific:
+      -1: HardSigmoid
   biases:
     default: ~
   learning_rate: !is-positive 0.002
+  optimizer: Adam
 
 Training:
+  device: cpu
   batch_size: !is-positive-int 1
   loss_function:
     name: MSELoss
   true_parameters:
-    sigma: *sigma
-  device: cpu
+    sigma: !is-positive-or-zero 0.0
diff --git a/models/Kuramoto/Kuramoto_plots.yml b/models/Kuramoto/Kuramoto_plots.yml
index 785736c..cf53fe8 100644
--- a/models/Kuramoto/Kuramoto_plots.yml
+++ b/models/Kuramoto/Kuramoto_plots.yml
@@ -1,66 +1,22 @@
 ---
 loss:
   based_on: loss
-  color: black
-
-frobenius_loss:
-  based_on: loss
-  select:
-    data: output_data/frobenius_error
-  color: black
-
-graphs/true_graph:
-  based_on: graph
-
-graphs/prediction:
-  based_on: graph
-  select:
-    graph_group: predicted_network
-
-marginals/mean_degree:
-  based_on: marginals_uni
-
-marginals/degree_distribution:
-  based_on: marginals_density_uni
-  dag_options:
-    define:
-      n_bins: 300
-      bw_method: 0.5
-      sigma: 6
-      min_bin: 0
-      max_bin: 100
-  helpers:
-    set_title:
-      title: Degree distribution $P(k)$
-    set_tick_locators:
-      x:
-        major:
-          nbins: 20
-
-marginals/clustering:
-  based_on: marginals_density_uni
-  dag_options:
-    define:
-      n_bins: 1000
-      min_bin: -8
-      max_bin: 257
-      bw_method: 0.2
-      sigma: 8
-  select:
-    coords:
-      path: predicted_network/_triangles
-    param:
-      path: predicted_network/_triangles
-    param_binned:
-      path: predicted_network/_triangles
-    true_param:
-      path: true_network/_triangles
-  helpers:
-    set_title:
-      title: Clustering distribution $P(c)$
-    set_limits:
-      x: [0, 40]
-
+#  color: black
+
+#frobenius_loss:
+#  based_on: loss
+#  select:
+#    data: output_data/frobenius_error
+#  color: black
+#
+#graphs/true_graph:
+#  based_on: graph
+#
+#graphs/prediction:
+#  based_on: graph
+#  select:
+#    graph_group: predicted_network
+#
 matrices/true_adjacency_matrix:
   based_on: adjacency_matrix
 
@@ -71,40 +27,40 @@ matrices/prediction:
       path: output_data/predictions
       transform:
         - .isel: [!dag_prev , {time: -1}]
-
-matrices/accuracy:
-  based_on: accuracy
-
-matrices/accuracy_on_true_edges:
-  based_on: accuracy_on_true_edges
-
-matrices/accuracy_on_false_edges:
-  based_on: accuracy_on_false_edges
-
-phases/polar:
-  based_on: phases
-
+#
+#matrices/accuracy:
+#  based_on: accuracy
+#
+#matrices/accuracy_on_true_edges:
+#  based_on: accuracy_on_true_edges
+#
+#matrices/accuracy_on_false_edges:
+#  based_on: accuracy_on_false_edges
+#
+#phases/polar:
+#  based_on: phases
+#
 phases/lines:
   based_on: phases_lines
-
-properties/density:
-  based_on: density
-
-properties/degree:
-  based_on: degree
-  select:
-    true_degree:
-      path: true_network/_degree
-    predicted_in_degree:
-      path: predicted_network/_in_degree
-    predicted_out_degree:
-      path: predicted_network/_in_degree
-
-properties/clustering:
-  based_on: clustering
-
-properties/mean_degree_over_time:
-  based_on: mean_degree_over_time
-
-properties/clustering_over_time:
-  based_on: clustering_over_time
+#
+#properties/density:
+#  based_on: density
+#
+#properties/degree:
+#  based_on: degree
+#  select:
+#    true_degree:
+#      path: true_network/_degree
+#    predicted_in_degree:
+#      path: predicted_network/_in_degree
+#    predicted_out_degree:
+#      path: predicted_network/_in_degree
+#
+#properties/clustering:
+#  based_on: clustering
+#
+#properties/mean_degree_over_time:
+#  based_on: mean_degree_over_time
+#
+#properties/clustering_over_time:
+#  based_on: clustering_over_time
diff --git a/models/Kuramoto/cfgs/Computational_performance/eval.yml b/models/Kuramoto/cfgs/Computational_performance/eval.yml
index e69de29..063e59e 100644
--- a/models/Kuramoto/cfgs/Computational_performance/eval.yml
+++ b/models/Kuramoto/cfgs/Computational_performance/eval.yml
@@ -0,0 +1,117 @@
+.variables:
+  page_widths:
+    full_width:     &full_width         7.5
+    half_width:     &half_width         !expr 7.5 / 2
+    third_width:    &third_width        !expr 7.5 / 3
+    quarter_width:  &quarter_width      !expr 7.5 / 4
+    fifth_width:     &fifth_width        !expr 7.5 / 5
+    eighth_width:    &eighth_width      !expr 7.5 / 8
+    two_thirds_width:    &two_thirds_width      !expr 2* 7.5 / 3
+  colors: &colors
+    yellow:         &yellow       '#F5DDA9'
+    darkblue: &darkblue     '#2F7194'
+    red: &red          '#ec7070'
+    skyblue: &skyblue      '#97c3d0'
+    green: &green        '#48675A'
+    lightbrown: &lightbrown   '#C6BFA2'
+    orange: &orange       '#EC9F7E'
+    lightgreen: &lightgreen   '#AFD8BC'
+    grey: &grey         '#3D4244'
+
+loss: !pspace
+  based_on:
+    - .creator.multiverse
+    - .plot.facet_grid.errorbands
+  select_and_combine:
+    fields:
+      loss: output_data/Training loss
+      prediction_error: output_data/Prediction error
+    subspace:
+      N: !sweep
+        default: 10
+        values: [10, 20, 40, 100, 200]
+  transform:
+    - .mean: [!dag_tag loss, 'seed' ]
+      tag: mean_loss
+    - .std: [!dag_tag loss, 'seed']
+    - xr.Dataset:
+      - y: !dag_tag mean_loss
+        yerr: !dag_prev
+      tag: stats_loss
+    - .mean: [!dag_tag prediction_error, 'seed' ]
+      tag: mean_l1_err
+    - .std: [!dag_tag prediction_error, 'seed']
+    - xr.Dataset:
+      - y: !dag_tag mean_l1_err
+        yerr: !dag_prev
+      tag: stats_pred_err
+    - pd.Index: [ [ 'Training loss', 'Frobenius error' ] ]
+      kwargs: {name: 'loss type'}
+    - xr.concat: [[!dag_tag stats_loss, !dag_tag stats_pred_err], !dag_prev ]
+      tag: data
+  x: time
+  y: y
+  yerr: yerr
+  hue: loss type
+  helpers:
+    set_scales:
+      y: log
+  style:
+    figure.figsize: [ *half_width, *quarter_width ]
+
+time_and_loss:
+  based_on:
+    - .creator.multiverse
+  module: model_plots.Kuramoto
+  plot_func: time_and_loss
+  select_and_combine:
+    fields:
+      loss:
+        path: output_data/Prediction error
+        transform:
+          - .isel: [!dag_prev , {time: -1}]
+            kwargs: {drop: true}
+      time: output_data/computation_time
+  transform:
+    - .mean: [!dag_tag loss, 'seed' ]
+      tag: mean_loss
+    - .std: [!dag_tag loss, 'seed']
+    - xr.Dataset:
+      - y: !dag_tag mean_loss
+        yerr: !dag_prev
+      tag: loss_data
+
+    - .mean: [!dag_tag time, 'seed' ]
+    - .mean: [!dag_prev , 'epoch']
+    - .squeeze: [!dag_prev ]
+      tag: mean_time
+    - .mean: [!dag_tag time, 'seed']
+    - .std: [!dag_prev , 'epoch']
+    - .squeeze: [!dag_prev ]
+    - xr.Dataset:
+      - y: !dag_tag mean_time
+        yerr: !dag_prev
+      tag: time_data
+  compute_only: [loss_data, time_data]
+  helpers:
+    set_labels:
+      x: $N$
+      y: 'Time per epoch [s]'
+    set_texts:
+      texts:
+        - x: 250
+          y: 40
+          s: 'CPU time'
+          color: *green
+        - x: 70
+          y: 25
+          s: '$l1$ error'
+          color: *darkblue
+    set_title:
+      title: ~
+  file_ext: pdf
+  loss_color: *darkblue
+  time_color: *green
+  style:
+    axes.spines.right: True
+    figure.figsize: [*half_width, *quarter_width]
diff --git a/models/Kuramoto/cfgs/Computational_performance/run.yml b/models/Kuramoto/cfgs/Computational_performance/run.yml
index e69de29..966cd5a 100644
--- a/models/Kuramoto/cfgs/Computational_performance/run.yml
+++ b/models/Kuramoto/cfgs/Computational_performance/run.yml
@@ -0,0 +1,38 @@
+---
+paths:
+  model_note: Computational_performance
+perform_sweep: True
+parameter_space:
+  num_epochs: 10
+  write_every: 1
+  write_start: 1
+  write_predictions_every: 1
+  seed: !sweep
+    default: 0
+    range: [2]
+  Kuramoto:
+    Data:
+      synthetic_data:
+        num_steps: 2
+        training_set_size: !coupled-sweep
+          default: 100
+          values: [5, 10, 20, 50, 100, 150, 200, 250, 300]
+          target_name: N
+        N: !sweep
+          default: 10
+          values: [10, 20, 40, 100, 200, 300, 400, 500, 600]
+        sigma: 1e-3
+        network:
+          type: random
+          mean_degree: !coupled-sweep
+            default: 6
+            values: [3, 4, 6, 10, 14, 17, 20, 22, 24]
+            target_name: N
+          graph_props:
+            is_directed: False
+      dt: 0.02
+    Training:
+      batch_size: !coupled-sweep
+        default: 5
+        values: [1, 2, 4, 10, 20, 30, 40, 50, 60]
+        target_name: N
\ No newline at end of file
diff --git a/models/Kuramoto/cfgs/Data_generation/training_data_noisy.yml b/models/Kuramoto/cfgs/Data_generation/training_data_noisy.yml
index 0415846..de969e1 100644
--- a/models/Kuramoto/cfgs/Data_generation/training_data_noisy.yml
+++ b/models/Kuramoto/cfgs/Data_generation/training_data_noisy.yml
@@ -10,8 +10,8 @@ parameter_space:
   Kuramoto:
     Data:
       load_from_dir:
-        network: /Users/thomasgaskin/utopya_output/Kuramoto/networks/data/N_100/data.h5
-        eigen_frequencies: /Users/thomasgaskin/utopya_output/Kuramoto/networks/data/N_100/data.h5
+        network: /Users/thomasgaskin/utopya_output/Kuramoto/networks/data/N_100_example/data.h5
+        eigen_frequencies: /Users/thomasgaskin/utopya_output/Kuramoto/networks/data/N_100_example/data.h5
       synthetic_data:
         num_steps: 10
         training_set_size: 200
diff --git a/models/Kuramoto/cfgs/N_1000_example/eval.yml b/models/Kuramoto/cfgs/N_1000_example/eval.yml
index 7fe572e..5a5960b 100644
--- a/models/Kuramoto/cfgs/N_1000_example/eval.yml
+++ b/models/Kuramoto/cfgs/N_1000_example/eval.yml
@@ -53,7 +53,7 @@
 # ╠═╝║  ║ ║ ║ ╚═╗
 # ╩  ╩═╝╚═╝ ╩ ╚═╝
 # ======================================================================================================================
-# Plot the training and Frobenius loss
+# Plot the training and prediction loss
 losses:
   based_on: losses_combined
   figsize: [*half_width, *third_width]
diff --git a/models/Kuramoto/cfgs/N_1000_example/run.yml b/models/Kuramoto/cfgs/N_1000_example/run.yml
index 9acdc9d..e348824 100644
--- a/models/Kuramoto/cfgs/N_1000_example/run.yml
+++ b/models/Kuramoto/cfgs/N_1000_example/run.yml
@@ -1,21 +1,19 @@
 ---
 paths:
-  model_note: Large_sample
+  model_note: N_1000_example
 parameter_space:
   num_epochs: 30
-  write_every: 5
+  write_every: 6
   seed: 20
-  write_predictions_every: 5
+  write_predictions_every: 6
   write_start: 15
   Kuramoto:
     Data:
       synthetic_data:
         num_steps: 2
-        training_set_size: 1250
+        training_set_size: 1400
         N: 1000
-        sigma: !sweep
-          default: 1e-3
-          values: [1e-3, 4e-2]
+        sigma: 1e-2
         network:
           type: random
           mean_degree: 40
@@ -29,13 +27,11 @@ parameter_space:
       activation_funcs:
         default: tanh
         layer_specific:
-          -1: Hardsigmoid
+          -1: HardSigmoid
       biases:
         default: ~
     Training:
       batch_size: 5
-      optimizer: Adam
-      learning_rate: 0.02
       loss_function:
         name: MSELoss
       true_parameters:
diff --git a/models/Kuramoto/cfgs/N_100_example/eval.yml b/models/Kuramoto/cfgs/N_100_example/eval.yml
index 8153a36..4228249 100644
--- a/models/Kuramoto/cfgs/N_100_example/eval.yml
+++ b/models/Kuramoto/cfgs/N_100_example/eval.yml
@@ -60,31 +60,24 @@ losses:
   based_on: losses_combined
   figsize: [*half_width, *third_width]
 
-# Plot the true and predicted adjacency matrices
-matrix_comparison:
+# Plot the true adjacency matrix
+true_network:
   based_on:
-    - .matrix
+    - adjacency_matrix
     - .matrix_defaults
+  cbar_kwargs:
+    label: True edge weight $a_{ij}$
+
+# Plot the predicted adjacency matrix
+predicted_network:
+  based_on: true_network
   select:
-    true_matrix:
-      path: true_network/_adjacency_matrix
-      transform: [.data ]
-    predicted_matrix:
+    data:
       path: output_data/predictions
       transform:
-        - .isel: [ !dag_prev , { time: -1 } ]
-          kwargs: {drop: true}
-  transform:
-    - pd.Index: [ [ 'true', 'predicted' ] ]
-      kwargs: {name: 'kind'}
-    - xr.concat: [ [ !dag_tag true_matrix, !dag_tag predicted_matrix ], !dag_prev ]
-      tag: data
-  col: kind
-  aspect: ~
-  size: ~
-  figsize: [*two_thirds_width, *third_width]
+        - .isel: [!dag_prev , {time: -1}]
   cbar_kwargs:
-    label: ~
+    label: Predicted edge weight $\hat{a}_{ij}$
 
 # Plot the prediction error on the matrices
 error:
@@ -96,13 +89,14 @@ error:
   vmin: 1e-7
   vmax: ~
   cbar_kwargs:
-    label: $\vert \hat{a}_{ij} - a_{ij} \vert$
+    label: Prediction error $\vert \hat{a}_{ij} - a_{ij} \vert$
   cmap:
     from_values:
       0: white
       0.5: *yellow
       1: *red
 
+# Plot the degree distribution with uncertainty
 degree:
   based_on: .multiplot_universe
   dag_options:
@@ -164,22 +158,7 @@ degree:
         error: Hellinger
         MLE_index: !dag_prev
       tag: data
-    - div: [!dag_tag loss_flattened , 20 ]
-      tag: losses
-    - .data: [!dag_tag loss_flattened]
-    - .to_dataset: [ !dag_tag samples ]
-      kwargs: { name: y }
-      tag: distributions
   to_plot:
-#    - function: [ model_plots.HarrisWilson, plot_prob_density ]
-#      args: [ !dag_result distributions ]
-#      y: y
-#      hue: sample
-#      alpha: !dag_result losses
-#      lw: 0.3 #!dag_result losses
-#      suppress_labels: true
-#      color: *darkblue
-#      pass_helper: True
     - function: [model_plots.HarrisWilson, plot_prob_density]
       args: [!dag_result data]
       x: bin_center
@@ -196,7 +175,7 @@ degree:
   x: bin_center
   smooth_kwargs:
     enabled: True
-    sigma: 2.5
+    sigma: 3
   helpers:
     set_title:
       title: ~
@@ -213,7 +192,7 @@ degree:
   style:
     figure.figsize: [*half_width, *quarter_width]
 
-
+# Plot the triangle distribution with uncertainty
 triangles:
   based_on: degree
   dag_options:
@@ -236,4 +215,4 @@ triangles:
       x: [0, 5]
     set_labels:
       x: Weighted triangle count $t$
-      y: $P(t)$
\ No newline at end of file
+      y: $P(t)$
diff --git a/models/Kuramoto/cfgs/N_100_example/run.yml b/models/Kuramoto/cfgs/N_100_example/run.yml
index f871fb3..6bb1888 100644
--- a/models/Kuramoto/cfgs/N_100_example/run.yml
+++ b/models/Kuramoto/cfgs/N_100_example/run.yml
@@ -1,6 +1,6 @@
 ---
 paths:
-  model_note: N_100
+  model_note: N_100_example
 perform_sweep: True
 parameter_space:
   num_epochs: 10
@@ -10,8 +10,6 @@ parameter_space:
   seed: 20
   Kuramoto:
     Data:
-      load_from_dir:
-        network: data/Kuramoto/synthetic_networks/N_100/data.h5
       synthetic_data:
         num_steps: 10
         training_set_size: 750
@@ -20,24 +18,6 @@ parameter_space:
           default: 0
           values: [0, 1e-3]
         network:
-          type: random
           mean_degree: 6
-          graph_props:
-            is_directed: False
-      dt: 0.02
-    NeuralNet:
-      num_layers: 5
-      nodes_per_layer:
-        default: 20
-      activation_funcs:
-        default: tanh
-        layer_specific:
-          -1: Hardsigmoid
     Training:
-      batch_size: 5
-      optimizer: Adam
-      learning_rate: 0.02
-      loss_function:
-        name: MSELoss
-      true_parameters:
-        sigma: 0.0
+      batch_size: 5
\ No newline at end of file
diff --git a/models/Kuramoto/cfgs/Noise_performance/run.yml b/models/Kuramoto/cfgs/Noise_performance/run.yml
index f7376cc..92bee85 100644
--- a/models/Kuramoto/cfgs/Noise_performance/run.yml
+++ b/models/Kuramoto/cfgs/Noise_performance/run.yml
@@ -5,7 +5,7 @@ paths:
 parameter_space:
   seed: !sweep
     default: 2
-    range: [10]
+    range: [1]
   num_epochs: 50
   write_every: 10
   write_start: 15
@@ -13,10 +13,10 @@ parameter_space:
   Kuramoto:
     Data:
       write_adjacency_matrix: True
-      load_from_dir:
-        copy_data: True
-        network: data/Kuramoto/networks/data/N_100/data.h5
-        eigen_frequencies: data/Kuramoto/networks/data/N_100/data.h5
+#      load_from_dir:
+#        copy_data: True
+#        network: data/Kuramoto/synthetic_networks/data/N_100_example/data.h5
+#        eigen_frequencies: data/Kuramoto/synthetic_networks/data/N_100_example/data.h5
       synthetic_data:
         num_steps: 5
         training_set_size: 500
@@ -26,27 +26,6 @@ parameter_space:
           values: [0.0, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 2e-2, 4e-2, 6e-2, 8e-2, 1e-1]
         network:
           type: random
-          mean_degree: 6
-          graph_props:
-            is_directed: False
-            WattsStrogatz:
-              p_rewire: 0.5
-        dt: 0.02
-    NeuralNet:
-      num_layers: 5
-      nodes_per_layer:
-        default: 20
-      activation_funcs:
-        default: tanh
-        layer_specific:
-          -1: Hardsigmoid
-      biases:
-        default: ~
+          mean_degree: 20
     Training:
       batch_size: 5
-      optimizer: Adam
-      learning_rate: 0.002
-      loss_function:
-        name: MSELoss
-      true_parameters:
-        sigma: 0.0
diff --git a/models/Kuramoto/cfgs/Regression_comparison/eval.yml b/models/Kuramoto/cfgs/Regression_comparison/eval.yml
index 91e8db3..4f3d434 100644
--- a/models/Kuramoto/cfgs/Regression_comparison/eval.yml
+++ b/models/Kuramoto/cfgs/Regression_comparison/eval.yml
@@ -31,8 +31,9 @@
 # ╠═╝║  ║ ║ ║ ╚═╗
 # ╩  ╩═╝╚═╝ ╩ ╚═╝
 # ======================================================================================================================
+
 # Compares the prediction performance of OLS regression to the neural scheme
-regression_comparison:
+regression_comparison: !pspace
   based_on: .multiplot_multiverse
   select_and_combine:
     fields:
@@ -47,6 +48,10 @@ regression_comparison:
       true_values:
         path: true_network/_adjacency_matrix
         transform: [.data]
+    subspace:
+      second_order: !sweep
+        default: False
+        values: [False, True]
   transform:
     - sub: [!dag_tag regression_data, !dag_tag true_values]
     - np.abs: [!dag_prev ]
@@ -83,6 +88,12 @@ regression_comparison:
       x: $\sigma$
       y: $\Vert \hat{\mathbf{A}} - \mathbf{A} \Vert_1$
     set_legend:
+      use_legend: !coupled-sweep
+        default: True
+        values: [True, False]
+        target_name: second_order
+      title: ~
+    set_title:
       title: ~
   style:
     figure.figsize: [ *half_width, *quarter_width ]
@@ -90,4 +101,4 @@ regression_comparison:
       fstr: "cycler('color', ['black',
                               '{colors[yellow]:}',
                               ])"
-      colors: *colors
+      colors: *colors
\ No newline at end of file
diff --git a/models/Kuramoto/cfgs/Regression_comparison/run.yml b/models/Kuramoto/cfgs/Regression_comparison/run.yml
index cff4cef..661dc04 100644
--- a/models/Kuramoto/cfgs/Regression_comparison/run.yml
+++ b/models/Kuramoto/cfgs/Regression_comparison/run.yml
@@ -6,43 +6,41 @@ parameter_space:
   seed: !sweep
     default: 0
     range: [10]
-  num_epochs: 500
-  write_every: 10
+  num_epochs: !coupled-sweep
+    default: 500
+    values: [500, 2500]
+    target_name: second_order
+  write_start: !coupled-sweep
+    default: 5000
+    values: [5000, 100000]
+    target_name: second_order
+  write_every: 1
   write_predictions_every: -1
   perform_regression: true
   Kuramoto:
     Data:
       write_adjacency_matrix: True
       synthetic_data:
-        num_steps: 5
-        training_set_size: 10
+        num_steps: !coupled-sweep
+          default: 5
+          values: [5, 6]
+          target_name: second_order
+        training_set_size: !coupled-sweep
+          default: 10
+          values: [10, 40]
+          target_name: second_order
         N: 16
         sigma: !sweep
-          default: 0
+          default: 1e-6
           values: [1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 2e-2, 4e-2, 6e-2, 8e-2, 1e-1]
         network:
           type: random
           mean_degree: 6
           graph_props:
             is_directed: False
-            WattsStrogatz:
-              p_rewire: 0.5
-        dt: 0.01
-    NeuralNet:
-      num_layers: 5
-      nodes_per_layer:
-        default: 20
-      activation_funcs:
-        default: tanh
-        layer_specific:
-          -1: Hardsigmoid
-      biases:
-        default: ~
+      dt: 0.02
+    second_order: !sweep
+      default: False
+      values: [False, True]
     Training:
-      batch_size: 10
-      optimizer: Adam
-      learning_rate: 0.002
-      loss_function:
-        name: MSELoss
-      true_parameters:
-        sigma: 0
+      batch_size: 5
diff --git a/models/Kuramoto/cfgs/Training_data_example/eval.yml b/models/Kuramoto/cfgs/Training_data_example/eval.yml
index e69de29..42dce14 100644
--- a/models/Kuramoto/cfgs/Training_data_example/eval.yml
+++ b/models/Kuramoto/cfgs/Training_data_example/eval.yml
@@ -0,0 +1,13 @@
+phases:
+  based_on: phases_lines
+  style:
+    figure.figsize: [2.775, 2.5]
+  alpha: 0.2
+  lw: 0.4
+  helpers:
+    set_limits:
+      x: [0, ~]
+      y: [0, ~]
+    set_labels:
+      x: $t$
+      y: $\theta(t)$
diff --git a/models/Kuramoto/cfgs/Training_data_example/run.yml b/models/Kuramoto/cfgs/Training_data_example/run.yml
index e69de29..31a3def 100644
--- a/models/Kuramoto/cfgs/Training_data_example/run.yml
+++ b/models/Kuramoto/cfgs/Training_data_example/run.yml
@@ -0,0 +1,22 @@
+---
+paths:
+  model_note: Training_data_example
+parameter_space:
+  num_epochs: 0
+  write_every: 1
+  seed: 20
+  write_predictions_every: 1
+  write_start: 1
+  Kuramoto:
+    Data:
+      synthetic_data:
+        num_steps: 500
+        training_set_size: 1
+        N: 100
+        sigma: 0
+        network:
+          type: random
+          mean_degree: 6
+          graph_props:
+            is_directed: False
+      dt: 0.02
diff --git a/models/Kuramoto/cfgs/UK_power_grid/eval.yml b/models/Kuramoto/cfgs/UK_power_grid/eval.yml
index e69de29..a519116 100644
--- a/models/Kuramoto/cfgs/UK_power_grid/eval.yml
+++ b/models/Kuramoto/cfgs/UK_power_grid/eval.yml
@@ -0,0 +1,215 @@
+# ======================================================================================================================
+#  ╦  ╦╔═╗╦═╗╦╔═╗╔╗ ╦  ╔═╗╔═╗
+#  ╚╗╔╝╠═╣╠╦╝║╠═╣╠╩╗║  ║╣ ╚═╗
+#   ╚╝ ╩ ╩╩╚═╩╩ ╩╚═╝╩═╝╚═╝╚═╝
+# ======================================================================================================================
+
+.variables:
+  colors: &colors
+    yellow:       &yellow       '#F5DDA9'
+    darkblue:     &darkblue     '#2F7194'
+    red:          &red          '#ec7070'
+    skyblue:      &skyblue      '#97c3d0'
+    green:        &green        '#48675A'
+    lightbrown:   &lightbrown   '#C6BFA2'
+    orange:       &orange       '#EC9F7E'
+    lightgreen:   &lightgreen   '#AFD8BC'
+    grey:         &grey         '#3D4244'
+
+  # Page widths in inches for latex documents: ensures easy integration into latex documents
+  page_widths:
+    full_width:         &full_width         7.5
+    half_width:         &half_width         !expr 7.5 / 2
+    two_thirds_width:   &two_thirds_width   !expr 2 * 7.5 / 3
+    third_width:        &third_width        !expr 7.5 / 3
+    quarter_width:      &quarter_width      !expr 7.5 / 4
+    fifth_width:        &fifth_width        !expr 7.5 / 5
+    eighth_width:       &eighth_width       !expr 7.5 / 8
+
+.matrix_defaults:
+  style:
+    figure.figsize: [ *third_width, *third_width ]
+    axes.grid: False
+    axes.spines.top: True
+    axes.spines.right: True
+    axes.linewidth: 0.5
+  vmax: 1
+  helpers:
+    set_title:
+      title: ''
+    set_ticks:
+      x:
+        major: []
+      y:
+        major: []
+    set_labels:
+      x: ' '
+      y: ' '
+    set_limits:
+      y: [max, min]
+      x: [min, max]
+
+# ======================================================================================================================
+# ╔═╗╦  ╔═╗╔╦╗╔═╗
+# ╠═╝║  ║ ║ ║ ╚═╗
+# ╩  ╩═╝╚═╝ ╩ ╚═╝
+# ======================================================================================================================
+
+# Plot the training and Frobenius loss
+losses:
+  based_on: losses_combined
+  figsize: [*half_width, *third_width]
+
+# Plot the true and predicted adjacency matrices
+matrix_comparison:
+  based_on:
+    - .matrix
+    - .matrix_defaults
+  select:
+    true_matrix:
+      path: true_network/_adjacency_matrix
+      transform: [.data ]
+    predicted_matrix:
+      path: output_data/predictions
+      transform:
+        - .isel: [ !dag_prev , { time: -1 } ]
+          kwargs: {drop: true}
+  transform:
+    - pd.Index: [ [ 'true', 'predicted' ] ]
+      kwargs: {name: 'kind'}
+    - xr.concat: [ [ !dag_tag true_matrix, !dag_tag predicted_matrix ], !dag_prev ]
+      tag: data
+  col: kind
+  aspect: ~
+  size: ~
+  figsize: [*two_thirds_width, *third_width]
+  cbar_kwargs:
+    label: ~
+
+# Plot the prediction error on the matrices
+error:
+  based_on:
+    - accuracy
+    - .matrix_defaults
+  norm:
+    name: LogNorm
+  vmin: 1e-7
+  vmax: ~
+  cbar_kwargs:
+    label: $\vert \hat{a}_{ij} - a_{ij} \vert$
+  cmap:
+    from_values:
+      0: white
+      0.5: *yellow
+      1: *red
+
+accuracy_on_true_edges: !pspace
+  based_on:
+    - .creator.universe
+    - .marginals
+  select:
+    predictions: output_data/predictions
+    true_values:
+      path: true_network/_adjacency_matrix
+      transform:
+        - .isel: [!dag_prev , {time: -1}]
+          kwargs: {drop: true}
+    loss:
+      path: output_data/Training loss
+      transform:
+        - mul: [ !dag_prev , -1 ]
+        - np.exp: [ !dag_prev ]
+  transform:
+
+    # Define a sweep over the first four edges
+    - define: !sweep
+        default: 0
+        values: [0, 1, 2, 3]
+      tag: idx
+    - .isel: [!dag_tag predictions, {time: -1}]
+      kwargs: {drop: true}
+      tag: prediction
+    - sub: [!dag_prev , !dag_tag true_values]
+    - np.abs: [!dag_prev ]
+      tag: l1_accuracy
+    - pass: [!dag_tag true_values]
+
+    # Filter by edges where predictions are below the unperturbed network value
+    - sub: [!dag_tag true_values, !dag_tag prediction]
+    - create_mask: [!dag_prev , '>=', 0]
+    - xr.where: [!dag_prev , !dag_tag l1_accuracy, 0]
+      tag: errors_on_true_edges
+
+    # Normalise to the unperturbed edge weight values
+    - pass: [ !dag_tag true_values ]
+    - xr.where: [ !dag_prev ^= 0, !dag_tag true_values, 1]
+    - div: [!dag_tag errors_on_true_edges, !dag_prev ]
+
+    # Get the 4 edges with the highest error
+    - NeuralABM.largest_entry_indices: [!dag_prev , 4]
+      kwargs: {symmetric: true}
+      tag: indices_and_errors
+
+    # Get calculate the marginals on those edges
+    - NeuralABM.sel_matrix_indices: [!dag_tag predictions, !dag_prev ]
+    - .squeeze: [!dag_prev ]
+      kwargs: {drop: true}
+    - xr.Dataset:
+      - param1: !dag_prev
+        loss: !dag_tag loss
+    - NeuralABM.compute_marginals: [!dag_prev ]
+      kwargs:
+        bins: 1000
+        along_dim: edge_idx
+    - .sel: [!dag_prev , {edge_idx: !dag_tag idx}]
+      kwargs: {drop: true}
+      tag: data
+
+    # Format the title for each plot
+    - NeuralABM.sel_matrix_indices: [ !dag_tag true_values, !dag_tag indices_and_errors ]
+      tag: true_vals
+    - getitem: [!dag_tag indices_and_errors, -1 ]
+    - getitem: [!dag_prev , !dag_tag idx]
+    - np.around: [!dag_prev , 3]
+    - str: [!dag_prev ]
+      tag: error
+    - .coords: [!dag_tag true_vals, 'i']
+    - .data: [!dag_prev ]
+      tag: i
+    - .coords: [!dag_tag true_vals, 'j']
+    - .data: [!dag_prev ]
+    - zip: [!dag_tag i, !dag_prev ]
+    - list: [!dag_prev ]
+      tag: edges
+    - getitem: [!dag_prev , !dag_tag idx]
+    - .format: ["edge: {} \n relative error: {}", !dag_prev , !dag_tag error ]
+      tag: title
+
+    # Get the unperturbed value to plot as a dashed line
+    - getitem: [ !dag_tag true_vals , !dag_tag idx ]
+      tag: unperturbed_value
+
+  x: param1
+  y: prob
+  smooth_kwargs:
+    enabled: true
+    sigma: 10
+  helpers:
+    set_labels:
+      x: ' '
+      y: ' '
+    set_title:
+      title: !dag_result title
+    set_hv_lines:
+      vlines:
+        - pos: !dag_result unperturbed_value
+          color: *red
+          linestyle: dotted
+  style:
+    figure.figsize: [*quarter_width, *quarter_width]
+
+phases_lines:
+  based_on: phases_lines
+
+oscillations:
+  based_on: oscillations
diff --git a/models/Kuramoto/cfgs/UK_power_grid/run.yml b/models/Kuramoto/cfgs/UK_power_grid/run.yml
index e69de29..75fbab5 100644
--- a/models/Kuramoto/cfgs/UK_power_grid/run.yml
+++ b/models/Kuramoto/cfgs/UK_power_grid/run.yml
@@ -0,0 +1,46 @@
+---
+paths:
+  model_note: UK_power_grid
+parameter_space:
+  num_epochs: 1000
+  write_every: 20
+  seed: 2
+  write_predictions_every: 20
+  write_start: 50
+  Kuramoto:
+    second_order: true
+    Data:
+      write_adjacency_matrix: true
+      load_from_dir:
+        network: data/Kuramoto/UK_power_grid/UK_power_grid_weighted.h5
+        eigen_frequencies: /Users/thomasgaskin/utopya_output/Kuramoto/perturbed_data_second_order_weighted/data/uni0/data.h5
+        training_data: /Users/thomasgaskin/utopya_output/Kuramoto/perturbed_data_second_order_weighted/data/uni0/data.h5
+        copy_data: True
+      synthetic_data:
+        num_steps: 2
+        training_set_size: 400
+        sigma: &sigma 0.0
+        eigen_frequencies:
+          distribution: normal
+          parameters:
+            mean: 50
+            std: 1.25
+          time_series: true
+      dt: 0.02
+      gamma: 3
+    NeuralNet:
+      num_layers: 5
+      nodes_per_layer:
+        default: 20
+      activation_funcs:
+        default: tanh
+        layer_specific:
+          -1: Hardsigmoid
+    Training:
+      batch_size: 5
+      optimizer: Adam
+      learning_rate: 0.002
+      loss_function:
+        name: MSELoss
+      true_parameters:
+        sigma: *sigma
diff --git a/models/Kuramoto/run.py b/models/Kuramoto/run.py
index fff1d1d..2a9a35b 100755
--- a/models/Kuramoto/run.py
+++ b/models/Kuramoto/run.py
@@ -83,8 +83,7 @@ class Kuramoto_NN:
 
         # Current training loss, Frobenius error, and current predictions
         self.current_loss = torch.tensor(0.0)
-        self.current_frob_error = torch.tensor(0.0)
-        self.current_predictions = torch.zeros(self.nw_size)
+        self.current_prediction_error = torch.tensor(0.0)
         self.current_adjacency_matrix = torch.zeros(self.num_agents, self.num_agents)
 
         # Store the neural net training loss
@@ -100,16 +99,16 @@ class Kuramoto_NN:
         self._dset_loss.attrs["coords__time"] = [write_start, write_every]
 
         # Store the prediction error
-        self._dset_frob_error = self._output_data_group.create_dataset(
-            "Frobenius error",
-            (0, ),
-            maxshape=(None, ),
+        self._dset_prediction_error = self._output_data_group.create_dataset(
+            "Prediction error",
+            (0,),
+            maxshape=(None,),
             chunks=True,
             compression=3,
         )
-        self._dset_frob_error.attrs["dim_names"] = ["time"]
-        self._dset_frob_error.attrs["coords_mode__time"] = "start_and_step"
-        self._dset_frob_error.attrs["coords__time"] = [write_start, write_every]
+        self._dset_prediction_error.attrs["dim_names"] = ["time"]
+        self._dset_prediction_error.attrs["coords_mode__time"] = "start_and_step"
+        self._dset_prediction_error.attrs["coords__time"] = [write_start, write_every]
 
         # Store the neural net output, possibly less regularly than the loss
         self._dset_predictions = self._output_data_group.create_dataset(
@@ -131,22 +130,25 @@ class Kuramoto_NN:
         # Store the computation time
         self.dset_time = self._output_data_group.create_dataset(
             "computation_time",
-            (0, 1),
-            maxshape=(None, 1),
+            (0, ),
+            maxshape=(None, ),
             chunks=True,
             compression=3,
         )
-        self.dset_time.attrs["dim_names"] = ["epoch", "training_time"]
+        self.dset_time.attrs["dim_names"] = ["epoch"]
         self.dset_time.attrs["coords_mode__epoch"] = "trivial"
-        self.dset_time.attrs["coords_mode__training_time"] = "trivial"
 
-    def epoch(self, *, training_data, batch_size: int, second_order: bool):
+    def epoch(
+        self, *, training_data, eigen_frequencies, batch_size: int, second_order: bool
+    ):
 
         """Trains the model for a single epoch.
 
         :param training_data: the training data to use
+        :param eigen_frequencies: the time series of the nodes' eigenfrequencies
         :param batch_size: the number of training data time frames to process before updating the neural net
             parameters
+        :param second_order: whether the dynamics are second order
         """
 
         # Track the start time
@@ -160,22 +162,27 @@ class Kuramoto_NN:
             if batches[-1] != training_data.shape[1] - 1:
                 batches = np.append(batches, training_data.shape[1] - 1)
 
+        # Track the total number of processed time series frames
+        counter = 0
+
+        # Make an initial prediction
+        predicted_adj_matrix = torch.reshape(
+            self.neural_net(torch.flatten(training_data[0, 0])), (self.num_agents, self.num_agents)
+        )
+
+        loss = torch.tensor(0.0, requires_grad=True)
+
         # Process the training data in batches
         for batch_no, batch_idx in enumerate(batches[:-1]):
 
             for i, dset in enumerate(training_data):
 
-                predicted_parameters = self.neural_net(torch.flatten(dset[batch_idx]))
-                pred_adj_matrix = torch.reshape(
-                    predicted_parameters, (self.num_agents, self.num_agents)
-                )
                 current_values = dset[batch_idx].clone()
                 current_values.requires_grad_(True)
 
                 # Calculate the current velocities if the dynamics are second order
-                current_velocities = (dset[batch_idx].clone() - dset[batch_idx - 1].clone()) / self.ABM.dt if second_order else None
-
-                loss = torch.tensor(0.0, requires_grad=True)
+                current_velocities = (dset[batch_idx].clone() - dset[
+                    batch_idx - 1].clone()) / self.ABM.dt if second_order else None
 
                 for ele in range(batch_idx + 1, batches[batch_no + 1] + 1):
 
@@ -183,50 +190,68 @@ class Kuramoto_NN:
                     new_values = self.ABM.run_single(
                         current_phases=current_values,
                         current_velocities=current_velocities,
-                        adjacency_matrix=pred_adj_matrix,
+                        adjacency_matrix=predicted_adj_matrix,
+                        eigen_frequencies=eigen_frequencies[i, ele-1],
                         requires_grad=True,
                     )
 
                     # Calculate loss
                     loss = loss + self.loss_function(new_values, dset[ele]) / (
-                        batches[batch_no + 1] - batch_idx
+                            batches[batch_no + 1] - batch_idx
                     )
 
-                    # Update the velocities, if required
-                    if second_order:
-                        current_velocities = (new_values - current_values)/self.ABM.dt
+                    counter += 1
 
-                    current_values = new_values.clone().detach()
+                    if counter % batch_size == 0:
 
-                # Penalise the trace (which cannot be learned)
-                loss = loss + torch.trace(pred_adj_matrix)
+                        # Penalise the trace (which cannot be learned)
+                        loss = loss + torch.trace(predicted_adj_matrix)
 
-                # Enforce symmetry of the predicted adjacency matrix
-                loss = loss + self.loss_function(
-                    pred_adj_matrix, torch.transpose(pred_adj_matrix, 0, 1)
-                )
+                        # Enforce symmetry of the predicted adjacency matrix
+                        loss = loss + self.loss_function(
+                            predicted_adj_matrix, torch.transpose(predicted_adj_matrix, 0, 1)
+                        )
 
-                # Perform a gradient descent step
-                loss.backward()
-                self.neural_net.optimizer.step()
-                self.neural_net.optimizer.zero_grad()
+                        # Perform a gradient descent step
+                        loss.backward()
+                        self.neural_net.optimizer.step()
+                        self.neural_net.optimizer.zero_grad()
 
-                # Write the data
-                self.current_loss = loss.clone().detach().numpy().item()
-                self.current_frob_error = (
-                    torch.nn.functional.mse_loss(self.true_network, pred_adj_matrix.clone().detach()
-                    )
-                    .numpy()
-                    .item()
-                )
-                self.current_predictions = predicted_parameters.clone().detach()
-                self.current_adjacency_matrix = pred_adj_matrix.clone().detach()
-                self._time += 1
-                self.write_data()
-                self.write_predictions()
+                        # Write the data
+                        self.current_loss = loss.clone().detach()
+                        self.current_prediction_error = (
+                            torch.nn.functional.l1_loss(self.true_network, predicted_adj_matrix.clone().detach()
+                                                         )
+                                .numpy()
+                                .item()
+                        )
+                        self.current_adjacency_matrix = predicted_adj_matrix.clone().detach()
+                        self._time += 1
+                        self.write_data()
+                        self.write_predictions()
+
+                        predicted_adj_matrix = torch.reshape(
+                            self.neural_net(torch.flatten(dset[batches[batch_no + 1]])),
+                            (self.num_agents, self.num_agents)
+                        )
+
+                        del loss
+                        loss = torch.tensor(0.0, requires_grad=True)
+
+                        if second_order:
+                            current_velocities = dset[ele] - dset[ele-1]
+                        current_values = dset[ele]
+
+                    else:
+
+                        # Update the velocities, if required
+                        if second_order:
+                            current_velocities = (new_values - current_values) / self.ABM.dt
+
+                        current_values = new_values.clone().detach()
 
         self.dset_time.resize(self.dset_time.shape[0] + 1, axis=0)
-        self.dset_time[-1, :] = time.time() - start_time
+        self.dset_time[-1] = time.time() - start_time
 
     def write_data(self):
 
@@ -240,10 +265,10 @@ class Kuramoto_NN:
 
             if self._time % self._write_every == 0:
                 self._dset_loss.resize(self._dset_loss.shape[0] + 1, axis=0)
-                self._dset_loss[-1] = self.current_loss
+                self._dset_loss[-1] = self.current_loss.cpu().numpy()
 
-                self._dset_frob_error.resize(self._dset_frob_error.shape[0] + 1, axis=0)
-                self._dset_frob_error[-1] = self.current_frob_error
+                self._dset_prediction_error.resize(self._dset_prediction_error.shape[0] + 1, axis=0)
+                self._dset_prediction_error[-1] = self.current_prediction_error
 
     def write_predictions(self, *, write_final: bool = False):
 
@@ -266,7 +291,7 @@ class Kuramoto_NN:
                 self._dset_predictions.resize(
                     self._dset_predictions.shape[0] + 1, axis=0
                 )
-                self._dset_predictions[-1, :] = self.current_adjacency_matrix
+                self._dset_predictions[-1, :] = self.current_adjacency_matrix.cpu()
 
 
 # ----------------------------------------------------------------------------------------------------------------------
@@ -318,29 +343,35 @@ if __name__ == "__main__":
     training_data_group = h5file.create_group("training_data")
     output_data_group = h5file.create_group("output_data")
 
-    second_order = model_cfg.get('second_order', False)
+    second_order = model_cfg.get("second_order", False)
 
     # Get the training data and the network, if synthetic data is used
     log.info("   Generating training data ...")
-    training_data, network = Kuramoto.DataGeneration.get_data(
-        model_cfg["Data"], h5file, training_data_group, seed=seed, device=device, second_order=second_order
+    training_data, eigen_frequencies, network = Kuramoto.DataGeneration.get_data(
+        model_cfg["Data"],
+        h5file,
+        training_data_group,
+        seed=seed,
+        device=device,
+        second_order=second_order,
     )
 
-    # Get the eigen frequencies of the nodes
-    eigen_frequencies = torch.stack(
-        list(nx.get_node_attributes(network, "eigen_frequency").values())
-    )
-
-    # Generate the h5group for the predicted network, if it is to be learned
+    # Initialise the neural net
     num_agents = training_data.shape[2]
     output_size = num_agents**2
 
     log.info(
         f"   Initializing the neural net; input size: {num_agents}, output size: {output_size} ..."
     )
+
     net = base.NeuralNet(
         input_size=num_agents, output_size=output_size, **model_cfg["NeuralNet"]
-    )
+    ).to(device)
+
+    # Set the neural net to an initial state, if given
+    if model_cfg["NeuralNet"].get("initial_state", None) is not None:
+        net.load_state_dict(torch.load(model_cfg["NeuralNet"].get("initial_state")))
+        net.eval()
 
     # Get the true parameters
     true_parameters = model_cfg["Training"]["true_parameters"]
@@ -348,9 +379,10 @@ if __name__ == "__main__":
     # Initialise the ABM
     ABM = Kuramoto.Kuramoto_ABM(
         N=num_agents,
-        dt=model_cfg["Data"]["synthetic_data"]["dt"],
-        gamma=model_cfg["Data"]["synthetic_data"]["gamma"],
-        eigen_frequencies=eigen_frequencies,
+        dt=model_cfg["Data"]["dt"],
+        gamma=model_cfg["Data"]["gamma"],
+        **true_parameters,
+        device=device,
     )
 
     # Calculate the frequency with which to write out the model predictions
@@ -374,19 +406,26 @@ if __name__ == "__main__":
         write_start=cfg["write_start"],
     )
 
-    log.info(f"   Initialized model '{model_name}'.")
+    log.info(f"   Initialized model '{model_name}'. Now commencing training for {num_epochs} epochs ...")
 
     # Train the neural net
-    log.info(f"   Now commencing training for {num_epochs} epochs ...")
-
     for i in range(num_epochs):
-        model.epoch(training_data=training_data.to(device), batch_size=batch_size, second_order=second_order)
+        model.epoch(
+            training_data=training_data.to(device),
+            eigen_frequencies=eigen_frequencies.to(device),
+            batch_size=batch_size,
+            second_order=second_order,
+        )
 
         log.progress(
             f"   Completed epoch {i + 1} / {num_epochs}; current loss: {model.current_loss}; "
-            f"current Frobenius error: {model.current_frob_error}"
+            f"current L1 prediction error: {model.current_prediction_error}"
         )
 
+        # Save neural net, if specified
+        if model_cfg["NeuralNet"].get("save_to", None) is not None:
+            torch.save(net.state_dict(), model_cfg["NeuralNet"].get("save_to"))
+
     if write_predictions_every == -1:
         model.write_predictions(write_final=True)
 
@@ -400,7 +439,8 @@ if __name__ == "__main__":
             eigen_frequencies,
             h5file,
             model_cfg["Data"]["synthetic_data"]["dt"],
-            second_order=second_order
+            second_order=second_order,
+            gamma=model_cfg["Data"]["gamma"]
         )
 
     log.info("   Wrapping up ...")
diff --git a/models/SIR/cfgs/ABM_data/run.yml b/models/SIR/cfgs/ABM_data/run.yml
index 2896bb7..e2636e6 100644
--- a/models/SIR/cfgs/ABM_data/run.yml
+++ b/models/SIR/cfgs/ABM_data/run.yml
@@ -1,7 +1,9 @@
 paths:
   model_note: ABM_data
 parameter_space:
-  seed: 0
+  seed: !sweep
+    default: 0
+    range: [2]
   num_epochs: 1
   write_start: 1
   write_every: 1