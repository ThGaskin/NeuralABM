---
# Deep example
NN_deep1:
  num_layers: 1
  nodes_per_layer: 10
  activation_funcs: sigmoid

# Deep example with different activation functions
NN_deep2:
  num_layers: 3
  nodes_per_layer: 15
  activation_funcs:
    1: sigmoid
    2: relu
    3: tanh

# Deep example with activation functions only on first and last layer
NN_deep3:
  num_layers: 5
  nodes_per_layer: 5
  activation_funcs:
    0: sigmoid
    -1: tanh
  init_bias: [-3, -2]
  bias: True

NN_deep4:
  num_layers: 5
  nodes_per_layer: 5
  activation_funcs:
    name: HardTanh
    args:
      - -2
      - +2
  bias: False

NN_deep5:
  num_layers: 5
  nodes_per_layer: 5
  activation_funcs:
    0:
      name: HardTanh
      args:
        - -2
        - +2
    1: abs

# Shallow case with no activation function
NN_shallow1:
  num_layers: 1
  nodes_per_layer: 5
  activation_funcs: None

# Shallow case with activation function
NN_shallow2:
  num_layers: 1
  nodes_per_layer: 5
  activation_funcs: tanh

# Deep case with bias initialised
NN_bias:
  num_layers: 2
  nodes_per_layer: 5
  init_bias: [-1, 1]
  bias: True

# Different optimiser function
NN_SGD:
  num_layers: 1
  nodes_per_layer: 10
  optimiser: SGD
  learning_rate: 0.1
