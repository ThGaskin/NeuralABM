---
# Deep example
deep1:
  num_layers: 1
  nodes_per_layer: 10
  activation_funcs: sigmoid

# Deep example with different activation functions
deep2:
  num_layers: 3
  nodes_per_layer: 15
  activation_funcs:
    1: sigmoid
    2: relu
    3: tanh

# Deep example with activation functions only on first and last layer
deep3:
  num_layers: 5
  nodes_per_layer: 5
  activation_funcs:
    0: sigmoid
    -1: tanh
  biases: [-3, -2]

deep4:
  num_layers: 5
  nodes_per_layer: 5
  activation_funcs:
    name: HardTanh
    args:
      - -2
      - +2

deep5:
  num_layers: 5
  nodes_per_layer: 5
  activation_funcs:
    0:
      name: HardTanh
      args:
        - -2
        - +2
    1: abs

# Shallow case with no activation function
shallow1:
  num_layers: 1
  nodes_per_layer: 5
  activation_funcs: None

# Shallow case with activation function
shallow2:
  num_layers: 1
  nodes_per_layer: 5
  activation_funcs: tanh


# Different optimiser function
SGD:
  num_layers: 1
  nodes_per_layer: 10
  optimiser: SGD
  learning_rate: 0.1

# Identical bias on all layers
bias:
  num_layers: 2
  nodes_per_layer: 5
  biases: [-1, 1]

# Bias only on some layers
biases:
  num_layers: 5
  nodes_per_layer: 5
  activation_funcs:
    0: sigmoid
    -1: tanh
  biases:
    0: [ -3, -2 ]
    1: [1, 2]
    2: [-1, 1]
    5: ~
