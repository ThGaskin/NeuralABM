# Settings for data loading or generation
Data:

  # Load synthetic data from a location. The file must be a torch tensor of shape (L, 3) with L the number of steps
  load_from: ~

  # Alternatively, generate synthetic data for each ensemble member
  synthetic_data:

    # Number of training sets
    n_train: 4

    # Number of test sets
    n_test: 2

    # Initial condition: must be given as a vector of normalised densities
    y0: [0.9, 0.1, 0]

    # Parameters: can be a single value, a list of parameters - one for each training and test set -- or a dictionary
    # specifying how to generate random values
    k_S: 0.0
    k_I:
      distribution: uniform
      parameters:
        lower: 0.
        upper: 2
    k_R:
      distribution: uniform
      parameters:
        lower: 0.0
        upper: 0.3
    noise: 0.03 # Multiplicative noise variance

    # Time range; together with dt this determines the number of time steps
    t_span: [0, 50]

    # Time differential
    dt: 0.5

# Here you can select which parameters to learn; if not all parameters are to be learned, you must supply the values
# to use for the others during training in the `fixed_parameters` entry
Learning:
  parameters_to_learn: [k_I, k_R]
  fixed_parameters:
    k_S: 0
    noise: 0
    dt: 0.1

# Settings for the feed-forward network
# TODO: allow controlling the architecture from the config
NeuralNet:

  # Number of layers
  num_layers: 1

  # Nodes per layer
  nodes_per_layer:
    default: 6

  # Initialisation range for the biases (can be None, in which case no bias is used)
  biases:
    default: [-1, 1]

  # Default activation function to use. Can specify a different activation for each layer using the 'layer_specific'
  # argument
  activation_funcs:
    default: softplus

# Training settings
Training:

  # Loss function to use
  loss_function:
    name: MSELoss
    kwargs:
      reduction: sum